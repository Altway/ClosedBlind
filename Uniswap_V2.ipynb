{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from decimal import Decimal\n",
    "from threading import Lock\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from requests.exceptions import HTTPError\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "from web3 import Web3\n",
    "from web3.exceptions import (\n",
    "    Web3RPCError,\n",
    "    TransactionNotFound,\n",
    "    BlockNotFound,\n",
    "    Web3Exception,\n",
    ")\n",
    "from web3.providers.rpc.utils import (\n",
    "    ExceptionRetryConfiguration,\n",
    "    REQUEST_RETRY_ALLOWLIST,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "pd.options.display.float_format = \"{:20,.4f}\".format\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "ETHERSCAN_API_KEY_DICT = {\n",
    "    \"hearthquake\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_HEARTHQUAKE\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "    \"opensee\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_OPENSEE\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "    \"eco\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_ECO\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "INFURA_URL = ETHERSCAN_API_KEY_DICT[\"hearthquake\"][\"INFURA_URL\"]\n",
    "ETHERSCAN_API_KEY = ETHERSCAN_API_KEY_DICT[\"hearthquake\"][\"ETHERSCAN_API_KEY\"]\n",
    "UNISWAP_V2_CONTRACT = \"0x5C69bEe701ef814a2B6a3EDD4B1652CB9cc5aA6f\"\n",
    "\n",
    "OUTPUT_FILE = \"out/V2/V2_final_tx.jsonl\"\n",
    "STATE_FILE = \"out/V2/V2_final_scan_state.json\"\n",
    "TOKEN_NAME_FILE = \"out/V2/V2_token_name.json\"\n",
    "V2_EVENT_BY_CONTRACTS = \"out/V2/uniswap_v2_pairs_events.json\"\n",
    "DB_PATH = \"out/V2/uniswap_v2_events.duckdb\"\n",
    "GLOBAL_DICT_TOKEN_SYMBOL = {}\n",
    "if os.path.exists(TOKEN_NAME_FILE):\n",
    "    with open(TOKEN_NAME_FILE, \"r\") as f:\n",
    "        GLOBAL_DICT_TOKEN_SYMBOL = json.load(f)\n",
    "\n",
    "w3 = Web3(\n",
    "    Web3.HTTPProvider(\n",
    "        endpoint_uri=INFURA_URL,\n",
    "        request_kwargs={\"timeout\": 30},\n",
    "        exception_retry_configuration=ExceptionRetryConfiguration(\n",
    "            errors=(ConnectionError, HTTPError, TimeoutError),\n",
    "            retries=5,\n",
    "            backoff_factor=1,\n",
    "            method_allowlist=REQUEST_RETRY_ALLOWLIST,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "assert w3.is_connected(), \"Web3 provider connection failed\"\n",
    "print(f\"✓ Connected to Ethereum. Latest block: {w3.eth.block_number:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0df9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Helper Function: Get ABI from Etherscan or Disk\n",
    "# --------------------\n",
    "def get_abi(contract_address: str, api_key: str) -> list:\n",
    "    \"\"\"\n",
    "    Retrieves the ABI for a given contract address.\n",
    "    Checks if the ABI is available in the local 'ABI' folder.\n",
    "    If not, it fetches the ABI from Etherscan using the provided API key,\n",
    "    then saves it to disk for future use.\n",
    "\n",
    "    Parameters:\n",
    "        contract_address (str): The contract address (checksum not required here).\n",
    "        api_key (str): Your Etherscan API key.\n",
    "\n",
    "    Returns:\n",
    "        list: The ABI loaded as a Python list.\n",
    "    \"\"\"\n",
    "    # Ensure the ABI folder exists.\n",
    "    abi_folder = \"ABI\"\n",
    "    if not os.path.exists(abi_folder):\n",
    "        os.makedirs(abi_folder)\n",
    "\n",
    "    # Save ABI with filename based on contract address.\n",
    "    filename = os.path.join(abi_folder, f\"{contract_address}.json\")\n",
    "\n",
    "    # If file exists, load and return the ABI.\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\") as file:\n",
    "            abi = json.load(file)\n",
    "    else:\n",
    "        try:\n",
    "            url = f\"https://api.etherscan.io/v2/api?chainid=1&module=contract&action=getabi&address={contract_address}&apikey={api_key}\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            if data[\"status\"] == \"1\":\n",
    "                # Parse the ABI and save it for later use.\n",
    "                abi = json.loads(data[\"result\"])\n",
    "                with open(filename, \"w\") as file:\n",
    "                    json.dump(abi, file)\n",
    "        except Exception as e:\n",
    "            Exception(\n",
    "                f\"Error fetching ABI for contract {contract_address}: {data['result']}\"\n",
    "            )\n",
    "    return abi\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Helper: Convert event to dict\n",
    "# -----------------------\n",
    "def event_to_dict(event):\n",
    "    d = dict(event)\n",
    "    if \"args\" in d:\n",
    "        d[\"args\"] = dict(d[\"args\"])\n",
    "    if \"transactionHash\" in d:\n",
    "        d[\"transactionHash\"] = d[\"transactionHash\"].hex()\n",
    "    if \"blockHash\" in d:\n",
    "        d[\"blockHash\"] = d[\"blockHash\"].hex()\n",
    "    return d\n",
    "\n",
    "\n",
    "class Web3JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        # HexBytes → hex string\n",
    "        if isinstance(obj, HexBytes):\n",
    "            return obj.hex()\n",
    "        # Peel off any other web3-specific types here as needed...\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# ETHERSCAN VERSION\n",
    "# Used to find at which block 1 contract has been deployed\n",
    "# Might be useful later, put it in JSON in the end\n",
    "# -----------------------\n",
    "def get_contract_creation_block_etherscan(\n",
    "    contract_address: str, etherscan_api_key: str\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Retrieves the contract creation block from Etherscan.\n",
    "    Returns the block number as an integer.\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        f\"https://api.etherscan.io/api?module=contract&action=getcontractcreation\"\n",
    "        f\"&contractaddresses={contract_address}&apikey={etherscan_api_key}\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if data.get(\"status\") == \"1\":\n",
    "        results = data.get(\"result\", [])\n",
    "        if results and len(results) > 0:\n",
    "            return int(results[0][\"blockNumber\"])\n",
    "        else:\n",
    "            raise Exception(\"No contract creation data found.\")\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Error fetching creation block: \" + data.get(\"result\", \"Unknown error\")\n",
    "        )\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Used to find at which block 1 contract has been deployed\n",
    "# Might be useful later, put it in JSON in the end\n",
    "# -----------------------\n",
    "def get_contract_creation_block_custom(start_block=0, end_block=100000):\n",
    "\n",
    "    def get_contract_deployments(start_block, end_block, max_workers=8):\n",
    "        deployments = []\n",
    "\n",
    "        def process_block(block_number):\n",
    "            block = w3.eth.get_block(block_number, full_transactions=True)\n",
    "            block_deployments = []\n",
    "            for tx in block.transactions:\n",
    "                if tx.to is None:\n",
    "                    try:\n",
    "                        receipt = w3.eth.get_transaction_receipt(tx.hash)\n",
    "                        contract_address = receipt.contractAddress\n",
    "                        if contract_address:\n",
    "                            block_deployments.append(\n",
    "                                {\n",
    "                                    \"block_number\": block_number,\n",
    "                                    \"contract_address\": contract_address,\n",
    "                                }\n",
    "                            )\n",
    "                    except:\n",
    "                        print(tx.hash)\n",
    "            return block_deployments\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_block = {\n",
    "                executor.submit(process_block, bn): bn\n",
    "                for bn in range(start_block, end_block + 1)\n",
    "            }\n",
    "            for future in as_completed(future_to_block):\n",
    "                block_deployments = future.result()\n",
    "                deployments.extend(block_deployments)\n",
    "\n",
    "        return deployments\n",
    "\n",
    "    deployments = get_contract_deployments(start_block, end_block)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    with open(\"contract_deployments.json\", \"w\") as f:\n",
    "        json.dump(deployments, f, indent=4)\n",
    "\n",
    "\n",
    "# -- Step 2: Reconstruct an Event’s Signature --\n",
    "def get_event_signature(event_name: str, abi: list) -> str:\n",
    "    \"\"\"\n",
    "    Given an event name and an ABI, find the event definition and reconstruct its signature.\n",
    "    For example, for event Transfer(address,address,uint256) this returns its keccak256 hash.\n",
    "    \"\"\"\n",
    "    from eth_utils import keccak, encode_hex\n",
    "\n",
    "    for item in abi:\n",
    "        if item.get(\"type\") == \"event\" and item.get(\"name\") == event_name:\n",
    "            # Build the signature string: \"Transfer(address,address,uint256)\"\n",
    "            types = \",\".join([inp[\"type\"] for inp in item.get(\"inputs\", [])])\n",
    "            signature = f\"{event_name}({types})\"\n",
    "            return encode_hex(keccak(text=signature))\n",
    "    raise ValueError(f\"Event {event_name} not found in ABI.\")\n",
    "\n",
    "\n",
    "def block_to_utc(block_number):\n",
    "    \"\"\"\n",
    "    Convert a block number into its UTC timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        w3 (Web3): A Web3 instance\n",
    "        block_number (int): The block number\n",
    "\n",
    "    Returns:\n",
    "        datetime: The block timestamp in UTC\n",
    "    \"\"\"\n",
    "    block = w3.eth.get_block(block_number)\n",
    "    timestamp = block[\"timestamp\"]\n",
    "    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "def read_and_sort_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file, each line being a JSON object with a field `blockNumber`,\n",
    "    and returns a list of those objects sorted by blockNumber (ascending).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                # Handle bad JSON if needed, e.g., log or skip\n",
    "                print(line)\n",
    "                print(f\"Skipping bad JSON line: {e}\")\n",
    "                continue\n",
    "            # Optionally, you could check that 'blockNumber' exists, is int, etc.\n",
    "            if \"blockNumber\" not in obj:\n",
    "                print(f\"Skipping line with no blockNumber: {obj}\")\n",
    "                continue\n",
    "            data.append(obj)\n",
    "    # Now sort by blockNumber ascending\n",
    "    # If blockNumber in file is already int, fine; else convert\n",
    "    sorted_data = sorted(data, key=lambda o: int(o[\"blockNumber\"]))\n",
    "    return sorted_data\n",
    "\n",
    "\n",
    "def get_address_abi_contract(contract_address, etherscan_api_key=ETHERSCAN_API_KEY):\n",
    "    address = w3.to_checksum_address(contract_address)\n",
    "    contract_abi = get_abi(address, etherscan_api_key)\n",
    "    contract = w3.eth.contract(address=contract_address, abi=contract_abi)\n",
    "\n",
    "    return address, contract_abi, contract\n",
    "\n",
    "\n",
    "# Find the amount of token depending on the contract at the very specific block_number\n",
    "# but it use ETHERSCAN API (to go further: explorer the reconstruct from all the Transfer event but slow)\n",
    "# Not super useful for the moment\n",
    "def get_erc20_balance_at_block(user_address, token_address, block_number):\n",
    "    \"\"\"\n",
    "    Query ERC-20 balance of an address at a specific block.\n",
    "\n",
    "    user_address = \"0xe2dFC8F41DB4169A24e7B44095b9E92E20Ed57eD\"\n",
    "    token_address = \"0x514910771AF9Ca656af840dff83E8264EcF986CA\"\n",
    "    block_number = 23405236\n",
    "    balance = get_erc20_balance_at_block(user_address, token_address, block_number)\n",
    "\n",
    "    Parameters:\n",
    "        user_address: string, account to check\n",
    "        token_address: Web3 contract instance for the ERC-20 token\n",
    "        block_number: int, historical block\n",
    "\n",
    "    Returns:\n",
    "        int: token balance\n",
    "        None if contract is a proxy\n",
    "    \"\"\"\n",
    "    token_address, token_abi, token_contract = get_address_abi_contract(token_address)\n",
    "    user_address = w3.to_checksum_address(user_address)\n",
    "    token_name = None\n",
    "    token_symbol = None\n",
    "    try:\n",
    "        token_name = token_contract.functions.name().call()\n",
    "        token_symbol = token_contract.functions.symbol().call()\n",
    "    except Exception as e:\n",
    "        print(f\"Error {e}\")\n",
    "        print(f\"{token_address}\")\n",
    "        return None\n",
    "    balance = token_contract.functions.balanceOf(user_address).call(\n",
    "        block_identifier=block_number\n",
    "    )\n",
    "    print(\n",
    "        f\"Address {user_address} had {w3.from_wei(balance, \"ether\")} of {token_symbol} at block {block_number}\"\n",
    "    )\n",
    "    return balance\n",
    "\n",
    "\n",
    "def get_token_name_by_contract(\n",
    "    token_address,\n",
    "    TOKEN_NAME_FILE=TOKEN_NAME_FILE,\n",
    "    proxy_address=None,\n",
    "    global_cache=GLOBAL_DICT_TOKEN_SYMBOL,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the token name for `token_address`, using a local JSON cache.\n",
    "    If not in cache, will call get_token_name_by_contract (your ABI/Web3 function),\n",
    "    store the result (or None) in the cache file, and return it.\n",
    "    \"\"\"\n",
    "    # 1. Load cache\n",
    "    cache = global_cache\n",
    "    # if os.path.exists(TOKEN_NAME_FILE):\n",
    "    #     try:\n",
    "    #         with open(TOKEN_NAME_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    #             cache = json.load(f)\n",
    "    #     except Exception as e:\n",
    "    #         # If file is corrupted, proceed with empty cache\n",
    "    #         print(f\"Warning: cannot read token name cache: {e}\")\n",
    "\n",
    "    # 2. Check cache\n",
    "    if token_address in cache:\n",
    "        return cache[token_address]\n",
    "\n",
    "    # Not in cache → fetch from contract\n",
    "    name = None\n",
    "    symbol = None\n",
    "    address = None\n",
    "    try:\n",
    "        if proxy_address:\n",
    "            proxy_address, proxy_abi, proxy_contract = get_address_abi_contract(\n",
    "                proxy_address\n",
    "            )\n",
    "            token_address = proxy_contract.functions.getToken(token_address).call()\n",
    "        token_address, token_abi, token_contract = get_address_abi_contract(\n",
    "            token_address\n",
    "        )\n",
    "        # call name\n",
    "        name_raw = token_contract.functions.name().call()\n",
    "        symbol_raw = token_contract.functions.symbol().call()\n",
    "        address = token_contract.address\n",
    "        # Convert raw to str if needed\n",
    "        name = str(name_raw)\n",
    "        if isinstance(name_raw, (bytes, bytearray)):\n",
    "            name = name_raw.decode(\"utf-8\", errors=\"ignore\").rstrip(\"\\x00\")\n",
    "        symbol = str(symbol_raw)\n",
    "        if isinstance(symbol_raw, (bytes, bytearray)):\n",
    "            symbol = symbol_raw.decode(\"utf-8\", errors=\"ignore\").rstrip(\"\\x00\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching token name/symbol for {address}: {e}\")\n",
    "        if token_address:\n",
    "            cache[token_address] = {\n",
    "                \"name\": None,\n",
    "                \"symbol\": None,\n",
    "                \"address\": None,\n",
    "            }\n",
    "        try:\n",
    "            dirn = os.path.dirname(TOKEN_NAME_FILE) or \".\"\n",
    "            fd, tmp = tempfile.mkstemp(dir=dirn, text=True)\n",
    "            with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(cache, f, indent=2, ensure_ascii=False)\n",
    "            os.replace(tmp, TOKEN_NAME_FILE)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to save token cache: {e}\")\n",
    "        return {\n",
    "            \"name\": None,\n",
    "            \"symbol\": None,\n",
    "            \"address\": None,\n",
    "        }\n",
    "\n",
    "    # Update cache\n",
    "    cache[address] = {\n",
    "        \"name\": name,\n",
    "        \"symbol\": symbol,\n",
    "        \"address\": address,\n",
    "    }\n",
    "\n",
    "    # Write back atomically (overwrite)\n",
    "    try:\n",
    "        dirn = os.path.dirname(TOKEN_NAME_FILE) or \".\"\n",
    "        fd, tmp = tempfile.mkstemp(dir=dirn, text=True)\n",
    "        with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cache, f, indent=2, ensure_ascii=False)\n",
    "        os.replace(tmp, TOKEN_NAME_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: failed to save token cache: {e}\")\n",
    "\n",
    "    return cache[address]\n",
    "\n",
    "\n",
    "def decode_topics(log):\n",
    "    _, abi, contract = get_address_abi_contract(log[\"address\"])\n",
    "    # Try matching this log against the ABI events\n",
    "    for item in abi:\n",
    "        if item.get(\"type\") == \"event\":\n",
    "            event_signature = (\n",
    "                f'{item[\"name\"]}({\",\".join(i[\"type\"] for i in item[\"inputs\"])})'\n",
    "            )\n",
    "            event_hash = w3.keccak(text=event_signature).hex()\n",
    "\n",
    "            if log[\"topics\"][0].hex() == event_hash:\n",
    "                # Found matching event\n",
    "                decoded = contract.events[item[\"name\"]]().process_log(log)\n",
    "                return {\n",
    "                    \"event\": item[\"name\"],\n",
    "                    \"args\": dict(decoded[\"args\"]),\n",
    "                }\n",
    "\n",
    "    return {}  # no matching event in ABI\n",
    "\n",
    "\n",
    "def release_list(a):\n",
    "    del a[:]\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Database Functions (FIXED - VARCHAR for large numbers)\n",
    "\n",
    "import signal\n",
    "from threading import Lock, Event as ThreadEvent\n",
    "\n",
    "_db_path = None\n",
    "_connection_lock = Lock()\n",
    "_shutdown_event = ThreadEvent()\n",
    "\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    global _shutdown_event\n",
    "    logging.warning(\"\\n⚠️  Shutdown signal received - finishing current ranges...\")\n",
    "    _shutdown_event.set()\n",
    "\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "\n",
    "def is_shutdown_requested():\n",
    "    return _shutdown_event.is_set()\n",
    "\n",
    "\n",
    "def setup_database(db_path):\n",
    "    global _db_path\n",
    "    _db_path = db_path\n",
    "\n",
    "    conn = duckdb.connect(db_path)\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS transfer (\n",
    "            transaction_hash VARCHAR NOT NULL,\n",
    "            block_number BIGINT NOT NULL,\n",
    "            log_index INTEGER NOT NULL,\n",
    "            pair_address VARCHAR NOT NULL,\n",
    "            from_address VARCHAR NOT NULL,\n",
    "            to_address VARCHAR NOT NULL,\n",
    "            value VARCHAR NOT NULL,\n",
    "            inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            PRIMARY KEY (transaction_hash, log_index)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS swap (\n",
    "            transaction_hash VARCHAR NOT NULL,\n",
    "            block_number BIGINT NOT NULL,\n",
    "            log_index INTEGER NOT NULL,\n",
    "            pair_address VARCHAR NOT NULL,\n",
    "            sender VARCHAR NOT NULL,\n",
    "            to_address VARCHAR NOT NULL,\n",
    "            amount0_in VARCHAR NOT NULL,\n",
    "            amount1_in VARCHAR NOT NULL,\n",
    "            amount0_out VARCHAR NOT NULL,\n",
    "            amount1_out VARCHAR NOT NULL,\n",
    "            inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            PRIMARY KEY (transaction_hash, log_index)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS mint (\n",
    "            transaction_hash VARCHAR NOT NULL,\n",
    "            block_number BIGINT NOT NULL,\n",
    "            log_index INTEGER NOT NULL,\n",
    "            pair_address VARCHAR NOT NULL,\n",
    "            sender VARCHAR NOT NULL,\n",
    "            amount0 VARCHAR NOT NULL,\n",
    "            amount1 VARCHAR NOT NULL,\n",
    "            inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            PRIMARY KEY (transaction_hash, log_index)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS burn (\n",
    "            transaction_hash VARCHAR NOT NULL,\n",
    "            block_number BIGINT NOT NULL,\n",
    "            log_index INTEGER NOT NULL,\n",
    "            pair_address VARCHAR NOT NULL,\n",
    "            sender VARCHAR NOT NULL,\n",
    "            to_address VARCHAR NOT NULL,\n",
    "            amount0 VARCHAR NOT NULL,\n",
    "            amount1 VARCHAR NOT NULL,\n",
    "            inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            PRIMARY KEY (transaction_hash, log_index)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS sync (\n",
    "            transaction_hash VARCHAR NOT NULL,\n",
    "            block_number BIGINT NOT NULL,\n",
    "            log_index INTEGER NOT NULL,\n",
    "            pair_address VARCHAR NOT NULL,\n",
    "            reserve0 VARCHAR NOT NULL,\n",
    "            reserve1 VARCHAR NOT NULL,\n",
    "            inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            PRIMARY KEY (transaction_hash, log_index)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS approval (\n",
    "            transaction_hash VARCHAR NOT NULL,\n",
    "            block_number BIGINT NOT NULL,\n",
    "            log_index INTEGER NOT NULL,\n",
    "            pair_address VARCHAR NOT NULL,\n",
    "            owner VARCHAR NOT NULL,\n",
    "            spender VARCHAR NOT NULL,\n",
    "            value VARCHAR NOT NULL,\n",
    "            inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            PRIMARY KEY (transaction_hash, log_index)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_transfer_block ON transfer(block_number)\"\n",
    "    )\n",
    "    conn.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_transfer_pair ON transfer(pair_address)\"\n",
    "    )\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_swap_block ON swap(block_number)\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_swap_pair ON swap(pair_address)\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_mint_block ON mint(block_number)\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_mint_pair ON mint(pair_address)\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_burn_block ON burn(block_number)\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_burn_pair ON burn(pair_address)\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_sync_block ON sync(block_number)\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_sync_pair ON sync(pair_address)\")\n",
    "    conn.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_approval_block ON approval(block_number)\"\n",
    "    )\n",
    "    conn.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_approval_pair ON approval(pair_address)\"\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS processing_state (\n",
    "            start_block BIGINT NOT NULL,\n",
    "            end_block BIGINT NOT NULL,\n",
    "            status VARCHAR NOT NULL,\n",
    "            worker_id VARCHAR,\n",
    "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            PRIMARY KEY (start_block, end_block)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    conn.close()\n",
    "    print(\"✓ Database schema created successfully\")\n",
    "\n",
    "\n",
    "def batch_insert_events(events, worker_id=\"main\"):\n",
    "    if not events:\n",
    "        return 0\n",
    "\n",
    "    transfers = []\n",
    "    swaps = []\n",
    "    mints = []\n",
    "    burns = []\n",
    "    syncs = []\n",
    "    approvals = []\n",
    "\n",
    "    for e in events:\n",
    "        event_type = e.get(\"event\")\n",
    "        args = e.get(\"args\", {})\n",
    "\n",
    "        if event_type == \"Transfer\":\n",
    "            transfers.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"from\", \"\"),\n",
    "                    args.get(\"to\", \"\"),\n",
    "                    str(args.get(\"value\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Swap\":\n",
    "            swaps.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"sender\", \"\"),\n",
    "                    args.get(\"to\", \"\"),\n",
    "                    str(args.get(\"amount0In\", 0)),\n",
    "                    str(args.get(\"amount1In\", 0)),\n",
    "                    str(args.get(\"amount0Out\", 0)),\n",
    "                    str(args.get(\"amount1Out\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Mint\":\n",
    "            mints.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"sender\", \"\"),\n",
    "                    str(args.get(\"amount0\", 0)),\n",
    "                    str(args.get(\"amount1\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Burn\":\n",
    "            burns.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"sender\", \"\"),\n",
    "                    args.get(\"to\", \"\"),\n",
    "                    str(args.get(\"amount0\", 0)),\n",
    "                    str(args.get(\"amount1\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Sync\":\n",
    "            syncs.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    str(args.get(\"reserve0\", 0)),\n",
    "                    str(args.get(\"reserve1\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Approval\":\n",
    "            approvals.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"owner\", \"\"),\n",
    "                    args.get(\"spender\", \"\"),\n",
    "                    str(args.get(\"value\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    with _connection_lock:\n",
    "        conn = duckdb.connect(_db_path)\n",
    "\n",
    "        try:\n",
    "            if transfers:\n",
    "                conn.executemany(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO transfer (transaction_hash, block_number, log_index, pair_address, \n",
    "                                         from_address, to_address, value)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                    ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "                \"\"\",\n",
    "                    transfers,\n",
    "                )\n",
    "\n",
    "            if swaps:\n",
    "                conn.executemany(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO swap (transaction_hash, block_number, log_index, pair_address,\n",
    "                                     sender, to_address, amount0_in, amount1_in, amount0_out, amount1_out)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "                \"\"\",\n",
    "                    swaps,\n",
    "                )\n",
    "\n",
    "            if mints:\n",
    "                conn.executemany(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO mint (transaction_hash, block_number, log_index, pair_address,\n",
    "                                     sender, amount0, amount1)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                    ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "                \"\"\",\n",
    "                    mints,\n",
    "                )\n",
    "\n",
    "            if burns:\n",
    "                conn.executemany(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO burn (transaction_hash, block_number, log_index, pair_address,\n",
    "                                     sender, to_address, amount0, amount1)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "                \"\"\",\n",
    "                    burns,\n",
    "                )\n",
    "\n",
    "            if syncs:\n",
    "                conn.executemany(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO sync (transaction_hash, block_number, log_index, pair_address,\n",
    "                                     reserve0, reserve1)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?)\n",
    "                    ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "                \"\"\",\n",
    "                    syncs,\n",
    "                )\n",
    "\n",
    "            if approvals:\n",
    "                conn.executemany(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO approval (transaction_hash, block_number, log_index, pair_address,\n",
    "                                         owner, spender, value)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                    ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "                \"\"\",\n",
    "                    approvals,\n",
    "                )\n",
    "\n",
    "            total = (\n",
    "                len(transfers)\n",
    "                + len(swaps)\n",
    "                + len(mints)\n",
    "                + len(burns)\n",
    "                + len(syncs)\n",
    "                + len(approvals)\n",
    "            )\n",
    "            logging.info(\n",
    "                f\"[{worker_id}] Inserted {total} events (T:{len(transfers)} S:{len(swaps)} M:{len(mints)} B:{len(burns)} Sy:{len(syncs)} A:{len(approvals)})\"\n",
    "            )\n",
    "            return total\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "def mark_range_completed(start_block, end_block, worker_id=\"main\"):\n",
    "    with _connection_lock:\n",
    "        conn = duckdb.connect(_db_path)\n",
    "        try:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO processing_state (start_block, end_block, status, worker_id, updated_at)\n",
    "                VALUES (?, ?, 'completed', ?, NOW())\n",
    "                ON CONFLICT (start_block, end_block) \n",
    "                DO UPDATE SET \n",
    "                    status = 'completed', \n",
    "                    worker_id = ?,\n",
    "                    updated_at = NOW()\n",
    "            \"\"\",\n",
    "                (start_block, end_block, worker_id, worker_id),\n",
    "            )\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "def mark_range_processing(start_block, end_block, worker_id=\"main\"):\n",
    "    with _connection_lock:\n",
    "        conn = duckdb.connect(_db_path)\n",
    "        try:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO processing_state (start_block, end_block, status, worker_id, updated_at)\n",
    "                VALUES (?, ?, 'processing', ?, NOW())\n",
    "                ON CONFLICT (start_block, end_block) \n",
    "                DO UPDATE SET \n",
    "                    status = 'processing',\n",
    "                    worker_id = ?,\n",
    "                    updated_at = NOW()\n",
    "            \"\"\",\n",
    "                (start_block, end_block, worker_id, worker_id),\n",
    "            )\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "def get_completed_ranges():\n",
    "    with _connection_lock:\n",
    "        conn = duckdb.connect(_db_path)\n",
    "        try:\n",
    "            result = conn.execute(\n",
    "                \"\"\"\n",
    "                SELECT start_block, end_block \n",
    "                FROM processing_state \n",
    "                WHERE status = 'completed'\n",
    "            \"\"\"\n",
    "            ).fetchall()\n",
    "            return set((r[0], r[1]) for r in result)\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "def get_database_stats():\n",
    "    with _connection_lock:\n",
    "        conn = duckdb.connect(_db_path)\n",
    "        try:\n",
    "            completed_count = conn.execute(\n",
    "                \"\"\"\n",
    "                SELECT COUNT(*) FROM processing_state WHERE status = 'completed'\n",
    "            \"\"\"\n",
    "            ).fetchone()[0]\n",
    "\n",
    "            stats = {\n",
    "                \"total_transfers\": conn.execute(\n",
    "                    \"SELECT COUNT(*) FROM transfer\"\n",
    "                ).fetchone()[0],\n",
    "                \"total_swaps\": conn.execute(\"SELECT COUNT(*) FROM swap\").fetchone()[0],\n",
    "                \"total_mints\": conn.execute(\"SELECT COUNT(*) FROM mint\").fetchone()[0],\n",
    "                \"total_burns\": conn.execute(\"SELECT COUNT(*) FROM burn\").fetchone()[0],\n",
    "                \"total_syncs\": conn.execute(\"SELECT COUNT(*) FROM sync\").fetchone()[0],\n",
    "                \"total_approvals\": conn.execute(\n",
    "                    \"SELECT COUNT(*) FROM approval\"\n",
    "                ).fetchone()[0],\n",
    "                \"completed_ranges\": completed_count,\n",
    "            }\n",
    "            return stats\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "print(\"✓ Database functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Web3 Pool Functions\n",
    "\n",
    "_providers = []\n",
    "_provider_names = []\n",
    "_current_index = 0\n",
    "_provider_lock = Lock()\n",
    "\n",
    "\n",
    "def setup_web3_pool(api_key_dict):\n",
    "    global _providers, _provider_names\n",
    "\n",
    "    for name, config in api_key_dict.items():\n",
    "        provider = Web3(\n",
    "            Web3.HTTPProvider(\n",
    "                endpoint_uri=config[\"INFURA_URL\"],\n",
    "                request_kwargs={\"timeout\": 30},\n",
    "                exception_retry_configuration=ExceptionRetryConfiguration(\n",
    "                    errors=(ConnectionError, HTTPError, TimeoutError),\n",
    "                    retries=5,\n",
    "                    backoff_factor=1,\n",
    "                    method_allowlist=REQUEST_RETRY_ALLOWLIST,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if provider.is_connected():\n",
    "            _providers.append(provider)\n",
    "            _provider_names.append(name)\n",
    "            logging.info(f\"✓ Provider '{name}' connected\")\n",
    "        else:\n",
    "            logging.warning(f\"✗ Provider '{name}' failed to connect\")\n",
    "\n",
    "    if not _providers:\n",
    "        raise Exception(\"No providers connected!\")\n",
    "\n",
    "\n",
    "def get_provider():\n",
    "    global _current_index\n",
    "\n",
    "    with _provider_lock:\n",
    "        provider = _providers[_current_index]\n",
    "        name = _provider_names[_current_index]\n",
    "        _current_index = (_current_index + 1) % len(_providers)\n",
    "        return provider, name\n",
    "\n",
    "\n",
    "print(\"✓ Web3 pool functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def2d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Scanning Functions (updated to use decode_topics)\n",
    "\n",
    "\n",
    "def fetch_logs_for_range(\n",
    "    start_block, end_block, addresses, worker_id=\"main\", retry_count=0, max_retries=5\n",
    "):\n",
    "    provider, provider_name = get_provider()\n",
    "\n",
    "    try:\n",
    "        params = {\n",
    "            \"fromBlock\": start_block,\n",
    "            \"toBlock\": end_block,\n",
    "            \"address\": addresses,\n",
    "        }\n",
    "\n",
    "        logs = provider.eth.get_logs(params)\n",
    "\n",
    "        transactions = []\n",
    "        for log in logs:\n",
    "            transaction = {\n",
    "                \"transactionHash\": provider.to_hex(log[\"transactionHash\"]),\n",
    "                \"blockNumber\": log[\"blockNumber\"],\n",
    "                \"logIndex\": log.get(\"logIndex\", 0),\n",
    "                \"address\": log[\"address\"],\n",
    "                \"data\": provider.to_hex(log[\"data\"]),\n",
    "            }\n",
    "\n",
    "            topics = decode_topics(log)\n",
    "            transaction.update(topics)\n",
    "\n",
    "            if log.get(\"topics\") and len(log[\"topics\"]) > 0:\n",
    "                transaction[\"eventSignature\"] = provider.to_hex(log[\"topics\"][0])\n",
    "            else:\n",
    "                transaction[\"eventSignature\"] = \"\"\n",
    "\n",
    "            transactions.append(transaction)\n",
    "\n",
    "        logging.info(\n",
    "            f\"[{worker_id}] [{provider_name}] Fetched {len(transactions)} events from blocks [{start_block:,}, {end_block:,}]\"\n",
    "        )\n",
    "        return transactions\n",
    "\n",
    "    except HTTPError as e:\n",
    "        if e.response.status_code == 429:\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = 2**retry_count\n",
    "                logging.warning(\n",
    "                    f\"[{worker_id}] [{provider_name}] Rate limit hit, waiting {wait_time}s...\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                return fetch_logs_for_range(\n",
    "                    start_block,\n",
    "                    end_block,\n",
    "                    addresses,\n",
    "                    worker_id,\n",
    "                    retry_count + 1,\n",
    "                    max_retries,\n",
    "                )\n",
    "            else:\n",
    "                logging.error(f\"[{worker_id}] Max retries reached\")\n",
    "                raise\n",
    "        elif e.response.status_code == 402:\n",
    "            logging.critical(f\"[{worker_id}] Payment required (402)\")\n",
    "            raise\n",
    "        else:\n",
    "            logging.error(f\"[{worker_id}] HTTP error {e.response.status_code}: {e}\")\n",
    "            raise\n",
    "\n",
    "    except Web3RPCError as e:\n",
    "        if \"more than 10000 results\" in str(e) or \"-32005\" in str(e):\n",
    "            raise\n",
    "        else:\n",
    "            logging.error(f\"[{worker_id}] Web3 RPC error: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def process_block_range(start_block, end_block, addresses, worker_id=\"main\"):\n",
    "    if is_shutdown_requested():\n",
    "        logging.info(\n",
    "            f\"[{worker_id}] Shutdown requested - skipping range [{start_block:,}, {end_block:,}]\"\n",
    "        )\n",
    "        return 0\n",
    "\n",
    "    if (start_block, end_block) in get_completed_ranges():\n",
    "        logging.info(\n",
    "            f\"[{worker_id}] Skipping already processed range [{start_block:,}, {end_block:,}]\"\n",
    "        )\n",
    "        return 0\n",
    "\n",
    "    mark_range_processing(start_block, end_block, worker_id)\n",
    "\n",
    "    try:\n",
    "        events = fetch_logs_for_range(start_block, end_block, addresses, worker_id)\n",
    "\n",
    "        if is_shutdown_requested():\n",
    "            logging.warning(\n",
    "                f\"[{worker_id}] Shutdown requested after fetch - saving {len(events)} events before stopping\"\n",
    "            )\n",
    "\n",
    "        batch_insert_events(events, worker_id)\n",
    "        mark_range_completed(start_block, end_block, worker_id)\n",
    "\n",
    "        logging.info(\n",
    "            f\"[{worker_id}] ✓ Processed [{start_block:,}, {end_block:,}] - {len(events)} events\"\n",
    "        )\n",
    "        return len(events)\n",
    "\n",
    "    except Web3RPCError as e:\n",
    "        if \"more than 10000 results\" in str(e) or \"-32005\" in str(e):\n",
    "            mid = (start_block + end_block) // 2\n",
    "\n",
    "            if mid == start_block:\n",
    "                logging.error(\n",
    "                    f\"[{worker_id}] Cannot split range [{start_block:,}, {end_block:,}] further\"\n",
    "                )\n",
    "                return 0\n",
    "\n",
    "            logging.info(\n",
    "                f\"[{worker_id}] Splitting [{start_block:,}, {end_block:,}] at {mid:,}\"\n",
    "            )\n",
    "\n",
    "            count1 = process_block_range(start_block, mid, addresses, worker_id)\n",
    "\n",
    "            if is_shutdown_requested():\n",
    "                logging.warning(\n",
    "                    f\"[{worker_id}] Shutdown requested - skipping second half of split\"\n",
    "                )\n",
    "                return count1\n",
    "\n",
    "            count2 = process_block_range(mid + 1, end_block, addresses, worker_id)\n",
    "\n",
    "            return count1 + count2\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"[{worker_id}] Failed to process [{start_block:,}, {end_block:,}]: {e}\"\n",
    "            )\n",
    "            return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{worker_id}] Unexpected error: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return 0\n",
    "\n",
    "\n",
    "def generate_block_ranges(start_block, end_block, chunk_size):\n",
    "    completed = get_completed_ranges()\n",
    "\n",
    "    ranges = []\n",
    "    current = start_block\n",
    "\n",
    "    while current <= end_block:\n",
    "        end = min(current + chunk_size - 1, end_block)\n",
    "\n",
    "        if (current, end) not in completed:\n",
    "            ranges.append((current, end))\n",
    "        else:\n",
    "            logging.info(f\"Skipping completed range [{current:,}, {end:,}]\")\n",
    "\n",
    "        current = end + 1\n",
    "\n",
    "    return ranges\n",
    "\n",
    "\n",
    "def scan_blockchain(addresses, start_block, end_block, chunk_size=10000, max_workers=3):\n",
    "    ranges = generate_block_ranges(start_block, end_block, chunk_size)\n",
    "\n",
    "    if not ranges:\n",
    "        logging.info(\"No ranges to process - all already completed!\")\n",
    "        return\n",
    "\n",
    "    total_ranges = len(ranges)\n",
    "    logging.info(f\"Processing {total_ranges} block ranges with {max_workers} workers\")\n",
    "\n",
    "    total_events = 0\n",
    "    completed_ranges = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_range = {\n",
    "            executor.submit(\n",
    "                process_block_range,\n",
    "                start,\n",
    "                end,\n",
    "                addresses,\n",
    "                f\"worker-{i % max_workers}\",\n",
    "            ): (start, end, i)\n",
    "            for i, (start, end) in enumerate(ranges)\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            for future in as_completed(future_to_range):\n",
    "                if is_shutdown_requested():\n",
    "                    logging.warning(\n",
    "                        \"Shutdown requested - waiting for active tasks to complete...\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                start, end, idx = future_to_range[future]\n",
    "\n",
    "                try:\n",
    "                    event_count = future.result()\n",
    "                    total_events += event_count\n",
    "                    completed_ranges += 1\n",
    "\n",
    "                    progress = (completed_ranges / total_ranges) * 100\n",
    "                    logging.info(\n",
    "                        f\"Progress: {completed_ranges}/{total_ranges} ({progress:.1f}%) | \"\n",
    "                        f\"Total events: {total_events:,}\"\n",
    "                    )\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Range [{start:,}, {end:,}] failed: {e}\")\n",
    "\n",
    "            if is_shutdown_requested():\n",
    "                logging.warning(\n",
    "                    \"Waiting for active workers to finish their current ranges...\"\n",
    "                )\n",
    "                executor.shutdown(wait=True, cancel_futures=True)\n",
    "                logging.info(\n",
    "                    f\"✓ Graceful shutdown complete. Processed {completed_ranges}/{total_ranges} ranges.\"\n",
    "                )\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logging.warning(\n",
    "                \"\\n⚠️  Additional Ctrl+C detected - forcing immediate shutdown...\"\n",
    "            )\n",
    "            executor.shutdown(wait=False, cancel_futures=True)\n",
    "            raise\n",
    "\n",
    "    logging.info(f\"\\n{'='*60}\")\n",
    "    logging.info(f\"Scan completed!\")\n",
    "    logging.info(f\"Total events fetched: {total_events:,}\")\n",
    "    logging.info(f\"Ranges processed: {completed_ranges}/{total_ranges}\")\n",
    "    logging.info(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "print(\"✓ Scanning functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbaf009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Main Scanning Function (UPDATED stats display)\n",
    "\n",
    "\n",
    "def scan_blockchain_to_duckdb(\n",
    "    event_file=V2_EVENT_BY_CONTRACTS,\n",
    "    db_path=\"out/V2/uniswap_v2_events.duckdb\",\n",
    "    start_block=10000001,\n",
    "    end_block=20000000,\n",
    "    chunk_size=10000,\n",
    "    max_workers=3,\n",
    "    token_filter=None,\n",
    "):\n",
    "\n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(\"BLOCKCHAIN SCANNER STARTING\")\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    logging.info(f\"Loading addresses from {event_file}...\")\n",
    "    with open(event_file, \"r\") as f:\n",
    "        all_pairs = json.load(f)\n",
    "\n",
    "    all_addresses = [Web3.to_checksum_address(addr) for addr in all_pairs.keys()]\n",
    "\n",
    "    if token_filter:\n",
    "        filter_checksummed = [Web3.to_checksum_address(addr) for addr in token_filter]\n",
    "        addresses = [addr for addr in all_addresses if addr in filter_checksummed]\n",
    "        logging.info(\n",
    "            f\"Filtered to {len(addresses)} addresses from {len(all_addresses)} total\"\n",
    "        )\n",
    "    else:\n",
    "        addresses = all_addresses\n",
    "        logging.info(f\"Using all {len(addresses)} addresses\")\n",
    "\n",
    "    logging.info(\"Setting up Web3 connection pool...\")\n",
    "    setup_web3_pool(ETHERSCAN_API_KEY_DICT)\n",
    "\n",
    "    logging.info(f\"Setting up database at {db_path}...\")\n",
    "    setup_database(db_path)\n",
    "\n",
    "    stats = get_database_stats()\n",
    "    logging.info(\"\\nCurrent database stats:\")\n",
    "    logging.info(f\"  Transfers: {stats['total_transfers']:,}\")\n",
    "    logging.info(f\"  Swaps: {stats['total_swaps']:,}\")\n",
    "    logging.info(f\"  Mints: {stats['total_mints']:,}\")\n",
    "    logging.info(f\"  Burns: {stats['total_burns']:,}\")\n",
    "    logging.info(f\"  Syncs: {stats['total_syncs']:,}\")\n",
    "    logging.info(f\"  Approvals: {stats['total_approvals']:,}\")\n",
    "    logging.info(f\"  Completed ranges: {stats['completed_ranges']}\")\n",
    "\n",
    "    logging.info(f\"\\nStarting scan:\")\n",
    "    logging.info(f\"  Block range: {start_block:,} to {end_block:,}\")\n",
    "    logging.info(f\"  Chunk size: {chunk_size:,}\")\n",
    "    logging.info(f\"  Workers: {max_workers}\")\n",
    "    logging.info(f\"  Addresses: {len(addresses)}\")\n",
    "    logging.info(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        scan_blockchain(addresses, start_block, end_block, chunk_size, max_workers)\n",
    "\n",
    "        final_stats = get_database_stats()\n",
    "        logging.info(\"\\n\" + \"=\" * 60)\n",
    "        logging.info(\"SCAN COMPLETED!\")\n",
    "        logging.info(\"=\" * 60)\n",
    "        logging.info(f\"  Transfers: {final_stats['total_transfers']:,}\")\n",
    "        logging.info(f\"  Swaps: {final_stats['total_swaps']:,}\")\n",
    "        logging.info(f\"  Mints: {final_stats['total_mints']:,}\")\n",
    "        logging.info(f\"  Burns: {final_stats['total_burns']:,}\")\n",
    "        logging.info(f\"  Syncs: {final_stats['total_syncs']:,}\")\n",
    "        logging.info(f\"  Approvals: {final_stats['total_approvals']:,}\")\n",
    "        logging.info(\"=\" * 60)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"\\n\\nInterrupted by user - progress saved to database\")\n",
    "        logging.info(\"You can restart to continue from where it left off\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def query_database(db_path=\"out/V2/uniswap_v2_events.duckdb\"):\n",
    "\n",
    "    conn = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"DATABASE QUERIES\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        print(\"\\n1. Event counts by type:\")\n",
    "        print(\n",
    "            f\"  Transfers: {conn.execute('SELECT COUNT(*) FROM transfer').fetchone()[0]:,}\"\n",
    "        )\n",
    "        print(f\"  Swaps: {conn.execute('SELECT COUNT(*) FROM swap').fetchone()[0]:,}\")\n",
    "        print(f\"  Mints: {conn.execute('SELECT COUNT(*) FROM mint').fetchone()[0]:,}\")\n",
    "        print(f\"  Burns: {conn.execute('SELECT COUNT(*) FROM burn').fetchone()[0]:,}\")\n",
    "        print(f\"  Syncs: {conn.execute('SELECT COUNT(*) FROM sync').fetchone()[0]:,}\")\n",
    "        print(\n",
    "            f\"  Approvals: {conn.execute('SELECT COUNT(*) FROM approval').fetchone()[0]:,}\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n2. Most active pairs (by swaps):\")\n",
    "        result = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT pair_address, COUNT(*) as swap_count\n",
    "            FROM swap\n",
    "            GROUP BY pair_address\n",
    "            ORDER BY swap_count DESC\n",
    "            LIMIT 10\n",
    "        \"\"\"\n",
    "        ).fetchdf()\n",
    "        print(result)\n",
    "\n",
    "        print(\"\\n3. Swap volume by pair:\")\n",
    "        result = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                pair_address,\n",
    "                SUM(amount0_in + amount0_out) as total_amount0,\n",
    "                SUM(amount1_in + amount1_out) as total_amount1\n",
    "            FROM swap\n",
    "            GROUP BY pair_address\n",
    "            ORDER BY total_amount0 DESC\n",
    "            LIMIT 10\n",
    "        \"\"\"\n",
    "        ).fetchdf()\n",
    "        print(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "print(\"✓ Main functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da79d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_filter = [\n",
    "    \"0xB4e16d0168e52d35CaCD2c6185b44281Ec28C9Dc\",\n",
    "    \"0x3139Ffc91B99aa94DA8A2dc13f1fC36F9BDc98eE\",\n",
    "    \"0x12EDE161c702D1494612d19f05992f43aa6A26FB\",\n",
    "    \"0xA478c2975Ab1Ea89e8196811F51A7B7Ade33eB11\",\n",
    "    \"0x07F068ca326a469Fc1d87d85d448990C8cBa7dF9\",\n",
    "    \"0xAE461cA67B15dc8dc81CE7615e0320dA1A9aB8D5\",\n",
    "    \"0xCe407CD7b95B39d3B4d53065E711e713dd5C5999\",\n",
    "    \"0x33C2d48Bc95FB7D0199C5C693e7a9F527145a9Af\",\n",
    "]\n",
    "\n",
    "START_BLOCK = 10000000\n",
    "END_BLOCK = 10500000\n",
    "# END_BLOCK = w3.eth.block_number\n",
    "\n",
    "print(f\"Scanning from block {START_BLOCK:,} to {END_BLOCK:,}\")\n",
    "\n",
    "db = scan_blockchain_to_duckdb(\n",
    "    event_file=V2_EVENT_BY_CONTRACTS,\n",
    "    db_path=DB_PATH,\n",
    "    start_block=START_BLOCK,\n",
    "    end_block=END_BLOCK,\n",
    "    chunk_size=10000,\n",
    "    max_workers=3,\n",
    "    token_filter=token_filter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logs_with_chunking(\n",
    "    get_logs_fn,\n",
    "    from_block,\n",
    "    to_block,\n",
    "    argument_filters=None,\n",
    "    initial_chunk_size=10000,\n",
    "    max_retries=5,\n",
    "    base_delay=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch logs with automatic chunking and rate limit handling.\n",
    "    \"\"\"\n",
    "\n",
    "    if argument_filters is None:\n",
    "        argument_filters = {}\n",
    "\n",
    "    # Convert 'latest' to actual block number\n",
    "    if to_block == \"latest\":\n",
    "        to_block = w3.eth.block_number\n",
    "\n",
    "    def fetch_with_retry(start, end, retries=0):\n",
    "        \"\"\"Fetch logs with exponential backoff on rate limit errors.\"\"\"\n",
    "        try:\n",
    "            time.sleep(base_delay)\n",
    "\n",
    "            logs = get_logs_fn(\n",
    "                from_block=start, to_block=end, argument_filters=argument_filters\n",
    "            )\n",
    "            return logs\n",
    "\n",
    "        except HTTPError as e:\n",
    "            if \"429\" in str(e) or \"Too Many Requests\" in str(e):\n",
    "                if retries < max_retries:\n",
    "                    wait_time = base_delay * (2**retries)\n",
    "                    print(\n",
    "                        f\"  ⚠ Rate limit hit, waiting {wait_time:.1f}s... (retry {retries + 1}/{max_retries})\"\n",
    "                    )\n",
    "                    time.sleep(wait_time)\n",
    "                    return fetch_with_retry(start, end, retries + 1)\n",
    "                else:\n",
    "                    print(f\"  ✗ Max retries reached\")\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def fetch_range(start, end, chunk_size):\n",
    "        \"\"\"Recursive function to fetch a block range with dynamic chunking.\"\"\"\n",
    "\n",
    "        if end - start <= chunk_size:\n",
    "            try:\n",
    "                print(f\"Fetching blocks {start} to {end} ({end - start + 1} blocks)...\")\n",
    "                logs = fetch_with_retry(start, end)\n",
    "                print(f\"  ✓ Got {len(logs)} logs\")\n",
    "                return logs\n",
    "\n",
    "            except HTTPError as e:\n",
    "                raise\n",
    "\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "\n",
    "                if \"-32005\" in error_str or \"more than 10000 results\" in error_str:\n",
    "                    print(f\"  ⚠ Too many results, splitting...\")\n",
    "\n",
    "                    mid = (start + end) // 2\n",
    "\n",
    "                    if mid == start:\n",
    "                        print(f\"  ⚠ Cannot split further\")\n",
    "                        try:\n",
    "                            if hasattr(e, \"args\") and len(e.args) > 0:\n",
    "                                error_data = e.args[0]\n",
    "                                if (\n",
    "                                    isinstance(error_data, dict)\n",
    "                                    and \"data\" in error_data\n",
    "                                ):\n",
    "                                    suggested_to = int(\n",
    "                                        error_data[\"data\"].get(\"to\", hex(end)), 16\n",
    "                                    )\n",
    "                                    if suggested_to < end:\n",
    "                                        print(f\"  Using RPC hint: {suggested_to}\")\n",
    "                                        return fetch_range(\n",
    "                                            start, suggested_to, chunk_size // 2\n",
    "                                        )\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        raise Exception(\n",
    "                            f\"Cannot split block range {start}-{end} further\"\n",
    "                        )\n",
    "\n",
    "                    left_logs = fetch_range(start, mid, chunk_size // 2)\n",
    "                    right_logs = fetch_range(mid + 1, end, chunk_size // 2)\n",
    "\n",
    "                    return left_logs + right_logs\n",
    "                else:\n",
    "                    print(f\"  ✗ Error: {e}\")\n",
    "                    raise\n",
    "\n",
    "        else:\n",
    "            print(f\"Splitting range {start}-{end} into chunks of {chunk_size}...\")\n",
    "            current = start\n",
    "            logs = []\n",
    "\n",
    "            while current <= end:\n",
    "                chunk_end = min(current + chunk_size - 1, end)\n",
    "                chunk_logs = fetch_range(current, chunk_end, chunk_size)\n",
    "                logs.extend(chunk_logs)\n",
    "                current = chunk_end + 1\n",
    "\n",
    "            return logs\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fetching logs from block {from_block} to {to_block}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    all_logs = fetch_range(from_block, to_block, initial_chunk_size)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ Complete! Total logs fetched: {len(all_logs)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return all_logs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN CODE\n",
    "# ============================================================\n",
    "\n",
    "# 1. Get the factory contract\n",
    "address, abi, contract = get_address_abi_contract(UNISWAP_V2_CONTRACT)\n",
    "\n",
    "start_block = 0\n",
    "end_block = \"latest\"\n",
    "\n",
    "# 2. Fetch all PairCreated events\n",
    "print(\"Fetching all PairCreated events from Uniswap V2 Factory...\")\n",
    "pair_created_logs = get_logs_with_chunking(\n",
    "    get_logs_fn=contract.events.PairCreated().get_logs,\n",
    "    from_block=start_block,\n",
    "    to_block=end_block,\n",
    "    argument_filters={},\n",
    "    initial_chunk_size=10000,\n",
    "    max_retries=5,\n",
    "    base_delay=0.5,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Found {len(pair_created_logs)} pairs\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 3. Get event names from one sample pair (all pairs have same interface)\n",
    "if len(pair_created_logs) > 0:\n",
    "    sample_pair_address = pair_created_logs[0].args.pair\n",
    "    print(f\"Getting event list from sample pair: {sample_pair_address}\")\n",
    "\n",
    "    pair_address, pair_abi, pair_contract = get_address_abi_contract(\n",
    "        sample_pair_address\n",
    "    )\n",
    "    event_names = [ev.event_name for ev in pair_contract.events]\n",
    "\n",
    "    print(f\"All pairs have these events: {event_names}\\n\")\n",
    "\n",
    "    # 4. Build the dictionary structure\n",
    "    FULL_EVENT_BY_CONTRACTS = {}\n",
    "\n",
    "    print(f\"Building dictionary structure for {len(pair_created_logs)} pairs...\")\n",
    "\n",
    "    for idx, log in enumerate(pair_created_logs):\n",
    "        pair_addr = log.args.pair\n",
    "\n",
    "        # Create structure with empty dicts for each event\n",
    "        FULL_EVENT_BY_CONTRACTS[pair_addr] = {event: {} for event in event_names}\n",
    "\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(pair_created_logs)} pairs...\")\n",
    "\n",
    "    print(f\"  ✓ Completed all {len(pair_created_logs)} pairs\\n\")\n",
    "\n",
    "    # 5. Save to disk\n",
    "    output_file = \"uniswap_v2_pairs_events.json\"\n",
    "\n",
    "    print(f\"Saving to {output_file}...\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(FULL_EVENT_BY_CONTRACTS, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"✓ Saved successfully!\")\n",
    "\n",
    "    # 6. Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total pairs: {len(FULL_EVENT_BY_CONTRACTS)}\")\n",
    "    print(f\"Events per pair: {len(event_names)}\")\n",
    "    print(f\"Event types: {', '.join(event_names)}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Print first 3 pairs as sample\n",
    "    print(\"Sample (first 3 pairs):\")\n",
    "    for idx, (pair_addr, events) in enumerate(\n",
    "        list(FULL_EVENT_BY_CONTRACTS.items())[:3]\n",
    "    ):\n",
    "        print(f\"\\n{pair_addr}:\")\n",
    "        for event_name in events.keys():\n",
    "            print(f\"  - {event_name}: {{}}\")\n",
    "\n",
    "else:\n",
    "    print(\"No pairs found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3db616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important code\n",
    "# We look for the Genesis Uniswap factory, and we get all its events (Only 1 for the V1_factory: 'NewExchange', Only 1 for the V2_factory: PairCreated)\n",
    "# Then we scan from 0 to latest block every NexEchange created from this Factory\n",
    "# (We have the filter of events in case we are filtering events from contract that have multiple events to remove when we don't care)\n",
    "address, abi, contract = get_address_abi_contract(\n",
    "    UNISWAP_V2_CONTRACT\n",
    ")  # Uniswap Genesis Factory\n",
    "start_block = 0\n",
    "end_block = 'latest'\n",
    "# list all event names\n",
    "event_names = [ev.event_name for ev in contract.events]\n",
    "print(event_names)\n",
    "# define which events you want and filters directly\n",
    "events_to_scan = [\n",
    "    contract.events.PairCreated().get_logs,\n",
    "    #contract.events.Transfer().get_logs,\n",
    "    #contract.events.Approval().get_logs,\n",
    "]\n",
    "L_LOGS = [] # IMPORTANT\n",
    "for get_logs_fn in events_to_scan:\n",
    "    logs = get_logs_fn(\n",
    "        from_block=start_block,\n",
    "        to_block=end_block,\n",
    "        argument_filters={},  # or {\"from\": some_address}, {\"to\": [addr1, addr2]}\n",
    "    )\n",
    "    for log in logs:\n",
    "        # print(log[\"transactionHash\"].hex(), log[\"blockNumber\"], log[\"event\"])\n",
    "        L_LOGS.append(log)\n",
    "\n",
    "# Important code we use in combination with the events filter\n",
    "# We created a list of Exchange created by the Uniswap V1 Factory Contract and we list all their Events\n",
    "# We create the Dictionnary\n",
    "# \"exchange_address_1\": {\"event_1\": {}, event_2: {}, event_3:{}}\n",
    "# This dict fed with the code allow us to retrieve every transactions with the events(logs) of this exchange\n",
    "# we can then sniff Liquidity out of it\n",
    "\n",
    "FULL_EVENT_BY_CONTRACTS = {}  # IMPORTANT\n",
    "for log in L_LOGS:\n",
    "    add, abi, contract = get_address_abi_contract(log.args.exchange)\n",
    "    event_names = [ev.event_name for ev in contract.events]\n",
    "    FULL_EVENT_BY_CONTRACTS[add] = {event: {} for event in event_names}\n",
    "    time.sleep(1)\n",
    "\n",
    "print(len(FULL_EVENT_BY_CONTRACTS)) \n",
    "\n",
    "if not os.path.exists(V2_EVENT_BY_CONTRACTS):\n",
    "    with open(V2_EVENT_BY_CONTRACTS, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(V2_EVENT_BY_CONTRACTS, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaeba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Load ALL events from DuckDB into pandas DataFrame\n",
    "\n",
    "conn = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "df = conn.execute(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        block_number as block,\n",
    "        pair_address as address,\n",
    "        'Transfer' as event,\n",
    "        CASE \n",
    "            WHEN from_address = '0x0000000000000000000000000000000000000000' \n",
    "                THEN to_address\n",
    "            WHEN to_address = '0x0000000000000000000000000000000000000000' \n",
    "                THEN from_address\n",
    "            ELSE from_address\n",
    "        END as provider,\n",
    "        CASE \n",
    "            WHEN from_address = '0x0000000000000000000000000000000000000000' \n",
    "                THEN CAST(value AS DOUBLE) / 1e18\n",
    "            WHEN to_address = '0x0000000000000000000000000000000000000000' \n",
    "                THEN -CAST(value AS DOUBLE) / 1e18\n",
    "            ELSE 0\n",
    "        END as value\n",
    "    FROM transfer\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        block_number as block,\n",
    "        pair_address as address,\n",
    "        'Mint' as event,\n",
    "        sender as provider,\n",
    "        (CAST(amount0 AS DOUBLE) / 1e18 + CAST(amount1 AS DOUBLE) / 1e18) as value\n",
    "    FROM mint\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        block_number as block,\n",
    "        pair_address as address,\n",
    "        'Burn' as event,\n",
    "        sender as provider,\n",
    "        -(CAST(amount0 AS DOUBLE) / 1e18 + CAST(amount1 AS DOUBLE) / 1e18) as value\n",
    "    FROM burn\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        block_number as block,\n",
    "        pair_address as address,\n",
    "        'Swap' as event,\n",
    "        sender as provider,\n",
    "        (CAST(amount0_in AS DOUBLE) / 1e18 + CAST(amount0_out AS DOUBLE) / 1e18) as value\n",
    "    FROM swap\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        block_number as block,\n",
    "        pair_address as address,\n",
    "        'Sync' as event,\n",
    "        pair_address as provider,\n",
    "        (CAST(reserve0 AS DOUBLE) / 1e18 + CAST(reserve1 AS DOUBLE) / 1e18) as value\n",
    "    FROM sync\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        block_number as block,\n",
    "        pair_address as address,\n",
    "        'Approval' as event,\n",
    "        owner as provider,\n",
    "        CAST(value AS DOUBLE) / 1e18 as value\n",
    "    FROM approval\n",
    "    \n",
    "    ORDER BY block, address\n",
    "\"\"\"\n",
    ").fetchdf()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "df[\"event\"] = df[\"event\"].astype(\"category\")\n",
    "df[\"provider\"] = df[\"provider\"].apply(Web3.to_checksum_address)\n",
    "df[\"address\"] = df[\"address\"].apply(Web3.to_checksum_address)\n",
    "df[\"block\"] = df[\"block\"].astype(np.int32)\n",
    "df[\"value\"] = pd.to_numeric(df[\"value\"], downcast=\"float\")\n",
    "\n",
    "df[\"symbol\"] = \"Unknown\"\n",
    "df[\"symbol\"] = df[\"symbol\"].astype(\"category\")\n",
    "\n",
    "print(f\"Total events: {len(df):,}\")\n",
    "print(f\"\\nEvent breakdown:\")\n",
    "print(df[\"event\"].value_counts())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ac509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ST GRAPH, evolution of the UNISWAP v1 (UNI-V1) amount of token issued/burned (GLOBAL TOTAL over block)\n",
    "# Important to compare the size of every pool but we need to link \"value\" to either $ or something relevant for comparison\n",
    "# NEED: df\n",
    "totals = (\n",
    "    df.groupby([\"block\", \"address\"], as_index=False)[\"value\"]\n",
    "    .sum()\n",
    "    .sort_values([\"address\", \"block\"])\n",
    ")\n",
    "totals[\"cum_value\"] = totals.groupby(\"address\")[\"value\"].cumsum()\n",
    "\n",
    "# # 2) fill missing blocks only inside each address' span (min..max), then cumulate\n",
    "# totals = totals.groupby(\"address\", group_keys=False).apply(\n",
    "#     lambda g: (\n",
    "#         g.set_index(\"block\")\n",
    "#         .reindex(range(g[\"block\"].min(), g[\"block\"].max() + 1), fill_value=0)\n",
    "#         .rename_axis(\"block\")\n",
    "#         .reset_index()\n",
    "#         .assign(address=g.name)\n",
    "#     )\n",
    "# )\n",
    "# totals = (\n",
    "#     totals[[\"block\", \"address\", \"value\"]]\n",
    "#     .sort_values([\"address\", \"block\"])\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "pools_of_interest = [\n",
    "    w3.to_checksum_address(token_filter[0]),\n",
    "    w3.to_checksum_address(token_filter[1]),\n",
    "    w3.to_checksum_address(token_filter[2]),\n",
    "]\n",
    "\n",
    "# pools_of_interest = [\"add_1\",\"add_2\",\"add_3\"]\n",
    "cum_long_sub = totals[totals[\"address\"].isin(pools_of_interest)]\n",
    "\n",
    "fig = px.area(\n",
    "    cum_long_sub,\n",
    "    x=\"block\",\n",
    "    y=\"cum_value\",\n",
    "    color=\"address\",\n",
    "    line_group=\"address\",\n",
    "    title=\"Cumulative liquidity evolution per pool\",\n",
    "    labels={\"cum_value\": \"Cumulative liquidity\", \"address\": \"Pool address\"},\n",
    ")\n",
    "\n",
    "# Optionally, you can also do px.line instead of px.area if you prefer lines without fill\n",
    "fig = px.line(cum_long_sub, x=\"block\", y=\"cum_value\", color=\"address\",\n",
    "              title=\"Cumulative liquidity per pool\")\n",
    "# You can also make it not stacked (i.e. overlayed) by doing:\n",
    "# fig = px.area(\n",
    "#     cum_long_sub,\n",
    "#     x=\"block\",\n",
    "#     y=\"cum_value\",\n",
    "#     color=\"address\",\n",
    "#     line_group=\"address\",\n",
    "#     facet_col=None,\n",
    "#     # maybe set `groupnorm=None` or other arguments\n",
    "# )\n",
    "\n",
    "fig.update_layout(legend_title=\"Pool address\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block_filter(block_start=None, block_end=None):\n",
    "    if block_start is not None and block_end is not None:\n",
    "        return f\"AND block_number BETWEEN {block_start} AND {block_end}\"\n",
    "    if block_start is not None:\n",
    "        return f\"AND block_number >= {block_start}\"\n",
    "    if block_end is not None:\n",
    "        return f\"AND block_number <= {block_end}\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def calculate_pool_liquidity_pure_sql(\n",
    "    db_path,\n",
    "    pair_address,\n",
    "    block_start=None,\n",
    "    block_end=None,\n",
    "    min_share_pct=0.1,\n",
    "    min_balance_threshold=1e-8,\n",
    "):\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "    block_filter = build_block_filter(block_start, block_end)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH provider_events AS (\n",
    "        SELECT \n",
    "            block_number AS block,\n",
    "            from_address AS provider,\n",
    "            -CAST(value AS DOUBLE) AS value\n",
    "        FROM transfer\n",
    "        WHERE pair_address = '{pair_address}'\n",
    "            AND from_address != '0x0000000000000000000000000000000000000000'\n",
    "        {block_filter}\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            block_number AS block,\n",
    "            to_address AS provider,\n",
    "            CAST(value AS DOUBLE) AS value\n",
    "        FROM transfer\n",
    "        WHERE pair_address = '{pair_address}'\n",
    "            AND to_address != '0x0000000000000000000000000000000000000000'\n",
    "        {block_filter}\n",
    "    ),\n",
    "    aggregated_events AS (\n",
    "        SELECT \n",
    "            block,\n",
    "            provider,\n",
    "            SUM(value) AS value\n",
    "        FROM provider_events\n",
    "        GROUP BY block, provider\n",
    "    ),\n",
    "    cumulative_balances AS (\n",
    "        SELECT \n",
    "            block,\n",
    "            provider,\n",
    "            SUM(value) OVER (PARTITION BY provider ORDER BY block) AS cum_provider\n",
    "        FROM aggregated_events\n",
    "    ),\n",
    "    filtered_balances AS (\n",
    "        SELECT *\n",
    "        FROM cumulative_balances\n",
    "        WHERE cum_provider > {min_balance_threshold}\n",
    "    ),\n",
    "    pool_totals AS (\n",
    "        SELECT \n",
    "            block,\n",
    "            SUM(cum_provider) AS cum_pool\n",
    "        FROM filtered_balances\n",
    "        GROUP BY block\n",
    "    )\n",
    "    SELECT \n",
    "        cb.block,\n",
    "        cb.provider,\n",
    "        cb.cum_provider,\n",
    "        pt.cum_pool,\n",
    "        CASE \n",
    "            WHEN pt.cum_pool < 1e-10 THEN 0.0\n",
    "            ELSE LEAST(100.0, GREATEST(0.0, (cb.cum_provider / pt.cum_pool * 100)))\n",
    "        END AS share_pct\n",
    "    FROM filtered_balances cb\n",
    "    JOIN pool_totals pt ON cb.block = pt.block\n",
    "    WHERE (cb.cum_provider / NULLIF(pt.cum_pool, 0) * 100) >= {min_share_pct}\n",
    "    ORDER BY cb.block, cb.provider\n",
    "    \"\"\"\n",
    "\n",
    "    with duckdb.connect(db_path, read_only=True) as conn:\n",
    "        df_result = conn.execute(query).fetch_df()\n",
    "\n",
    "    if df_result.empty:\n",
    "        return df_result\n",
    "\n",
    "    df_result[\"provider_label\"] = df_result[\"provider\"].apply(create_provider_label)\n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def create_provider_label(address):\n",
    "    checksum_addr = w3.to_checksum_address(address)\n",
    "    short_addr = f\"{checksum_addr[:6]}...{checksum_addr[-4:]}\"\n",
    "    return short_addr\n",
    "\n",
    "\n",
    "def add_million_block_markers(fig, min_block, max_block):\n",
    "    start = (min_block // 1_000_000) * 1_000_000\n",
    "    end = (max_block // 1_000_000 + 1) * 1_000_000 + 1\n",
    "\n",
    "    for million_block in range(start, end, 1_000_000):\n",
    "        if min_block <= million_block <= max_block:\n",
    "            fig.add_vline(\n",
    "                x=million_block,\n",
    "                line_width=2,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"black\",\n",
    "                opacity=0.4,\n",
    "                annotation_text=f\"{million_block / 1_000_000:.0f}M\",\n",
    "                annotation_position=\"top\",\n",
    "                annotation_font_size=12,\n",
    "            )\n",
    "\n",
    "\n",
    "def plot_staircase_ownership(df):\n",
    "    fig = go.Figure()\n",
    "    providers = sorted(df[\"provider_label\"].unique())\n",
    "\n",
    "    for provider in providers:\n",
    "        provider_data = df[df[\"provider_label\"] == provider].sort_values(\"block\")\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=provider_data[\"block\"],\n",
    "                y=provider_data[\"share_pct\"],\n",
    "                name=provider,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5, shape=\"hv\"),\n",
    "                stackgroup=\"one\",\n",
    "                groupnorm=\"\",\n",
    "                hovertemplate=\"<b>%{fullData.name}</b><br>Block: %{x}<br>Share: %{y:.4f}%<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    add_million_block_markers(fig, df[\"block\"].min(), df[\"block\"].max())\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Pool Ownership Distribution (Staircase View)\",\n",
    "        hovermode=\"x\",\n",
    "        yaxis_title=\"Ownership Share (%)\",\n",
    "        xaxis_title=\"Block Number\",\n",
    "        legend=dict(\n",
    "            title=\"Provider\",\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "        ),\n",
    "        yaxis=dict(range=[0, 100]),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_absolute_liquidity_staircase(df):\n",
    "    fig = go.Figure()\n",
    "    providers = sorted(df[\"provider_label\"].unique())\n",
    "\n",
    "    for provider in providers:\n",
    "        provider_data = df[df[\"provider_label\"] == provider].sort_values(\"block\")\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=provider_data[\"block\"],\n",
    "                y=provider_data[\"cum_provider\"],\n",
    "                name=provider,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5, shape=\"hv\"),\n",
    "                stackgroup=\"one\",\n",
    "                hovertemplate=\"<b>%{fullData.name}</b><br>Block: %{x}<br>Amount: %{y:.6f}<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    add_million_block_markers(fig, df[\"block\"].min(), df[\"block\"].max())\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Pool Liquidity by Provider (Absolute Values)\",\n",
    "        hovermode=\"x\",\n",
    "        yaxis_title=\"Liquidity Amount (Token Units)\",\n",
    "        xaxis_title=\"Block Number\",\n",
    "        legend=dict(\n",
    "            title=\"Provider\",\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def calculate_hhi_metrics(df):\n",
    "    share_clean = np.where(\n",
    "        np.isinf(df[\"share_pct\"]) | np.isnan(df[\"share_pct\"]), 0, df[\"share_pct\"]\n",
    "    )\n",
    "    df = df.assign(share_pct_clean=share_clean)\n",
    "\n",
    "    hhi_agg = (\n",
    "        df.groupby(\"block\")\n",
    "        .agg(\n",
    "            hhi=(\"share_pct_clean\", lambda x: (x**2).sum()),\n",
    "            active_providers=(\"share_pct_clean\", lambda x: (x > 0.01).sum()),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return hhi_agg\n",
    "\n",
    "\n",
    "def add_hhi_zones(fig):\n",
    "    zones = [\n",
    "        (0, 1500, \"green\", \"Competitive\"),\n",
    "        (1500, 2500, \"yellow\", \"Moderate\"),\n",
    "        (2500, 10000, \"red\", \"Concentrated\"),\n",
    "    ]\n",
    "\n",
    "    for y0, y1, color, label in zones:\n",
    "        fig.add_hrect(\n",
    "            y0=y0,\n",
    "            y1=y1,\n",
    "            fillcolor=color,\n",
    "            opacity=0.1,\n",
    "            annotation_text=label,\n",
    "            secondary_y=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_ownership_concentration(df):\n",
    "    hhi_df = calculate_hhi_metrics(df)\n",
    "\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hhi_df[\"block\"],\n",
    "            y=hhi_df[\"hhi\"],\n",
    "            name=\"HHI (Concentration)\",\n",
    "            line=dict(color=\"#F46821\", width=2),\n",
    "        ),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hhi_df[\"block\"],\n",
    "            y=hhi_df[\"active_providers\"],\n",
    "            name=\"Active Providers\",\n",
    "            line=dict(color=\"#29BEFD\", width=2),\n",
    "        ),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title=\"Pool Concentration Analysis\", hovermode=\"x unified\")\n",
    "    fig.update_xaxes(title_text=\"Block Number\")\n",
    "    fig.update_yaxes(title_text=\"HHI Score\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Number of Providers\", secondary_y=True)\n",
    "\n",
    "    add_hhi_zones(fig)\n",
    "\n",
    "    return fig, hhi_df\n",
    "\n",
    "\n",
    "def get_concentration_status(hhi):\n",
    "    if hhi < 1500:\n",
    "        return \"✅ COMPETITIVE (Decentralized)\"\n",
    "    elif hhi < 2500:\n",
    "        return \"⚠️  MODERATE CONCENTRATION\"\n",
    "    else:\n",
    "        return \"🔴 HIGHLY CONCENTRATED\"\n",
    "\n",
    "\n",
    "def print_liquidity_summary(df):\n",
    "    max_block = df[\"block\"].max()\n",
    "\n",
    "    summary = (\n",
    "        df.groupby([\"provider\", \"provider_label\"])[\"cum_provider\"]\n",
    "        .last()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LIQUIDITY SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for (provider, label), amount in summary.items():\n",
    "        provider_checksum = w3.to_checksum_address(provider)\n",
    "        final_data = df[\n",
    "            (df[\"provider\"] == provider_checksum) & (df[\"block\"] == max_block)\n",
    "        ]\n",
    "\n",
    "        if not final_data.empty:\n",
    "            share = final_data[\"share_pct\"].values[0]\n",
    "            print(f\"{label}: {amount:.6f} tokens ({share:.2f}% of pool)\")\n",
    "        else:\n",
    "            print(f\"{label}: {amount:.6f} tokens (exited)\")\n",
    "\n",
    "\n",
    "def print_concentration_summary(hhi_df):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CONCENTRATION METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Average HHI: {hhi_df['hhi'].mean():.2f}\")\n",
    "    print(f\"Current HHI: {hhi_df['hhi'].iloc[-1]:.2f}\")\n",
    "    print(f\"Max providers at any block: {hhi_df['active_providers'].max()}\")\n",
    "    print(f\"Current active providers: {hhi_df['active_providers'].iloc[-1]}\")\n",
    "\n",
    "    current_hhi = hhi_df[\"hhi\"].iloc[-1]\n",
    "    status = get_concentration_status(current_hhi)\n",
    "    print(f\"Pool status: {status}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def analyze_pool_liquidity(\n",
    "    db_path, pair_address, block_start=None, block_end=None, show_plots=True\n",
    "):\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "\n",
    "    print(\"Calculating pool liquidity distribution...\")\n",
    "    liquidity_df = calculate_pool_liquidity_pure_sql(\n",
    "        db_path=db_path,\n",
    "        pair_address=pair_address,\n",
    "        block_start=block_start,\n",
    "        block_end=block_end,\n",
    "    )\n",
    "\n",
    "    print(f\"Total rows in liquidity data: {len(liquidity_df)}\")\n",
    "    print(\n",
    "        f\"Block range: {liquidity_df['block'].min()} to {liquidity_df['block'].max()}\"\n",
    "    )\n",
    "    print(f\"Number of unique providers: {liquidity_df['provider'].nunique()}\")\n",
    "\n",
    "    if show_plots:\n",
    "        print(\"\\nGenerating percentage ownership chart...\")\n",
    "        fig_pct = plot_staircase_ownership(liquidity_df)\n",
    "        fig_pct.show()\n",
    "\n",
    "        print(\"Generating absolute liquidity chart...\")\n",
    "        fig_abs = plot_absolute_liquidity_staircase(liquidity_df)\n",
    "        fig_abs.show()\n",
    "\n",
    "        print(\"Generating concentration analysis...\")\n",
    "        fig_conc, concentration_metrics = plot_ownership_concentration(liquidity_df)\n",
    "        fig_conc.show()\n",
    "    else:\n",
    "        _, concentration_metrics = plot_ownership_concentration(liquidity_df)\n",
    "\n",
    "    print_liquidity_summary(liquidity_df)\n",
    "    print_concentration_summary(concentration_metrics)\n",
    "\n",
    "    return liquidity_df, concentration_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edeb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "liquidity_df, concentration_metrics = analyze_pool_liquidity(\n",
    "    db_path=DB_PATH,\n",
    "    pair_address=token_filter[0],\n",
    "    block_start=START_BLOCK,\n",
    "    block_end=END_BLOCK,\n",
    "    show_plots=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bubble_ownership(df):\n",
    "    latest_block = df[\"block\"].max()\n",
    "    latest_data = df[df[\"block\"] == latest_block].copy()\n",
    "    latest_data = latest_data.sort_values(\"share_pct\", ascending=False)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=latest_data[\"provider_label\"],\n",
    "            y=[1] * len(latest_data),\n",
    "            mode=\"markers+text\",\n",
    "            marker=dict(\n",
    "                size=latest_data[\"share_pct\"] * 10,\n",
    "                sizemode=\"diameter\",\n",
    "                sizemin=20,\n",
    "                color=latest_data[\"share_pct\"],\n",
    "                colorscale=\"Viridis\",\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Share (%)\", thickness=15, len=0.7),\n",
    "                line=dict(color=\"white\", width=2),\n",
    "            ),\n",
    "            text=latest_data[\"share_pct\"].apply(lambda x: f\"{x:.2f}%\"),\n",
    "            textposition=\"middle center\",\n",
    "            textfont=dict(size=14, color=\"white\", family=\"Arial Black\"),\n",
    "            hovertemplate=(\n",
    "                \"<b>%{x}</b><br>\"\n",
    "                \"Share: %{customdata[0]:.4f}%<br>\"\n",
    "                \"Amount: %{customdata[1]:.6f}\"\n",
    "                \"<extra></extra>\"\n",
    "            ),\n",
    "            customdata=latest_data[[\"share_pct\", \"cum_provider\"]].values,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Pool Ownership at Block {latest_block} (Bubble Size = Share %)\",\n",
    "        xaxis=dict(title=\"\", tickangle=-45, showgrid=False),\n",
    "        yaxis=dict(visible=False, range=[0.5, 1.5]),\n",
    "        height=500,\n",
    "        showlegend=False,\n",
    "        hovermode=\"closest\",\n",
    "        plot_bgcolor=\"rgba(240, 240, 240, 0.5)\",\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_bubble_ownership_2d(df):\n",
    "    latest_block = df[\"block\"].max()\n",
    "    latest_data = df[df[\"block\"] == latest_block].copy()\n",
    "    latest_data = latest_data.sort_values(\"share_pct\", ascending=False).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    n_providers = len(latest_data)\n",
    "    cols = int(np.ceil(np.sqrt(n_providers)))\n",
    "\n",
    "    latest_data[\"x_pos\"] = latest_data.index % cols\n",
    "    latest_data[\"y_pos\"] = latest_data.index // cols\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=latest_data[\"x_pos\"],\n",
    "            y=latest_data[\"y_pos\"],\n",
    "            mode=\"markers+text\",\n",
    "            marker=dict(\n",
    "                size=latest_data[\"share_pct\"] * 15,\n",
    "                sizemode=\"diameter\",\n",
    "                sizemin=30,\n",
    "                color=latest_data[\"share_pct\"],\n",
    "                colorscale=\"RdYlGn_r\",\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Share (%)\", thickness=20, len=0.7),\n",
    "                line=dict(color=\"darkgray\", width=3),\n",
    "                opacity=0.8,\n",
    "            ),\n",
    "            text=latest_data[\"provider_label\"].str.split(\"(\").str[0].str.strip(),\n",
    "            textposition=\"middle center\",\n",
    "            textfont=dict(size=12, color=\"black\", family=\"Arial Black\"),\n",
    "            hovertemplate=(\n",
    "                \"<b>%{customdata[0]}</b><br>\"\n",
    "                \"Share: %{customdata[1]:.4f}%<br>\"\n",
    "                \"Amount: %{customdata[2]:.6f}\"\n",
    "                \"<extra></extra>\"\n",
    "            ),\n",
    "            customdata=latest_data[\n",
    "                [\"provider_label\", \"share_pct\", \"cum_provider\"]\n",
    "            ].values,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for idx, row in latest_data.iterrows():\n",
    "        fig.add_annotation(\n",
    "            x=row[\"x_pos\"],\n",
    "            y=row[\"y_pos\"] - 0.15,\n",
    "            text=f\"{row['share_pct']:.2f}%\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=10, color=\"white\", family=\"Arial Black\"),\n",
    "            bgcolor=\"rgba(0,0,0,0.5)\",\n",
    "            borderpad=2,\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Pool Ownership Distribution at Block {latest_block}\",\n",
    "        xaxis=dict(visible=False, range=[-0.5, cols - 0.5]),\n",
    "        yaxis=dict(visible=False, scaleanchor=\"x\", scaleratio=1),\n",
    "        height=600,\n",
    "        width=800,\n",
    "        showlegend=False,\n",
    "        hovermode=\"closest\",\n",
    "        plot_bgcolor=\"white\",\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def analyze_pool_liquidity(\n",
    "    db_path, pair_address, block_start=None, block_end=None, show_plots=True\n",
    "):\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "\n",
    "    print(\"Calculating pool liquidity distribution...\")\n",
    "    liquidity_df = calculate_pool_liquidity_pure_sql(\n",
    "        db_path=db_path,\n",
    "        pair_address=pair_address,\n",
    "        block_start=block_start,\n",
    "        block_end=block_end,\n",
    "    )\n",
    "\n",
    "    print(f\"Total rows in liquidity data: {len(liquidity_df)}\")\n",
    "    print(\n",
    "        f\"Block range: {liquidity_df['block'].min()} to {liquidity_df['block'].max()}\"\n",
    "    )\n",
    "    print(f\"Number of unique providers: {liquidity_df['provider'].nunique()}\")\n",
    "\n",
    "    if show_plots:\n",
    "        print(\"\\nGenerating percentage ownership chart...\")\n",
    "        fig_pct = plot_staircase_ownership(liquidity_df)\n",
    "        fig_pct.show()\n",
    "\n",
    "        print(\"Generating absolute liquidity chart...\")\n",
    "        fig_abs = plot_absolute_liquidity_staircase(liquidity_df)\n",
    "        fig_abs.show()\n",
    "\n",
    "        print(\"Generating concentration analysis...\")\n",
    "        fig_conc, concentration_metrics = plot_ownership_concentration(liquidity_df)\n",
    "        fig_conc.show()\n",
    "\n",
    "        print(\"\\nGenerating bubble ownership chart...\")\n",
    "        fig_bubble = plot_bubble_ownership(liquidity_df)\n",
    "        fig_bubble.show()\n",
    "\n",
    "        print(\"\\nGenerating 2D bubble ownership chart...\")\n",
    "        fig_bubble_2d = plot_bubble_ownership_2d(liquidity_df)\n",
    "        fig_bubble_2d.show()\n",
    "    else:\n",
    "        _, concentration_metrics = plot_ownership_concentration(liquidity_df)\n",
    "\n",
    "    print_liquidity_summary(liquidity_df)\n",
    "    print_concentration_summary(concentration_metrics)\n",
    "\n",
    "    return liquidity_df, concentration_metrics\n",
    "\n",
    "\n",
    "liquidity_df, concentration_metrics = analyze_pool_liquidity(\n",
    "    db_path=DB_PATH,\n",
    "    pair_address=token_filter[0],\n",
    "    block_start=START_BLOCK,\n",
    "    block_end=END_BLOCK,\n",
    "    show_plots=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "closedblind-ylehWVqW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
