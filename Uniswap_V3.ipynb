{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1545bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 15:24:41,669 INFO ‚úì Provider 'hearthquake' connected\n",
      "2025-10-26 15:24:42,067 INFO ‚úì Provider 'opensee' connected\n",
      "2025-10-26 15:24:42,455 INFO ‚úì Provider 'eco' connected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Connected to Ethereum. Latest block: 23,662,214\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import datetime\n",
    "from datetime import timezone\n",
    "import tempfile\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from decimal import Decimal\n",
    "\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from hexbytes import HexBytes\n",
    "from requests.exceptions import HTTPError, ConnectionError\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "\n",
    "from web3 import Web3\n",
    "from web3.providers.rpc.utils import (\n",
    "    ExceptionRetryConfiguration,\n",
    "    REQUEST_RETRY_ALLOWLIST,\n",
    ")\n",
    "from web3.exceptions import Web3RPCError\n",
    "\n",
    "# Configuration\n",
    "load_dotenv()\n",
    "pd.options.display.float_format = \"{:20,.4f}\".format\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "ETHERSCAN_API_KEY_DICT = {\n",
    "    \"hearthquake\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_HEARTHQUAKE\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "    \"opensee\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_OPENSEE\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "    \"eco\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_ECO\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "ETHERSCAN_API_KEY = ETHERSCAN_API_KEY_DICT[\"hearthquake\"][\"ETHERSCAN_API_KEY\"]\n",
    "\n",
    "STATE_FILE = \"out/V3/V3_final_scan_state.json\"\n",
    "TOKEN_NAME_FILE = \"out/V3/V3_token_name.json\"\n",
    "V3_EVENT_BY_CONTRACTS = \"out/V3/uniswap_v3_pairs_events.json\"\n",
    "DB_PATH = \"out/V3/uniswap_v3.duckdb\"\n",
    "ABI_CACHE_FOLDER = \"ABI\"\n",
    "\n",
    "GLOBAL_DICT_TOKEN_SYMBOL = {}\n",
    "if os.path.exists(TOKEN_NAME_FILE):\n",
    "    with open(TOKEN_NAME_FILE, \"r\") as f:\n",
    "        GLOBAL_DICT_TOKEN_SYMBOL = json.load(f)\n",
    "\n",
    "\n",
    "class ProviderPool:\n",
    "    def __init__(self, api_key_dict):\n",
    "        self.providers = []\n",
    "        self.provider_names = []\n",
    "        self.index = 0\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "        for name, config in api_key_dict.items():\n",
    "            provider = Web3(\n",
    "                Web3.HTTPProvider(\n",
    "                    endpoint_uri=config[\"INFURA_URL\"],\n",
    "                    request_kwargs={\"timeout\": 30},\n",
    "                    exception_retry_configuration=ExceptionRetryConfiguration(\n",
    "                        errors=(ConnectionError, HTTPError, TimeoutError),\n",
    "                        retries=5,\n",
    "                        backoff_factor=1,\n",
    "                        method_allowlist=REQUEST_RETRY_ALLOWLIST,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "            if provider.is_connected():\n",
    "                self.providers.append(provider)\n",
    "                self.provider_names.append(name)\n",
    "                logging.info(f\"‚úì Provider '{name}' connected\")\n",
    "            else:\n",
    "                logging.warning(f\"‚úó Provider '{name}' failed to connect\")\n",
    "\n",
    "        if not self.providers:\n",
    "            raise Exception(\"No providers connected!\")\n",
    "\n",
    "    def get_provider(self):\n",
    "        with self.lock:\n",
    "            provider = self.providers[self.index]\n",
    "            name = self.provider_names[self.index]\n",
    "            self.index = (self.index + 1) % len(self.providers)\n",
    "            return provider, name\n",
    "\n",
    "\n",
    "PROVIDER_POOL = ProviderPool(ETHERSCAN_API_KEY_DICT)\n",
    "w3, _ = PROVIDER_POOL.get_provider()\n",
    "assert w3.is_connected(), \"Web3 provider connection failed\"\n",
    "print(f\"‚úì Connected to Ethereum. Latest block: {w3.eth.block_number:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa0df9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Helper Function: Get ABI from Etherscan or Disk\n",
    "# --------------------\n",
    "def get_abi(contract_address, api_key):\n",
    "    abi_folder = \"ABI\"\n",
    "    if not os.path.exists(abi_folder):\n",
    "        os.makedirs(abi_folder)\n",
    "\n",
    "    filename = os.path.join(abi_folder, f\"{contract_address}.json\")\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\") as file:\n",
    "            abi = json.load(file)\n",
    "    else:\n",
    "        abi = None  # ‚Üê INITIALIZE abi BEFORE try block\n",
    "        try:\n",
    "            url = f\"https://api.etherscan.io/v2/api?chainid=1&module=contract&action=getabi&address={contract_address}&apikey={api_key}\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            if data[\"status\"] == \"1\":\n",
    "                abi = json.loads(data[\"result\"])\n",
    "                with open(filename, \"w\") as file:\n",
    "                    json.dump(abi, file)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error fetching ABI for contract {contract_address}: {e}\")\n",
    "\n",
    "    return abi\n",
    "\n",
    "\n",
    "class ABIFetchError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ABINotVerified(ABIFetchError):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ABIRateLimited(ABIFetchError):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ABINetworkError(ABIFetchError):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_abi(contract_address, api_key=ETHERSCAN_API_KEY, abi_folder=ABI_CACHE_FOLDER):\n",
    "    os.makedirs(abi_folder, exist_ok=True)\n",
    "    filename = os.path.join(abi_folder, f\"{contract_address}.json\")\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, \"r\") as f:\n",
    "                abi = json.load(f)\n",
    "                if abi is None or abi == []:\n",
    "                    raise ABINotVerified(\n",
    "                        f\"Contract {contract_address} not verified (cached)\"\n",
    "                    )\n",
    "                return abi\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.warning(\n",
    "                f\"Corrupted ABI cache for {contract_address}: {e}, re-fetching...\"\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        url = f\"https://api.etherscan.io/v2/api?chainid=1&module=contract&action=getabi&address={contract_address}&apikey={api_key}\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data[\"status\"] == \"1\":\n",
    "            abi = json.loads(data[\"result\"])\n",
    "\n",
    "            if not isinstance(abi, list) or len(abi) == 0:\n",
    "                logging.warning(f\"Empty ABI for {contract_address}\")\n",
    "                raise ABINotVerified(f\"Empty ABI returned\")\n",
    "\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(abi, f, indent=2)\n",
    "            return abi\n",
    "\n",
    "        else:\n",
    "            error_msg = data.get(\"result\", \"Unknown error\")\n",
    "\n",
    "            if \"not verified\" in error_msg.lower():\n",
    "                with open(filename, \"w\") as f:\n",
    "                    json.dump(None, f)\n",
    "                raise ABINotVerified(f\"Contract not verified: {error_msg}\")\n",
    "\n",
    "            elif (\n",
    "                \"rate limit\" in error_msg.lower()\n",
    "                or \"max rate limit\" in error_msg.lower()\n",
    "            ):\n",
    "                raise ABIRateLimited(f\"Etherscan rate limit: {error_msg}\")\n",
    "\n",
    "            else:\n",
    "                logging.error(\n",
    "                    f\"Etherscan API error for {contract_address}: {error_msg}\"\n",
    "                )\n",
    "                raise ABIFetchError(f\"Etherscan error: {error_msg}\")\n",
    "\n",
    "    except requests.Timeout:\n",
    "        raise ABINetworkError(f\"Timeout fetching ABI for {contract_address}\")\n",
    "\n",
    "    except requests.ConnectionError as e:\n",
    "        raise ABINetworkError(f\"Connection error: {e}\")\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        raise ABINetworkError(f\"Request failed: {e}\")\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        raise ABIFetchError(f\"Invalid response format: {e}\")\n",
    "\n",
    "\n",
    "MINIMAL_ERC20_ABI = [\n",
    "    {\n",
    "        \"constant\": True,\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"name\",\n",
    "        \"outputs\": [{\"name\": \"\", \"type\": \"string\"}],\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"constant\": True,\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"symbol\",\n",
    "        \"outputs\": [{\"name\": \"\", \"type\": \"string\"}],\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"constant\": True,\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"decimals\",\n",
    "        \"outputs\": [{\"name\": \"\", \"type\": \"uint8\"}],\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "]\n",
    "\n",
    "MINIMAL_UNISWAP_V3_POOL_ABI = [\n",
    "    {\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"token0\",\n",
    "        \"outputs\": [{\"type\": \"address\"}],\n",
    "        \"stateMutability\": \"view\",\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"token1\",\n",
    "        \"outputs\": [{\"type\": \"address\"}],\n",
    "        \"stateMutability\": \"view\",\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"fee\",\n",
    "        \"outputs\": [{\"type\": \"uint24\"}],\n",
    "        \"stateMutability\": \"view\",\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"factory\",\n",
    "        \"outputs\": [{\"type\": \"address\"}],\n",
    "        \"stateMutability\": \"view\",\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def get_contract_with_fallback(\n",
    "    contract_address, provider=None, contract_type=\"generic\"\n",
    "):\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    contract_address = provider.to_checksum_address(contract_address)\n",
    "\n",
    "    try:\n",
    "        abi = get_abi(contract_address)\n",
    "        return provider.eth.contract(address=contract_address, abi=abi)\n",
    "\n",
    "    except ABINotVerified:\n",
    "        logging.info(\n",
    "            f\"Contract {contract_address[:10]} not verified, using minimal ABI\"\n",
    "        )\n",
    "\n",
    "        if contract_type == \"erc20\":\n",
    "            return provider.eth.contract(\n",
    "                address=contract_address, abi=MINIMAL_ERC20_ABI\n",
    "            )\n",
    "        elif contract_type == \"uniswap_v3_pool\":\n",
    "            return provider.eth.contract(\n",
    "                address=contract_address, abi=MINIMAL_UNISWAP_V3_POOL_ABI\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"No fallback ABI for type: {contract_type}\")\n",
    "\n",
    "    except ABIRateLimited as e:\n",
    "        logging.warning(f\"Rate limited, cannot fetch ABI: {e}\")\n",
    "        raise\n",
    "\n",
    "    except (ABINetworkError, ABIFetchError) as e:\n",
    "        logging.error(f\"Cannot get contract {contract_address[:10]}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def validate_abi_has_functions(abi, required_functions):\n",
    "    if not abi or not isinstance(abi, list):\n",
    "        return False, \"Invalid ABI structure\"\n",
    "\n",
    "    available_functions = [\n",
    "        item.get(\"name\") for item in abi if item.get(\"type\") == \"function\"\n",
    "    ]\n",
    "\n",
    "    missing = [fn for fn in required_functions if fn not in available_functions]\n",
    "\n",
    "    if missing:\n",
    "        return False, f\"Missing functions: {missing}\"\n",
    "\n",
    "    return True, available_functions\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Helper: Convert event to dict\n",
    "# -----------------------\n",
    "def event_to_dict(event):\n",
    "    d = dict(event)\n",
    "    if \"args\" in d:\n",
    "        d[\"args\"] = dict(d[\"args\"])\n",
    "    if \"transactionHash\" in d:\n",
    "        d[\"transactionHash\"] = d[\"transactionHash\"].hex()\n",
    "    if \"blockHash\" in d:\n",
    "        d[\"blockHash\"] = d[\"blockHash\"].hex()\n",
    "    return d\n",
    "\n",
    "\n",
    "class Web3JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        # HexBytes ‚Üí hex string\n",
    "        if isinstance(obj, HexBytes):\n",
    "            return obj.hex()\n",
    "        # Peel off any other web3-specific types here as needed...\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# ETHERSCAN VERSION\n",
    "# Used to find at which block 1 contract has been deployed\n",
    "# Might be useful later, put it in JSON in the end\n",
    "# -----------------------\n",
    "def get_contract_creation_block_etherscan(\n",
    "    contract_address: str, etherscan_api_key: str\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Retrieves the contract creation block from Etherscan.\n",
    "    Returns the block number as an integer.\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        f\"https://api.etherscan.io/api?module=contract&action=getcontractcreation\"\n",
    "        f\"&contractaddresses={contract_address}&apikey={etherscan_api_key}\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if data.get(\"status\") == \"1\":\n",
    "        results = data.get(\"result\", [])\n",
    "        if results and len(results) > 0:\n",
    "            return int(results[0][\"blockNumber\"])\n",
    "        else:\n",
    "            raise Exception(\"No contract creation data found.\")\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Error fetching creation block: \" + data.get(\"result\", \"Unknown error\")\n",
    "        )\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Used to find at which block 1 contract has been deployed\n",
    "# Might be useful later, put it in JSON in the end\n",
    "# -----------------------\n",
    "def get_contract_creation_block_custom(start_block=0, end_block=100000):\n",
    "\n",
    "    def get_contract_deployments(start_block, end_block, max_workers=8):\n",
    "        deployments = []\n",
    "\n",
    "        def process_block(block_number):\n",
    "            block = w3.eth.get_block(block_number, full_transactions=True)\n",
    "            block_deployments = []\n",
    "            for tx in block.transactions:\n",
    "                if tx.to is None:\n",
    "                    try:\n",
    "                        receipt = w3.eth.get_transaction_receipt(tx.hash)\n",
    "                        contract_address = receipt.contractAddress\n",
    "                        if contract_address:\n",
    "                            block_deployments.append(\n",
    "                                {\n",
    "                                    \"block_number\": block_number,\n",
    "                                    \"contract_address\": contract_address,\n",
    "                                }\n",
    "                            )\n",
    "                    except:\n",
    "                        print(tx.hash)\n",
    "            return block_deployments\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_block = {\n",
    "                executor.submit(process_block, bn): bn\n",
    "                for bn in range(start_block, end_block + 1)\n",
    "            }\n",
    "            for future in as_completed(future_to_block):\n",
    "                block_deployments = future.result()\n",
    "                deployments.extend(block_deployments)\n",
    "\n",
    "        return deployments\n",
    "\n",
    "    deployments = get_contract_deployments(start_block, end_block)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    with open(\"contract_deployments.json\", \"w\") as f:\n",
    "        json.dump(deployments, f, indent=4)\n",
    "\n",
    "\n",
    "# -- Step 2: Reconstruct an Event‚Äôs Signature --\n",
    "def get_event_signature(event_name: str, abi: list) -> str:\n",
    "    \"\"\"\n",
    "    Given an event name and an ABI, find the event definition and reconstruct its signature.\n",
    "    For example, for event Transfer(address,address,uint256) this returns its keccak256 hash.\n",
    "    \"\"\"\n",
    "    from eth_utils import keccak, encode_hex\n",
    "\n",
    "    for item in abi:\n",
    "        if item.get(\"type\") == \"event\" and item.get(\"name\") == event_name:\n",
    "            # Build the signature string: \"Transfer(address,address,uint256)\"\n",
    "            types = \",\".join([inp[\"type\"] for inp in item.get(\"inputs\", [])])\n",
    "            signature = f\"{event_name}({types})\"\n",
    "            return encode_hex(keccak(text=signature))\n",
    "    raise ValueError(f\"Event {event_name} not found in ABI.\")\n",
    "\n",
    "\n",
    "def block_to_utc(block_number):\n",
    "    \"\"\"\n",
    "    Convert a block number into its UTC timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        w3 (Web3): A Web3 instance\n",
    "        block_number (int): The block number\n",
    "\n",
    "    Returns:\n",
    "        datetime: The block timestamp in UTC\n",
    "    \"\"\"\n",
    "    block = w3.eth.get_block(block_number)\n",
    "    timestamp = block[\"timestamp\"]\n",
    "    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "def read_and_sort_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file, each line being a JSON object with a field `blockNumber`,\n",
    "    and returns a list of those objects sorted by blockNumber (ascending).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                # Handle bad JSON if needed, e.g., log or skip\n",
    "                print(line)\n",
    "                print(f\"Skipping bad JSON line: {e}\")\n",
    "                continue\n",
    "            # Optionally, you could check that 'blockNumber' exists, is int, etc.\n",
    "            if \"blockNumber\" not in obj:\n",
    "                print(f\"Skipping line with no blockNumber: {obj}\")\n",
    "                continue\n",
    "            data.append(obj)\n",
    "    # Now sort by blockNumber ascending\n",
    "    # If blockNumber in file is already int, fine; else convert\n",
    "    sorted_data = sorted(data, key=lambda o: int(o[\"blockNumber\"]))\n",
    "    return sorted_data\n",
    "\n",
    "\n",
    "def get_address_abi_contract(contract_address, etherscan_api_key=ETHERSCAN_API_KEY):\n",
    "    address = w3.to_checksum_address(contract_address)\n",
    "    contract_abi = get_abi(address, etherscan_api_key)\n",
    "    contract = w3.eth.contract(address=contract_address, abi=contract_abi)\n",
    "\n",
    "    return address, contract_abi, contract\n",
    "\n",
    "\n",
    "# Find the amount of token depending on the contract at the very specific block_number\n",
    "# but it use ETHERSCAN API (to go further: explorer the reconstruct from all the Transfer event but slow)\n",
    "# Not super useful for the moment\n",
    "def get_erc20_balance_at_block(user_address, token_address, block_number):\n",
    "    \"\"\"\n",
    "    Query ERC-20 balance of an address at a specific block.\n",
    "\n",
    "    user_address = \"0xe2dFC8F41DB4169A24e7B44095b9E92E20Ed57eD\"\n",
    "    token_address = \"0x514910771AF9Ca656af840dff83E8264EcF986CA\"\n",
    "    block_number = 23405236\n",
    "    balance = get_erc20_balance_at_block(user_address, token_address, block_number)\n",
    "\n",
    "    Parameters:\n",
    "        user_address: string, account to check\n",
    "        token_address: Web3 contract instance for the ERC-20 token\n",
    "        block_number: int, historical block\n",
    "\n",
    "    Returns:\n",
    "        int: token balance\n",
    "        None if contract is a proxy\n",
    "    \"\"\"\n",
    "    token_address, token_abi, token_contract = get_address_abi_contract(token_address)\n",
    "    user_address = w3.to_checksum_address(user_address)\n",
    "    token_name = None\n",
    "    token_symbol = None\n",
    "    try:\n",
    "        token_name = token_contract.functions.name().call()\n",
    "        token_symbol = token_contract.functions.symbol().call()\n",
    "    except Exception as e:\n",
    "        print(f\"Error {e}\")\n",
    "        print(f\"{token_address}\")\n",
    "        return None\n",
    "    balance = token_contract.functions.balanceOf(user_address).call(\n",
    "        block_identifier=block_number\n",
    "    )\n",
    "    print(\n",
    "        f\"Address {user_address} had {w3.from_wei(balance, \"ether\")} of {token_symbol} at block {block_number}\"\n",
    "    )\n",
    "    return balance\n",
    "\n",
    "\n",
    "def get_token_name_by_contract(\n",
    "    token_address,\n",
    "    TOKEN_NAME_FILE=TOKEN_NAME_FILE,\n",
    "    proxy_address=None,\n",
    "    global_cache=GLOBAL_DICT_TOKEN_SYMBOL,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the token name for `token_address`, using a local JSON cache.\n",
    "    If not in cache, will call get_token_name_by_contract (your ABI/Web3 function),\n",
    "    store the result (or None) in the cache file, and return it.\n",
    "    \"\"\"\n",
    "    # 1. Load cache\n",
    "    cache = global_cache\n",
    "    # if os.path.exists(TOKEN_NAME_FILE):\n",
    "    #     try:\n",
    "    #         with open(TOKEN_NAME_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    #             cache = json.load(f)\n",
    "    #     except Exception as e:\n",
    "    #         # If file is corrupted, proceed with empty cache\n",
    "    #         print(f\"Warning: cannot read token name cache: {e}\")\n",
    "\n",
    "    # 2. Check cache\n",
    "    if token_address in cache:\n",
    "        return cache[token_address]\n",
    "\n",
    "    # Not in cache ‚Üí fetch from contract\n",
    "    name = None\n",
    "    symbol = None\n",
    "    address = None\n",
    "    try:\n",
    "        if proxy_address:\n",
    "            proxy_address, proxy_abi, proxy_contract = get_address_abi_contract(\n",
    "                proxy_address\n",
    "            )\n",
    "            token_address = proxy_contract.functions.getToken(token_address).call()\n",
    "        token_address, token_abi, token_contract = get_address_abi_contract(\n",
    "            token_address\n",
    "        )\n",
    "        # call name\n",
    "        name_raw = token_contract.functions.name().call()\n",
    "        symbol_raw = token_contract.functions.symbol().call()\n",
    "        address = token_contract.address\n",
    "        # Convert raw to str if needed\n",
    "        name = str(name_raw)\n",
    "        if isinstance(name_raw, (bytes, bytearray)):\n",
    "            name = name_raw.decode(\"utf-8\", errors=\"ignore\").rstrip(\"\\x00\")\n",
    "        symbol = str(symbol_raw)\n",
    "        if isinstance(symbol_raw, (bytes, bytearray)):\n",
    "            symbol = symbol_raw.decode(\"utf-8\", errors=\"ignore\").rstrip(\"\\x00\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching token name/symbol for {address}: {e}\")\n",
    "        if token_address:\n",
    "            cache[token_address] = {\n",
    "                \"name\": None,\n",
    "                \"symbol\": None,\n",
    "                \"address\": None,\n",
    "            }\n",
    "        try:\n",
    "            dirn = os.path.dirname(TOKEN_NAME_FILE) or \".\"\n",
    "            fd, tmp = tempfile.mkstemp(dir=dirn, text=True)\n",
    "            with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(cache, f, indent=2, ensure_ascii=False)\n",
    "            os.replace(tmp, TOKEN_NAME_FILE)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to save token cache: {e}\")\n",
    "        return {\n",
    "            \"name\": None,\n",
    "            \"symbol\": None,\n",
    "            \"address\": None,\n",
    "        }\n",
    "\n",
    "    # Update cache\n",
    "    cache[address] = {\n",
    "        \"name\": name,\n",
    "        \"symbol\": symbol,\n",
    "        \"address\": address,\n",
    "    }\n",
    "\n",
    "    # Write back atomically (overwrite)\n",
    "    try:\n",
    "        dirn = os.path.dirname(TOKEN_NAME_FILE) or \".\"\n",
    "        fd, tmp = tempfile.mkstemp(dir=dirn, text=True)\n",
    "        with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cache, f, indent=2, ensure_ascii=False)\n",
    "        os.replace(tmp, TOKEN_NAME_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: failed to save token cache: {e}\")\n",
    "\n",
    "    return cache[address]\n",
    "\n",
    "\n",
    "def decode_topics(log):\n",
    "    _, abi, contract = get_address_abi_contract(log[\"address\"])\n",
    "    # Try matching this log against the ABI events\n",
    "    for item in abi:\n",
    "        if item.get(\"type\") == \"event\":\n",
    "            event_signature = (\n",
    "                f'{item[\"name\"]}({\",\".join(i[\"type\"] for i in item[\"inputs\"])})'\n",
    "            )\n",
    "            event_hash = w3.keccak(text=event_signature).hex()\n",
    "\n",
    "            if log[\"topics\"][0].hex() == event_hash:\n",
    "                # Found matching event\n",
    "                decoded = contract.events[item[\"name\"]]().process_log(log)\n",
    "                return {\n",
    "                    \"event\": item[\"name\"],\n",
    "                    \"args\": dict(decoded[\"args\"]),\n",
    "                }\n",
    "\n",
    "    return {}  # no matching event in ABI\n",
    "\n",
    "\n",
    "def release_list(a):\n",
    "    del a[:]\n",
    "    del a\n",
    "\n",
    "\n",
    "def normalize_token_value(raw_value, decimals):\n",
    "    if decimals == 18:\n",
    "        return float(Web3.from_wei(raw_value, \"ether\"))\n",
    "    else:\n",
    "        return float(Decimal(raw_value) / Decimal(10**decimals))\n",
    "\n",
    "\n",
    "def inspect_contract_abi(contract_address, provider=None):\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    contract = get_contract_with_fallback(contract_address, provider)\n",
    "    abi = contract.abi\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CONTRACT: {contract_address}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Functions\n",
    "    functions = [item for item in abi if item.get(\"type\") == \"function\"]\n",
    "    if functions:\n",
    "        print(f\"\\nüìã FUNCTIONS ({len(functions)}):\")\n",
    "        for func in functions:\n",
    "            name = func.get(\"name\", \"unnamed\")\n",
    "            inputs = \", \".join(\n",
    "                [f\"{i['type']} {i.get('name', '')}\" for i in func.get(\"inputs\", [])]\n",
    "            )\n",
    "            outputs = \", \".join([o[\"type\"] for o in func.get(\"outputs\", [])])\n",
    "            state = func.get(\"stateMutability\", \"nonpayable\")\n",
    "            print(f\"  ‚Ä¢ {name}({inputs}) ‚Üí {outputs} [{state}]\")\n",
    "\n",
    "    # Events\n",
    "    events = [item for item in abi if item.get(\"type\") == \"event\"]\n",
    "    if events:\n",
    "        print(f\"\\nüì¢ EVENTS ({len(events)}):\")\n",
    "        for event in events:\n",
    "            name = event.get(\"name\", \"unnamed\")\n",
    "            inputs = \", \".join(\n",
    "                [f\"{i['type']} {i.get('name', '')}\" for i in event.get(\"inputs\", [])]\n",
    "            )\n",
    "            print(f\"  ‚Ä¢ {name}({inputs})\")\n",
    "\n",
    "    # Constructor\n",
    "    constructor = [item for item in abi if item.get(\"type\") == \"constructor\"]\n",
    "    if constructor:\n",
    "        print(f\"\\nüèóÔ∏è  CONSTRUCTOR:\")\n",
    "        for c in constructor:\n",
    "            inputs = \", \".join(\n",
    "                [f\"{i['type']} {i.get('name', '')}\" for i in c.get(\"inputs\", [])]\n",
    "            )\n",
    "            print(f\"  ‚Ä¢ constructor({inputs})\")\n",
    "\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "    return abi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c146680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_thread_local = threading.local()\n",
    "\n",
    "\n",
    "def get_thread_connection(db_path):\n",
    "    if not hasattr(_thread_local, \"conn\"):\n",
    "        _thread_local.conn = duckdb.connect(db_path)\n",
    "    return _thread_local.conn\n",
    "\n",
    "\n",
    "def setup_database(db_path=DB_PATH):\n",
    "    conn = duckdb.connect(db_path)\n",
    "    with open(\"./out/V3/database/schema.sql\", \"r\") as f:\n",
    "        schema_sql = f.read()\n",
    "    conn.execute(schema_sql)\n",
    "    conn.close()\n",
    "    logging.info(\"‚úì Database schema created successfully\")\n",
    "\n",
    "\n",
    "def batch_insert_events(events, db_path, worker_id=\"main\"):\n",
    "    if not events:\n",
    "        return 0\n",
    "\n",
    "    transfers = []\n",
    "    swaps = []\n",
    "    mints = []\n",
    "    burns = []\n",
    "    syncs = []\n",
    "    approvals = []\n",
    "\n",
    "    for e in events:\n",
    "        event_type = e.get(\"event\")\n",
    "        args = e.get(\"args\", {})\n",
    "\n",
    "        if event_type == \"Transfer\":\n",
    "            transfers.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"from\", \"\"),\n",
    "                    args.get(\"to\", \"\"),\n",
    "                    int(args.get(\"value\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Swap\":\n",
    "            swaps.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"sender\", \"\"),\n",
    "                    args.get(\"to\", \"\"),\n",
    "                    int(args.get(\"amount0In\", 0)),\n",
    "                    int(args.get(\"amount1In\", 0)),\n",
    "                    int(args.get(\"amount0Out\", 0)),\n",
    "                    int(args.get(\"amount1Out\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Mint\":\n",
    "            mints.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"sender\", \"\"),\n",
    "                    int(args.get(\"amount0\", 0)),\n",
    "                    int(args.get(\"amount1\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Burn\":\n",
    "            burns.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"sender\", \"\"),\n",
    "                    args.get(\"to\", \"\"),\n",
    "                    int(args.get(\"amount0\", 0)),\n",
    "                    int(args.get(\"amount1\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Sync\":\n",
    "            syncs.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    int(args.get(\"reserve0\", 0)),\n",
    "                    int(args.get(\"reserve1\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Approval\":\n",
    "            approvals.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"owner\", \"\"),\n",
    "                    args.get(\"spender\", \"\"),\n",
    "                    int(args.get(\"value\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    conn.execute(\"BEGIN TRANSACTION\")\n",
    "    try:\n",
    "        if transfers:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO transfer (transaction_hash, block_number, log_index, pair_address, \n",
    "                                     from_address, to_address, value)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                transfers,\n",
    "            )\n",
    "\n",
    "        if swaps:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO swap (transaction_hash, block_number, log_index, pair_address,\n",
    "                                 sender, to_address, amount0_in, amount1_in, amount0_out, amount1_out)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                swaps,\n",
    "            )\n",
    "\n",
    "        if mints:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO mint (transaction_hash, block_number, log_index, pair_address,\n",
    "                                 sender, amount0, amount1)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                mints,\n",
    "            )\n",
    "\n",
    "        if burns:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO burn (transaction_hash, block_number, log_index, pair_address,\n",
    "                                 sender, to_address, amount0, amount1)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                burns,\n",
    "            )\n",
    "\n",
    "        if syncs:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO sync (transaction_hash, block_number, log_index, pair_address,\n",
    "                                 reserve0, reserve1)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                syncs,\n",
    "            )\n",
    "\n",
    "        if approvals:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO approval (transaction_hash, block_number, log_index, pair_address,\n",
    "                                     owner, spender, value)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                approvals,\n",
    "            )\n",
    "\n",
    "        conn.execute(\"COMMIT\")\n",
    "\n",
    "        total = (\n",
    "            len(transfers)\n",
    "            + len(swaps)\n",
    "            + len(mints)\n",
    "            + len(burns)\n",
    "            + len(syncs)\n",
    "            + len(approvals)\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"[{worker_id}] Inserted {total} events (T:{len(transfers)} S:{len(swaps)} M:{len(mints)} B:{len(burns)} Sy:{len(syncs)} A:{len(approvals)})\"\n",
    "        )\n",
    "        return total\n",
    "\n",
    "    except Exception as e:\n",
    "        conn.execute(\"ROLLBACK\")\n",
    "        logging.error(f\"[{worker_id}] batch_insert_events failed: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def mark_range_completed(start_block, end_block, db_path, worker_id=\"main\"):\n",
    "    conn = get_thread_connection(db_path)\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO processing_state (start_block, end_block, status, worker_id, updated_at)\n",
    "        VALUES (?, ?, 'completed', ?, NOW())\n",
    "        ON CONFLICT (start_block, end_block) \n",
    "        DO UPDATE SET \n",
    "            status = 'completed', \n",
    "            worker_id = ?,\n",
    "            updated_at = NOW()\n",
    "    \"\"\",\n",
    "        (start_block, end_block, worker_id, worker_id),\n",
    "    )\n",
    "\n",
    "\n",
    "def mark_range_processing(start_block, end_block, db_path, worker_id=\"main\"):\n",
    "    conn = get_thread_connection(db_path)\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO processing_state (start_block, end_block, status, worker_id, updated_at)\n",
    "        VALUES (?, ?, 'processing', ?, NOW())\n",
    "        ON CONFLICT (start_block, end_block) \n",
    "        DO UPDATE SET \n",
    "            status = 'processing',\n",
    "            worker_id = ?,\n",
    "            updated_at = NOW()\n",
    "    \"\"\",\n",
    "        (start_block, end_block, worker_id, worker_id),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_completed_ranges(db_path):\n",
    "    conn = get_thread_connection(db_path)\n",
    "    result = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT start_block, end_block \n",
    "        FROM processing_state \n",
    "        WHERE status = 'completed'\n",
    "    \"\"\"\n",
    "    ).fetchall()\n",
    "    return set((r[0], r[1]) for r in result)\n",
    "\n",
    "\n",
    "def get_database_stats(db_path):\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    result = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            (SELECT COUNT(*) FROM transfer) as total_transfers,\n",
    "            (SELECT COUNT(*) FROM swap) as total_swaps,\n",
    "            (SELECT COUNT(*) FROM mint) as total_mints,\n",
    "            (SELECT COUNT(*) FROM burn) as total_burns,\n",
    "            (SELECT COUNT(*) FROM sync) as total_syncs,\n",
    "            (SELECT COUNT(*) FROM approval) as total_approvals,\n",
    "            (SELECT COUNT(*) FROM processing_state WHERE status = 'completed') as completed_ranges,\n",
    "            (SELECT COUNT(*) FROM pair_metadata) as total_pairs,\n",
    "            (SELECT COUNT(*) FROM block_metadata) as total_blocks\n",
    "    \"\"\"\n",
    "    ).fetchone()\n",
    "\n",
    "    return {\n",
    "        \"total_transfers\": result[0],\n",
    "        \"total_swaps\": result[1],\n",
    "        \"total_mints\": result[2],\n",
    "        \"total_burns\": result[3],\n",
    "        \"total_syncs\": result[4],\n",
    "        \"total_approvals\": result[5],\n",
    "        \"completed_ranges\": result[6],\n",
    "        \"total_pairs\": result[7],\n",
    "        \"total_blocks\": result[8],\n",
    "    }\n",
    "\n",
    "\n",
    "def insert_pair_metadata(\n",
    "    pair_address,\n",
    "    token0_address,\n",
    "    token1_address,\n",
    "    db_path,\n",
    "    token0_symbol=None,\n",
    "    token1_symbol=None,\n",
    "    token0_decimals=None,\n",
    "    token1_decimals=None,\n",
    "    created_block=None,\n",
    "):\n",
    "    conn = get_thread_connection(db_path)\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO pair_metadata (pair_address, token0_address, token1_address, token0_symbol, \n",
    "                                  token1_symbol, token0_decimals, token1_decimals, created_block)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT (pair_address) \n",
    "        DO UPDATE SET \n",
    "            token0_symbol = COALESCE(EXCLUDED.token0_symbol, pair_metadata.token0_symbol),\n",
    "            token1_symbol = COALESCE(EXCLUDED.token1_symbol, pair_metadata.token1_symbol),\n",
    "            token0_decimals = COALESCE(EXCLUDED.token0_decimals, pair_metadata.token0_decimals),\n",
    "            token1_decimals = COALESCE(EXCLUDED.token1_decimals, pair_metadata.token1_decimals),\n",
    "            last_updated = NOW()\n",
    "    \"\"\",\n",
    "        (\n",
    "            pair_address,\n",
    "            token0_address,\n",
    "            token1_address,\n",
    "            token0_symbol,\n",
    "            token1_symbol,\n",
    "            token0_decimals,\n",
    "            token1_decimals,\n",
    "            created_block,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_pair_metadata(pair_address, db_path):\n",
    "    conn = get_thread_connection(db_path)\n",
    "    result = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT token0_address, token1_address, token0_symbol, token1_symbol, \n",
    "               token0_decimals, token1_decimals, created_block\n",
    "        FROM pair_metadata\n",
    "        WHERE pair_address = ?\n",
    "    \"\"\",\n",
    "        (pair_address,),\n",
    "    ).fetchone()\n",
    "\n",
    "    if result:\n",
    "        return {\n",
    "            \"token0_address\": result[0],\n",
    "            \"token1_address\": result[1],\n",
    "            \"token0_symbol\": result[2],\n",
    "            \"token1_symbol\": result[3],\n",
    "            \"token0_decimals\": result[4],\n",
    "            \"token1_decimals\": result[5],\n",
    "            \"created_block\": result[6],\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def batch_insert_block_metadata(blocks_data, db_path):\n",
    "    if not blocks_data:\n",
    "        return 0\n",
    "\n",
    "    conn = get_thread_connection(db_path)\n",
    "    conn.executemany(\n",
    "        \"\"\"\n",
    "        INSERT INTO block_metadata (block_number, block_timestamp, block_hash)\n",
    "        VALUES (?, ?, ?)\n",
    "        ON CONFLICT (block_number) DO NOTHING\n",
    "    \"\"\",\n",
    "        blocks_data,\n",
    "    )\n",
    "    return len(blocks_data)\n",
    "\n",
    "\n",
    "def normalize_values_for_pair(pair_address, db_path):\n",
    "    metadata = get_pair_metadata(pair_address, db_path)\n",
    "    if (\n",
    "        not metadata\n",
    "        or metadata[\"token0_decimals\"] is None\n",
    "        or metadata[\"token1_decimals\"] is None\n",
    "    ):\n",
    "        logging.warning(f\"Cannot normalize values for {pair_address}: missing decimals\")\n",
    "        return\n",
    "\n",
    "    lp_decimals = 18\n",
    "    token0_decimals = metadata[\"token0_decimals\"]\n",
    "    token1_decimals = metadata[\"token1_decimals\"]\n",
    "\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        UPDATE transfer \n",
    "        SET value_normalized = value::DECIMAL / POWER(10, ?)\n",
    "        WHERE pair_address = ? AND value_normalized IS NULL\n",
    "    \"\"\",\n",
    "        (lp_decimals, pair_address),\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        UPDATE mint \n",
    "        SET amount0_normalized = amount0::DECIMAL / POWER(10, ?),\n",
    "            amount1_normalized = amount1::DECIMAL / POWER(10, ?)\n",
    "        WHERE pair_address = ? AND amount0_normalized IS NULL\n",
    "    \"\"\",\n",
    "        (token0_decimals, token1_decimals, pair_address),\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        UPDATE burn \n",
    "        SET amount0_normalized = amount0::DECIMAL / POWER(10, ?),\n",
    "            amount1_normalized = amount1::DECIMAL / POWER(10, ?)\n",
    "        WHERE pair_address = ? AND amount0_normalized IS NULL\n",
    "    \"\"\",\n",
    "        (token0_decimals, token1_decimals, pair_address),\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        UPDATE swap \n",
    "        SET amount0_in_normalized = amount0_in::DECIMAL / POWER(10, ?),\n",
    "            amount1_in_normalized = amount1_in::DECIMAL / POWER(10, ?),\n",
    "            amount0_out_normalized = amount0_out::DECIMAL / POWER(10, ?),\n",
    "            amount1_out_normalized = amount1_out::DECIMAL / POWER(10, ?)\n",
    "        WHERE pair_address = ? AND amount0_in_normalized IS NULL\n",
    "    \"\"\",\n",
    "        (\n",
    "            token0_decimals,\n",
    "            token1_decimals,\n",
    "            token0_decimals,\n",
    "            token1_decimals,\n",
    "            pair_address,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        UPDATE sync \n",
    "        SET reserve0_normalized = reserve0::DECIMAL / POWER(10, ?),\n",
    "            reserve1_normalized = reserve1::DECIMAL / POWER(10, ?)\n",
    "        WHERE pair_address = ? AND reserve0_normalized IS NULL\n",
    "    \"\"\",\n",
    "        (token0_decimals, token1_decimals, pair_address),\n",
    "    )\n",
    "\n",
    "    logging.info(f\"‚úì Normalized values for pair {pair_address}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7ae9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_uniswap_v3_pool(\n",
    "    pool_address, provider=None, factory_address=UNISWAP_V3_FACTORY\n",
    "):\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    pool_address = provider.to_checksum_address(pool_address)\n",
    "\n",
    "    # Layer 1: Check if address has code\n",
    "    code = provider.eth.get_code(pool_address)\n",
    "    if code == b\"\" or code == \"0x\":\n",
    "        return False, \"Address has no contract code\"\n",
    "\n",
    "    # Layer 2: Check if contract has required functions\n",
    "    required_functions = [\"factory\", \"token0\", \"token1\", \"fee\"]\n",
    "    abi = get_abi(pool_address)\n",
    "\n",
    "    if abi is None:\n",
    "        return False, \"Could not retrieve ABI\"\n",
    "\n",
    "    function_names = [\n",
    "        item.get(\"name\") for item in abi if item.get(\"type\") == \"function\"\n",
    "    ]\n",
    "\n",
    "    missing_functions = [fn for fn in required_functions if fn not in function_names]\n",
    "    if missing_functions:\n",
    "        return False, f\"Missing required functions: {missing_functions}\"\n",
    "\n",
    "    # Layer 3: Verify factory deployed this pool\n",
    "    try:\n",
    "        pool_contract = provider.eth.contract(address=pool_address, abi=abi)\n",
    "\n",
    "        reported_factory = pool_contract.functions.factory().call()\n",
    "        token0 = pool_contract.functions.token0().call()\n",
    "        token1 = pool_contract.functions.token1().call()\n",
    "        fee = pool_contract.functions.fee().call()\n",
    "\n",
    "        # Cross-check with factory\n",
    "        factory_contract = get_contract_with_fallback(\n",
    "            factory_address, provider, contract_type=\"erc20\"\n",
    "        )\n",
    "        expected_pool = factory_contract.functions.getPool(token0, token1, fee).call()\n",
    "\n",
    "        if expected_pool.lower() != pool_address.lower():\n",
    "            return (\n",
    "                False,\n",
    "                f\"Factory verification failed: expected {expected_pool}, got {pool_address}\",\n",
    "            )\n",
    "\n",
    "        if reported_factory.lower() != factory_address.lower():\n",
    "            return False, f\"Pool reports wrong factory: {reported_factory}\"\n",
    "\n",
    "        return True, {\"token0\": token0, \"token1\": token1, \"fee\": fee}\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"Validation call failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def fetch_uniswap_pair_metadata(\n",
    "    pair_address, provider=None, retry_count=0, max_retries=3\n",
    "):\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    pair_address = provider.to_checksum_address(pair_address)\n",
    "\n",
    "    try:\n",
    "        # Get pool contract with fallback\n",
    "        pair_contract = get_contract_with_fallback(\n",
    "            pair_address, provider, contract_type=\"uniswap_v3_pool\"\n",
    "        )\n",
    "\n",
    "        # Fetch token addresses\n",
    "        token0_address = pair_contract.functions.token0().call()\n",
    "        token1_address = pair_contract.functions.token1().call()\n",
    "\n",
    "        # Get token contracts with fallback to minimal ERC20 ABI\n",
    "        token0_contract = get_contract_with_fallback(\n",
    "            token0_address, provider, contract_type=\"erc20\"\n",
    "        )\n",
    "        token1_contract = get_contract_with_fallback(\n",
    "            token1_address, provider, contract_type=\"erc20\"\n",
    "        )\n",
    "\n",
    "        # Fetch metadata with error handling\n",
    "        metadata = {\n",
    "            \"pair_address\": pair_address,\n",
    "            \"token0_address\": token0_address,\n",
    "            \"token1_address\": token1_address,\n",
    "        }\n",
    "\n",
    "        # Try to get symbol/decimals, handle failures gracefully\n",
    "        try:\n",
    "            metadata[\"token0_symbol\"] = token0_contract.functions.symbol().call()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Cannot get symbol for token0 {token0_address[:10]}: {e}\")\n",
    "            metadata[\"token0_symbol\"] = None\n",
    "\n",
    "        try:\n",
    "            metadata[\"token0_decimals\"] = token0_contract.functions.decimals().call()\n",
    "        except Exception as e:\n",
    "            logging.warning(\n",
    "                f\"Cannot get decimals for token0 {token0_address[:10]}: {e}\"\n",
    "            )\n",
    "            metadata[\"token0_decimals\"] = None\n",
    "\n",
    "        try:\n",
    "            metadata[\"token1_symbol\"] = token1_contract.functions.symbol().call()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Cannot get symbol for token1 {token1_address[:10]}: {e}\")\n",
    "            metadata[\"token1_symbol\"] = None\n",
    "\n",
    "        try:\n",
    "            metadata[\"token1_decimals\"] = token1_contract.functions.decimals().call()\n",
    "        except Exception as e:\n",
    "            logging.warning(\n",
    "                f\"Cannot get decimals for token1 {token1_address[:10]}: {e}\"\n",
    "            )\n",
    "            metadata[\"token1_decimals\"] = None\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    except ABIRateLimited as e:\n",
    "        # Should retry with backoff\n",
    "        if retry_count < max_retries:\n",
    "            wait_time = min(2**retry_count, 60)  # Cap at 60s\n",
    "            logging.warning(\n",
    "                f\"Rate limited for {pair_address[:10]}, waiting {wait_time}s...\"\n",
    "            )\n",
    "            time.sleep(wait_time)\n",
    "            provider, _ = PROVIDER_POOL.get_provider()  # Switch provider\n",
    "            return fetch_uniswap_pair_metadata(\n",
    "                pair_address, provider, retry_count + 1, max_retries\n",
    "            )\n",
    "        else:\n",
    "            logging.error(f\"Max retries for rate limit on {pair_address[:10]}\")\n",
    "            return None\n",
    "\n",
    "    except ABINotVerified as e:\n",
    "        # Permanent failure - contract not verified\n",
    "        logging.error(f\"Cannot fetch metadata for {pair_address[:10]}: {e}\")\n",
    "        return None\n",
    "\n",
    "    except (ABINetworkError, ABIFetchError) as e:\n",
    "        # Transient error - should retry\n",
    "        if retry_count < max_retries:\n",
    "            wait_time = 2**retry_count\n",
    "            logging.warning(\n",
    "                f\"Network error for {pair_address[:10]}, retrying in {wait_time}s...\"\n",
    "            )\n",
    "            time.sleep(wait_time)\n",
    "            provider, _ = PROVIDER_POOL.get_provider()\n",
    "            return fetch_uniswap_pair_metadata(\n",
    "                pair_address, provider, retry_count + 1, max_retries\n",
    "            )\n",
    "        else:\n",
    "            logging.error(f\"Failed to fetch metadata after {max_retries} retries: {e}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Unexpected error fetching metadata for {pair_address[:10]}: {e}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_block_metadata(block_number, provider=None, retry_count=0, max_retries=3):\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    try:\n",
    "        block = provider.eth.get_block(block_number)\n",
    "        return (block_number, block[\"timestamp\"], block[\"hash\"].hex())\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 429:\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = 2**retry_count\n",
    "                logging.warning(\n",
    "                    f\"Rate limit (429) for block {block_number}, waiting {wait_time}s...\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                provider, _ = PROVIDER_POOL.get_provider()\n",
    "                return fetch_block_metadata(\n",
    "                    block_number, provider, retry_count + 1, max_retries\n",
    "                )\n",
    "            else:\n",
    "                raise Exception(f\"Max retries exceeded for block {block_number}\")\n",
    "\n",
    "        elif e.response.status_code == 402:\n",
    "            raise Exception(f\"Payment required (402) - Infura credits exhausted\")\n",
    "\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"HTTP {e.response.status_code} for block {block_number}: {e}\"\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        if retry_count < max_retries:\n",
    "            logging.warning(f\"Timeout for block {block_number}, retrying...\")\n",
    "            provider, _ = PROVIDER_POOL.get_provider()\n",
    "            return fetch_block_metadata(\n",
    "                block_number, provider, retry_count + 1, max_retries\n",
    "            )\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"Timeout after {max_retries} retries for block {block_number}\"\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        if retry_count < max_retries:\n",
    "            logging.warning(f\"Connection error for block {block_number}, retrying...\")\n",
    "            provider, _ = PROVIDER_POOL.get_provider()\n",
    "            return fetch_block_metadata(\n",
    "                block_number, provider, retry_count + 1, max_retries\n",
    "            )\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"Connection failed after {max_retries} retries for block {block_number}\"\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error fetching block {block_number}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_and_store_uniswap_pair_metadata(\n",
    "    pair_address, db_path, created_block=None, provider=None\n",
    "):\n",
    "\n",
    "    # CHECK DB FIRST (addresses Problem #7 too)\n",
    "    existing = get_pair_metadata(pair_address, db_path)\n",
    "    if existing and existing[\"token0_decimals\"] is not None:\n",
    "        logging.debug(f\"Using cached metadata for {pair_address[:10]}\")\n",
    "        return existing\n",
    "\n",
    "    # VALIDATE BEFORE FETCHING\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    is_valid, validation_result = validate_uniswap_v3_pool(pair_address, provider)\n",
    "\n",
    "    if not is_valid:\n",
    "        logging.error(f\"Skipping invalid pool {pair_address}: {validation_result}\")\n",
    "        # Store NULL in DB to avoid re-checking\n",
    "        insert_pair_metadata(\n",
    "            pair_address=pair_address,\n",
    "            token0_address=None,\n",
    "            token1_address=None,\n",
    "            db_path=db_path,\n",
    "            created_block=created_block,\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Fetch full metadata\n",
    "    metadata = fetch_uniswap_pair_metadata(pair_address, provider)\n",
    "\n",
    "    if metadata:\n",
    "        insert_pair_metadata(\n",
    "            pair_address=metadata[\"pair_address\"],\n",
    "            token0_address=metadata[\"token0_address\"],\n",
    "            token1_address=metadata[\"token1_address\"],\n",
    "            db_path=db_path,\n",
    "            token0_symbol=metadata[\"token0_symbol\"],\n",
    "            token1_symbol=metadata[\"token1_symbol\"],\n",
    "            token0_decimals=metadata[\"token0_decimals\"],\n",
    "            token1_decimals=metadata[\"token1_decimals\"],\n",
    "            created_block=created_block,\n",
    "        )\n",
    "        return metadata\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_and_store_block_metadata(\n",
    "    block_numbers, db_path, provider=None, max_workers=8\n",
    "):\n",
    "    provider_name = \"provided\"\n",
    "    if provider is None:\n",
    "        provider, provider_name = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    blocks_data = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_block = {\n",
    "            executor.submit(fetch_block_metadata, block_num, provider): block_num\n",
    "            for block_num in block_numbers\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_block):\n",
    "            block_num = future_to_block[future]\n",
    "            try:\n",
    "                block_data = future.result()\n",
    "                if block_data:\n",
    "                    blocks_data.append(block_data)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to fetch block {block_num}: {e}\")\n",
    "\n",
    "    if blocks_data:\n",
    "        batch_insert_block_metadata(blocks_data, db_path)\n",
    "        logging.info(\n",
    "            f\"‚úì Stored metadata for {len(blocks_data)} blocks using {provider_name}\"\n",
    "        )\n",
    "\n",
    "    return len(blocks_data)\n",
    "\n",
    "\n",
    "def collect_missing_pair_metadata(db_path, batch_size=50, provider=None, max_workers=4):\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    all_pairs = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT DISTINCT pair_address FROM transfer\n",
    "    \"\"\"\n",
    "    ).fetchall()\n",
    "    all_pairs = [r[0] for r in all_pairs]\n",
    "\n",
    "    existing_pairs = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT pair_address FROM pair_metadata\n",
    "        WHERE token0_decimals IS NOT NULL AND token1_decimals IS NOT NULL\n",
    "    \"\"\"\n",
    "    ).fetchall()\n",
    "    existing_pairs = set(r[0] for r in existing_pairs)\n",
    "\n",
    "    missing_pairs = [p for p in all_pairs if p not in existing_pairs]\n",
    "\n",
    "    if not missing_pairs:\n",
    "        logging.info(\"‚úì All pairs already have metadata\")\n",
    "        return\n",
    "\n",
    "    logging.info(\n",
    "        f\"Found {len(missing_pairs)} pairs missing metadata out of {len(all_pairs)} total\"\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(missing_pairs), batch_size):\n",
    "        batch = missing_pairs[i : i + batch_size]\n",
    "        logging.info(\n",
    "            f\"Processing batch {i // batch_size + 1}/{(len(missing_pairs) + batch_size - 1) // batch_size}\"\n",
    "        )\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_pair = {\n",
    "                executor.submit(\n",
    "                    fetch_and_store_uniswap_pair_metadata, pair, db_path, None, provider\n",
    "                ): pair\n",
    "                for pair in batch\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_pair):\n",
    "                pair = future_to_pair[future]\n",
    "                try:\n",
    "                    metadata = future.result()\n",
    "                    if metadata:\n",
    "                        logging.info(\n",
    "                            f\"‚úì {metadata['token0_symbol']}/{metadata['token1_symbol']} - {pair[:10]}...\"\n",
    "                        )\n",
    "                    else:\n",
    "                        logging.warning(f\"‚úó Failed: {pair[:10]}...\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"‚úó Error for {pair[:10]}: {e}\")\n",
    "\n",
    "    logging.info(\"‚úì Metadata collection complete\")\n",
    "\n",
    "\n",
    "def normalize_missing_pairs(db_path, max_workers=4):\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    pairs_with_metadata = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT pair_address, token0_decimals, token1_decimals\n",
    "        FROM pair_metadata\n",
    "        WHERE token0_decimals IS NOT NULL AND token1_decimals IS NOT NULL\n",
    "    \"\"\"\n",
    "    ).fetchall()\n",
    "\n",
    "    if not pairs_with_metadata:\n",
    "        logging.warning(\n",
    "            \"No pairs with metadata found - run collect_missing_pair_metadata() first\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    pairs_to_normalize = []\n",
    "    for pair_address, token0_dec, token1_dec in pairs_with_metadata:\n",
    "        needs_norm = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT COUNT(*) FROM transfer\n",
    "            WHERE pair_address = ? AND value_normalized IS NULL\n",
    "            LIMIT 1\n",
    "        \"\"\",\n",
    "            (pair_address,),\n",
    "        ).fetchone()[0]\n",
    "\n",
    "        if needs_norm > 0:\n",
    "            pairs_to_normalize.append(pair_address)\n",
    "\n",
    "    if not pairs_to_normalize:\n",
    "        logging.info(\"‚úì All pairs already normalized\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Normalizing {len(pairs_to_normalize)} pairs...\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_pair = {\n",
    "            executor.submit(normalize_values_for_pair, pair, db_path): pair\n",
    "            for pair in pairs_to_normalize\n",
    "        }\n",
    "\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_pair):\n",
    "            pair = future_to_pair[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                completed += 1\n",
    "                if completed % 10 == 0 or completed == len(pairs_to_normalize):\n",
    "                    logging.info(\n",
    "                        f\"Progress: {completed}/{len(pairs_to_normalize)} pairs normalized\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to normalize {pair[:10]}: {e}\")\n",
    "\n",
    "    logging.info(\"‚úì Normalization complete\")\n",
    "\n",
    "\n",
    "def populate_block_metadata_for_range(\n",
    "    start_block, end_block, db_path, batch_size=1000, provider=None, max_workers=8\n",
    "):\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    existing_blocks = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT block_number FROM block_metadata\n",
    "        WHERE block_number BETWEEN ? AND ?\n",
    "    \"\"\",\n",
    "        (start_block, end_block),\n",
    "    ).fetchall()\n",
    "    existing_blocks = {b[0] for b in existing_blocks}\n",
    "\n",
    "    all_blocks = set(range(start_block, end_block + 1))\n",
    "    missing_blocks = sorted(all_blocks - existing_blocks)\n",
    "\n",
    "    if not missing_blocks:\n",
    "        logging.info(\"‚úì All blocks already have metadata\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Need to fetch {len(missing_blocks)} block timestamps\")\n",
    "\n",
    "    for i in range(0, len(missing_blocks), batch_size):\n",
    "        batch = missing_blocks[i : i + batch_size]\n",
    "        fetch_and_store_block_metadata(batch, db_path, provider, max_workers)\n",
    "        logging.info(\n",
    "            f\"Progress: {min(i + batch_size, len(missing_blocks))}/{len(missing_blocks)} blocks processed\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bfaa061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced scanning functions loaded\n"
     ]
    }
   ],
   "source": [
    "def get_all_unique_pairs_from_db(db_path=DB_PATH):\n",
    "    conn = get_thread_connection(db_path)\n",
    "    result = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT DISTINCT pair_address \n",
    "        FROM transfer\n",
    "        WHERE pair_address IS NOT NULL\n",
    "        \"\"\"\n",
    "    ).fetchall()\n",
    "    return [r[0] for r in result]\n",
    "\n",
    "\n",
    "def fetch_logs_for_range(\n",
    "    start_block, end_block, addresses, worker_id=\"main\", retry_count=0, max_retries=5\n",
    "):\n",
    "    provider, provider_name = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    try:\n",
    "        params = {\n",
    "            \"fromBlock\": start_block,\n",
    "            \"toBlock\": end_block,\n",
    "            \"address\": addresses,\n",
    "        }\n",
    "\n",
    "        logs = provider.eth.get_logs(params)\n",
    "\n",
    "        transactions = []\n",
    "        for log in logs:\n",
    "            transaction = {\n",
    "                \"transactionHash\": provider.to_hex(log[\"transactionHash\"]),\n",
    "                \"blockNumber\": log[\"blockNumber\"],\n",
    "                \"logIndex\": log.get(\"logIndex\", 0),\n",
    "                \"address\": log[\"address\"],\n",
    "                \"data\": provider.to_hex(log[\"data\"]),\n",
    "            }\n",
    "\n",
    "            topics = decode_topics(log)\n",
    "            transaction.update(topics)\n",
    "\n",
    "            transaction[\"eventSignature\"] = \"\"\n",
    "            if log.get(\"topics\") and len(log[\"topics\"]) > 0:\n",
    "                transaction[\"eventSignature\"] = provider.to_hex(log[\"topics\"][0])\n",
    "\n",
    "            transactions.append(transaction)\n",
    "\n",
    "        logging.info(\n",
    "            f\"[{worker_id}] [{provider_name}] Fetched {len(transactions)} events from blocks [{start_block:,}, {end_block:,}]\"\n",
    "        )\n",
    "        return transactions\n",
    "\n",
    "    except HTTPError as e:\n",
    "        if e.response.status_code == 413:\n",
    "            # Response too large - need to split the range\n",
    "            logging.warning(\n",
    "                f\"[{worker_id}] [{provider_name}] Response too large (413) for range [{start_block:,}, {end_block:,}] - will split\"\n",
    "            )\n",
    "            raise Web3RPCError(\"Response payload too large - splitting range\")\n",
    "\n",
    "        elif e.response.status_code == 429:\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = 2**retry_count\n",
    "                logging.warning(\n",
    "                    f\"[{worker_id}] [{provider_name}] Rate limit hit, waiting {wait_time}s...\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                return fetch_logs_for_range(\n",
    "                    start_block,\n",
    "                    end_block,\n",
    "                    addresses,\n",
    "                    worker_id,\n",
    "                    retry_count + 1,\n",
    "                    max_retries,\n",
    "                )\n",
    "            else:\n",
    "                logging.error(f\"[{worker_id}] Max retries reached\")\n",
    "                raise\n",
    "\n",
    "        elif e.response.status_code == 402:\n",
    "            logging.critical(f\"[{worker_id}] Payment required (402)\")\n",
    "            raise\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"[{worker_id}] HTTP error {e.response.status_code}: {e}\")\n",
    "            raise\n",
    "\n",
    "    except Web3RPCError as e:\n",
    "        if (\n",
    "            \"more than 10000 results\" in str(e)\n",
    "            or \"-32005\" in str(e)\n",
    "            or \"Response payload too large\" in str(e)\n",
    "        ):\n",
    "            raise\n",
    "        else:\n",
    "            logging.error(f\"[{worker_id}] Web3 RPC error: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def collect_block_metadata_for_range(start_block, end_block, worker_id=\"main\"):\n",
    "    conn = get_thread_connection(DB_PATH)\n",
    "\n",
    "    existing_blocks = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT block_number FROM block_metadata \n",
    "        WHERE block_number BETWEEN ? AND ?\n",
    "        \"\"\",\n",
    "        (start_block, end_block),\n",
    "    ).fetchall()\n",
    "    existing_blocks = {b[0] for b in existing_blocks}\n",
    "\n",
    "    missing_blocks = [\n",
    "        b for b in range(start_block, end_block + 1) if b not in existing_blocks\n",
    "    ]\n",
    "    if not missing_blocks:\n",
    "        return 0\n",
    "\n",
    "    provider, provider_name = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    blocks_data = []\n",
    "    for block_num in missing_blocks:\n",
    "        try:\n",
    "            block = provider.eth.get_block(block_num)\n",
    "            blocks_data.append((block_num, block[\"timestamp\"], block[\"hash\"].hex()))\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[{worker_id}] Failed to fetch block {block_num}: {e}\")\n",
    "\n",
    "    if blocks_data:\n",
    "        batch_insert_block_metadata(blocks_data, DB_PATH)\n",
    "        logging.debug(f\"[{worker_id}] Stored metadata for {len(blocks_data)} blocks\")\n",
    "\n",
    "    return len(blocks_data)\n",
    "\n",
    "\n",
    "def collect_pair_metadata_from_events(events, worker_id=\"main\"):\n",
    "    unique_pairs = set(e[\"address\"] for e in events if \"address\" in e)\n",
    "\n",
    "    for pair_address in unique_pairs:\n",
    "        existing = get_pair_metadata(pair_address, DB_PATH)\n",
    "        if existing is None:\n",
    "            try:\n",
    "                metadata = fetch_and_store_uniswap_pair_metadata(pair_address, DB_PATH)\n",
    "                if metadata:\n",
    "                    logging.debug(\n",
    "                        f\"[{worker_id}] Stored metadata for {metadata['token0_symbol']}/{metadata['token1_symbol']}\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                logging.warning(\n",
    "                    f\"[{worker_id}] Failed to fetch metadata for {pair_address}: {e}\"\n",
    "                )\n",
    "\n",
    "\n",
    "def process_block_range(\n",
    "    start_block, end_block, addresses, worker_id=\"main\", collect_metadata=False\n",
    "):\n",
    "    if (start_block, end_block) in get_completed_ranges(DB_PATH):\n",
    "        logging.debug(\n",
    "            f\"[{worker_id}] Skipping already processed range [{start_block:,}, {end_block:,}]\"\n",
    "        )\n",
    "        return 0\n",
    "\n",
    "    mark_range_processing(start_block, end_block, DB_PATH, worker_id)\n",
    "\n",
    "    try:\n",
    "        events = fetch_logs_for_range(start_block, end_block, addresses, worker_id)\n",
    "\n",
    "        batch_insert_events(events, DB_PATH, worker_id)\n",
    "\n",
    "        if collect_metadata and events:\n",
    "            try:\n",
    "                collect_block_metadata_for_range(start_block, end_block, worker_id)\n",
    "                collect_pair_metadata_from_events(events, worker_id)\n",
    "            except Exception as e:\n",
    "                logging.warning(\n",
    "                    f\"[{worker_id}] Metadata collection failed (non-fatal): {e}\"\n",
    "                )\n",
    "\n",
    "        mark_range_completed(start_block, end_block, DB_PATH, worker_id)\n",
    "\n",
    "        logging.debug(\n",
    "            f\"[{worker_id}] ‚úì Processed [{start_block:,}, {end_block:,}] - {len(events)} events\"\n",
    "        )\n",
    "        return len(events)\n",
    "\n",
    "    except (Web3RPCError, HTTPError) as e:\n",
    "        # Check if we need to split (too many results OR response too large)\n",
    "        if (\n",
    "            \"more than 10000 results\" in str(e)\n",
    "            or \"-32005\" in str(e)\n",
    "            or \"Response payload too large\" in str(e)\n",
    "            or (hasattr(e, \"response\") and e.response.status_code == 413)\n",
    "        ):\n",
    "\n",
    "            mid = (start_block + end_block) // 2\n",
    "\n",
    "            if mid == start_block:\n",
    "                logging.error(\n",
    "                    f\"[{worker_id}] Cannot split range [{start_block:,}, {end_block:,}] further - skipping\"\n",
    "                )\n",
    "                mark_range_completed(start_block, end_block, DB_PATH, worker_id)\n",
    "                return 0\n",
    "\n",
    "            logging.info(\n",
    "                f\"[{worker_id}] Splitting [{start_block:,}, {end_block:,}] at {mid:,} (reason: {type(e).__name__})\"\n",
    "            )\n",
    "\n",
    "            count1 = process_block_range(\n",
    "                start_block, mid, addresses, worker_id, collect_metadata\n",
    "            )\n",
    "\n",
    "            count2 = process_block_range(\n",
    "                mid + 1, end_block, addresses, worker_id, collect_metadata\n",
    "            )\n",
    "\n",
    "            return count1 + count2\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"[{worker_id}] Failed to process [{start_block:,}, {end_block:,}]: {e}\"\n",
    "            )\n",
    "            return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{worker_id}] Unexpected error: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return 0\n",
    "\n",
    "\n",
    "def generate_block_ranges(start_block, end_block, chunk_size):\n",
    "    completed = get_completed_ranges(DB_PATH)\n",
    "\n",
    "    ranges = []\n",
    "    current = start_block\n",
    "\n",
    "    while current <= end_block:\n",
    "        end = min(current + chunk_size - 1, end_block)\n",
    "\n",
    "        if (current, end) not in completed:\n",
    "            ranges.append((current, end))\n",
    "\n",
    "        current = end + 1\n",
    "\n",
    "    return ranges\n",
    "\n",
    "\n",
    "def scan_blockchain(\n",
    "    addresses,\n",
    "    start_block,\n",
    "    end_block,\n",
    "    chunk_size=10000,\n",
    "    max_workers=3,\n",
    "    collect_metadata=False,\n",
    "):\n",
    "    ranges = generate_block_ranges(start_block, end_block, chunk_size)\n",
    "\n",
    "    if not ranges:\n",
    "        logging.info(\"No ranges to process - all already completed!\")\n",
    "        return\n",
    "\n",
    "    total_ranges = len(ranges)\n",
    "    logging.info(f\"Processing {total_ranges} block ranges with {max_workers} workers\")\n",
    "\n",
    "    total_events = 0\n",
    "    completed_ranges = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_range = {\n",
    "            executor.submit(\n",
    "                process_block_range,\n",
    "                start,\n",
    "                end,\n",
    "                addresses,\n",
    "                f\"worker-{i % max_workers}\",\n",
    "                collect_metadata,\n",
    "            ): (start, end, i)\n",
    "            for i, (start, end) in enumerate(ranges)\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_range):\n",
    "            start, end, idx = future_to_range[future]\n",
    "\n",
    "            try:\n",
    "                event_count = future.result()\n",
    "                total_events += event_count\n",
    "                completed_ranges += 1\n",
    "\n",
    "                progress = (completed_ranges / total_ranges) * 100\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = completed_ranges / elapsed if elapsed > 0 else 0\n",
    "                eta_seconds = (\n",
    "                    (total_ranges - completed_ranges) / rate if rate > 0 else 0\n",
    "                )\n",
    "                eta_str = f\"{int(eta_seconds // 60)}m {int(eta_seconds % 60)}s\"\n",
    "\n",
    "                logging.info(\n",
    "                    f\"Progress: {completed_ranges}/{total_ranges} ({progress:.1f}%) | \"\n",
    "                    f\"Events: {total_events:,} | \"\n",
    "                    f\"Rate: {rate:.1f} ranges/s | \"\n",
    "                    f\"ETA: {eta_str}\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Range [{start:,}, {end:,}] failed: {e}\")\n",
    "\n",
    "    elapsed_total = time.time() - start_time\n",
    "    logging.info(f\"\\n{'='*60}\")\n",
    "    logging.info(f\"Scan completed!\")\n",
    "    logging.info(f\"Total events fetched: {total_events:,}\")\n",
    "    logging.info(f\"Ranges processed: {completed_ranges}/{total_ranges}\")\n",
    "    logging.info(f\"Total time: {int(elapsed_total // 60)}m {int(elapsed_total % 60)}s\")\n",
    "    logging.info(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "def post_process_normalization():\n",
    "    logging.info(\"Starting post-processing normalization...\")\n",
    "\n",
    "    pairs = get_all_unique_pairs_from_db(DB_PATH)\n",
    "    logging.info(f\"Found {len(pairs)} unique pairs\")\n",
    "\n",
    "    for idx, pair_address in enumerate(pairs):\n",
    "        logging.info(f\"[{idx+1}/{len(pairs)}] Processing {pair_address}\")\n",
    "\n",
    "        metadata = get_pair_metadata(pair_address, DB_PATH)\n",
    "        if metadata is None:\n",
    "            logging.info(f\"  Fetching metadata...\")\n",
    "            metadata = fetch_and_store_uniswap_pair_metadata(pair_address, DB_PATH)\n",
    "\n",
    "        if metadata and metadata[\"token0_decimals\"] is not None:\n",
    "            logging.info(\n",
    "                f\"  Normalizing values for {metadata.get('token0_symbol', '?')}/{metadata.get('token1_symbol', '?')}...\"\n",
    "            )\n",
    "            normalize_values_for_pair(pair_address, DB_PATH)\n",
    "        else:\n",
    "            logging.warning(f\"  Cannot normalize - missing decimals\")\n",
    "\n",
    "    logging.info(\"‚úì Post-processing complete\")\n",
    "\n",
    "\n",
    "print(\"‚úì Enhanced scanning functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "462e09e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Main functions loaded\n"
     ]
    }
   ],
   "source": [
    "def scan_blockchain_to_duckdb(\n",
    "    event_file=V3_EVENT_BY_CONTRACTS,\n",
    "    db_path=DB_PATH,\n",
    "    start_block=10000001,\n",
    "    end_block=20000000,\n",
    "    chunk_size=10000,\n",
    "    max_workers=3,\n",
    "    token_filter=None,\n",
    "):\n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(\"BLOCKCHAIN SCANNER\")\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    with open(event_file, \"r\") as f:\n",
    "        all_pairs = json.load(f)\n",
    "\n",
    "    all_addresses = [Web3.to_checksum_address(addr) for addr in all_pairs.keys()]\n",
    "\n",
    "    if token_filter:\n",
    "        filter_checksummed = [Web3.to_checksum_address(addr) for addr in token_filter]\n",
    "        addresses = [addr for addr in all_addresses if addr in filter_checksummed]\n",
    "        logging.info(f\"Filtered: {len(addresses)}/{len(all_addresses)} addresses\")\n",
    "    else:\n",
    "        addresses = all_addresses\n",
    "        logging.info(f\"Total addresses: {len(addresses)}\")\n",
    "\n",
    "    setup_database(db_path)\n",
    "\n",
    "    stats = get_database_stats(db_path)\n",
    "    logging.info(\n",
    "        f\"Blocks: {start_block:,} ‚Üí {end_block:,} | Chunk: {chunk_size:,} | Workers: {max_workers}\"\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"DB: {stats['total_transfers']:,} transfers, {stats['total_swaps']:,} swaps, {stats['completed_ranges']} ranges done\"\n",
    "    )\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        scan_blockchain(\n",
    "            addresses, start_block, end_block, chunk_size, max_workers, False\n",
    "        )\n",
    "\n",
    "        final_stats = get_database_stats(db_path)\n",
    "        logging.info(\"=\" * 60)\n",
    "        logging.info(\"SCAN COMPLETE\")\n",
    "        logging.info(\n",
    "            f\"Transfers: {final_stats['total_transfers']:,} | Swaps: {final_stats['total_swaps']:,}\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Mints: {final_stats['total_mints']:,} | Burns: {final_stats['total_burns']:,}\"\n",
    "        )\n",
    "        logging.info(\"=\" * 60)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"\\nInterrupted - progress saved to database\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def scan_all_pairs_in_batches(\n",
    "    event_file=V3_EVENT_BY_CONTRACTS,\n",
    "    db_path=DB_PATH,\n",
    "    start_block=10000001,\n",
    "    end_block=20000000,\n",
    "    chunk_size=10000,\n",
    "    max_workers=3,\n",
    "    batch_size=100,\n",
    "):\n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(\"BATCH SCANNER\")\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    with open(event_file, \"r\") as f:\n",
    "        all_pairs = json.load(f)\n",
    "\n",
    "    all_addresses = list(all_pairs.keys())\n",
    "    total_pairs = len(all_addresses)\n",
    "    total_batches = (total_pairs + batch_size - 1) // batch_size\n",
    "\n",
    "    logging.info(\n",
    "        f\"Pairs: {total_pairs} | Batch size: {batch_size} | Batches: {total_batches}\"\n",
    "    )\n",
    "    logging.info(f\"Blocks: {start_block:,} ‚Üí {end_block:,}\")\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    for i in range(0, total_pairs, batch_size):\n",
    "        batch = all_addresses[i : i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "\n",
    "        logging.info(f\"\\nBatch {batch_num}/{total_batches} ({len(batch)} pairs)\")\n",
    "\n",
    "        try:\n",
    "            scan_blockchain_to_duckdb(\n",
    "                event_file=event_file,\n",
    "                db_path=db_path,\n",
    "                start_block=start_block,\n",
    "                end_block=end_block,\n",
    "                chunk_size=chunk_size,\n",
    "                max_workers=max_workers,\n",
    "                token_filter=batch,\n",
    "            )\n",
    "        except KeyboardInterrupt:\n",
    "            logging.warning(f\"Interrupted at batch {batch_num}/{total_batches}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Batch {batch_num} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "    final_stats = get_database_stats(db_path)\n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(\"ALL BATCHES COMPLETE\")\n",
    "    logging.info(\n",
    "        f\"Transfers: {final_stats['total_transfers']:,} | Swaps: {final_stats['total_swaps']:,}\"\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"Mints: {final_stats['total_mints']:,} | Burns: {final_stats['total_burns']:,}\"\n",
    "    )\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    logging.info(\"Collecting metadata...\")\n",
    "    collect_missing_pair_metadata(db_path)\n",
    "\n",
    "    logging.info(\"Normalizing values...\")\n",
    "    normalize_missing_pairs(db_path)\n",
    "\n",
    "    logging.info(\"‚úì Complete\")\n",
    "\n",
    "\n",
    "def query_database(db_path=DB_PATH):\n",
    "    conn = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"DATABASE QUERIES\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        print(\"\\nEvent counts:\")\n",
    "        print(\n",
    "            f\"  Transfers: {conn.execute('SELECT COUNT(*) FROM transfer').fetchone()[0]:,}\"\n",
    "        )\n",
    "        print(f\"  Swaps: {conn.execute('SELECT COUNT(*) FROM swap').fetchone()[0]:,}\")\n",
    "        print(f\"  Mints: {conn.execute('SELECT COUNT(*) FROM mint').fetchone()[0]:,}\")\n",
    "        print(f\"  Burns: {conn.execute('SELECT COUNT(*) FROM burn').fetchone()[0]:,}\")\n",
    "        print(f\"  Syncs: {conn.execute('SELECT COUNT(*) FROM sync').fetchone()[0]:,}\")\n",
    "        print(\n",
    "            f\"  Approvals: {conn.execute('SELECT COUNT(*) FROM approval').fetchone()[0]:,}\"\n",
    "        )\n",
    "\n",
    "        print(\"\\nPair metadata:\")\n",
    "        result = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                COUNT(DISTINCT t.pair_address) as total_pairs,\n",
    "                COUNT(DISTINCT pm.pair_address) as pairs_with_metadata,\n",
    "                COUNT(DISTINCT CASE WHEN pm.token0_decimals IS NOT NULL THEN pm.pair_address END) as pairs_with_decimals\n",
    "            FROM (SELECT DISTINCT pair_address FROM transfer) t\n",
    "            LEFT JOIN pair_metadata pm ON t.pair_address = pm.pair_address\n",
    "            \"\"\"\n",
    "        ).fetchone()\n",
    "        print(f\"  Total pairs: {result[0]:,}\")\n",
    "        print(f\"  With metadata: {result[1]:,}\")\n",
    "        print(f\"  With decimals: {result[2]:,}\")\n",
    "\n",
    "        print(\"\\nMost active pairs (by swaps):\")\n",
    "        result = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                s.pair_address,\n",
    "                COALESCE(pm.token0_symbol || '/' || pm.token1_symbol, 'Unknown') as pair_name,\n",
    "                COUNT(*) as swap_count\n",
    "            FROM swap s\n",
    "            LEFT JOIN pair_metadata pm ON s.pair_address = pm.pair_address\n",
    "            GROUP BY s.pair_address, pair_name\n",
    "            ORDER BY swap_count DESC\n",
    "            LIMIT 10\n",
    "            \"\"\"\n",
    "        ).fetchdf()\n",
    "        print(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def get_pair_info(pair_address, db_path=DB_PATH):\n",
    "    pair_address = Web3.to_checksum_address(pair_address)\n",
    "    conn = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"PAIR: {pair_address}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        metadata = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT token0_address, token1_address, token0_symbol, token1_symbol,\n",
    "                   token0_decimals, token1_decimals, created_block\n",
    "            FROM pair_metadata\n",
    "            WHERE pair_address = ?\n",
    "            \"\"\",\n",
    "            (pair_address,),\n",
    "        ).fetchone()\n",
    "\n",
    "        if metadata:\n",
    "            print(f\"\\n{metadata[2] or '?'}/{metadata[3] or '?'}\")\n",
    "            print(f\"  Token0: {metadata[0]} ({metadata[4] or '?'} decimals)\")\n",
    "            print(f\"  Token1: {metadata[1]} ({metadata[5] or '?'} decimals)\")\n",
    "            if metadata[6]:\n",
    "                print(f\"  Created: block {metadata[6]:,}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  No metadata found\")\n",
    "\n",
    "        transfers = conn.execute(\n",
    "            \"SELECT COUNT(*) FROM transfer WHERE pair_address = ?\", (pair_address,)\n",
    "        ).fetchone()[0]\n",
    "        swaps = conn.execute(\n",
    "            \"SELECT COUNT(*) FROM swap WHERE pair_address = ?\", (pair_address,)\n",
    "        ).fetchone()[0]\n",
    "        mints = conn.execute(\n",
    "            \"SELECT COUNT(*) FROM mint WHERE pair_address = ?\", (pair_address,)\n",
    "        ).fetchone()[0]\n",
    "        burns = conn.execute(\n",
    "            \"SELECT COUNT(*) FROM burn WHERE pair_address = ?\", (pair_address,)\n",
    "        ).fetchone()[0]\n",
    "\n",
    "        print(\n",
    "            f\"\\nEvents: {transfers:,} transfers | {swaps:,} swaps | {mints:,} mints | {burns:,} burns\"\n",
    "        )\n",
    "\n",
    "        latest_sync = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT reserve0_normalized, reserve1_normalized, block_number\n",
    "            FROM sync\n",
    "            WHERE pair_address = ?\n",
    "            ORDER BY block_number DESC\n",
    "            LIMIT 1\n",
    "            \"\"\",\n",
    "            (pair_address,),\n",
    "        ).fetchone()\n",
    "\n",
    "        if latest_sync and latest_sync[0] is not None:\n",
    "            print(f\"\\nLatest reserves (block {latest_sync[2]:,}):\")\n",
    "            print(f\"  Reserve0: {latest_sync[0]:,.6f}\")\n",
    "            print(f\"  Reserve1: {latest_sync[1]:,.6f}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "print(\"‚úì Main functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4da79d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 15:24:47,688 INFO ================================================================================\n",
      "2025-10-26 15:24:47,689 INFO UNISWAP V3 PIPELINE\n",
      "2025-10-26 15:24:47,690 INFO ================================================================================\n",
      "2025-10-26 15:24:47,691 INFO Blocks: 10,000,000 ‚Üí 10,500,000 (current)\n",
      "2025-10-26 15:24:47,691 INFO Config: chunk=5,000 | workers=4\n",
      "2025-10-26 15:24:47,767 ERROR \n",
      "================================================================================\n",
      "2025-10-26 15:24:47,767 ERROR ERROR: Catalog Error: Table with name transfer does not exist!\n",
      "Did you mean \"pg_attrdef\"?\n",
      "\n",
      "LINE 3:             (SELECT COUNT(*) FROM transfer) as total_transfers,\n",
      "                                          ^\n",
      "2025-10-26 15:24:47,768 ERROR Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12366/104390798.py\", line 26, in <module>\n",
      "    stats = get_database_stats(DB_PATH)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12366/3996062636.py\", line 253, in get_database_stats\n",
      "    result = conn.execute(\n",
      "             ^^^^^^^^^^^^^\n",
      "_duckdb.CatalogException: Catalog Error: Table with name transfer does not exist!\n",
      "Did you mean \"pg_attrdef\"?\n",
      "\n",
      "LINE 3:             (SELECT COUNT(*) FROM transfer) as total_transfers,\n",
      "                                          ^\n",
      "\n",
      "2025-10-26 15:24:47,769 ERROR ================================================================================\n"
     ]
    },
    {
     "ename": "CatalogException",
     "evalue": "Catalog Error: Table with name transfer does not exist!\nDid you mean \"pg_attrdef\"?\n\nLINE 3:             (SELECT COUNT(*) FROM transfer) as total_transfers,\n                                          ^",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCatalogException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBlocks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTART_BLOCK\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ‚Üí \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEND_BLOCK\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (current)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConfig: chunk=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHUNK_SIZE\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | workers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_WORKERS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m stats = \u001b[43mget_database_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDB_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m logging.info(\n\u001b[32m     28\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mtotal_transfers\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m transfers | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mtotal_swaps\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m swaps | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mcompleted_ranges\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ranges done\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 253\u001b[39m, in \u001b[36mget_database_stats\u001b[39m\u001b[34m(db_path)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_database_stats\u001b[39m(db_path):\n\u001b[32m    251\u001b[39m     conn = get_thread_connection(db_path)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     result = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;250;43m        \u001b[39;49m\u001b[33;43;03m\"\"\"\u001b[39;49;00m\n\u001b[32m    255\u001b[39m \u001b[33;43;03m        SELECT \u001b[39;49;00m\n\u001b[32m    256\u001b[39m \u001b[33;43;03m            (SELECT COUNT(*) FROM transfer) as total_transfers,\u001b[39;49;00m\n\u001b[32m    257\u001b[39m \u001b[33;43;03m            (SELECT COUNT(*) FROM swap) as total_swaps,\u001b[39;49;00m\n\u001b[32m    258\u001b[39m \u001b[33;43;03m            (SELECT COUNT(*) FROM mint) as total_mints,\u001b[39;49;00m\n\u001b[32m    259\u001b[39m \u001b[33;43;03m            (SELECT COUNT(*) FROM burn) as total_burns,\u001b[39;49;00m\n\u001b[32m    260\u001b[39m \u001b[33;43;03m            (SELECT COUNT(*) FROM sync) as total_syncs,\u001b[39;49;00m\n\u001b[32m    261\u001b[39m \u001b[33;43;03m            (SELECT COUNT(*) FROM approval) as total_approvals,\u001b[39;49;00m\n\u001b[32m    262\u001b[39m \u001b[33;43;03m            (SELECT COUNT(*) FROM processing_state WHERE status = 'completed') as completed_ranges,\u001b[39;49;00m\n\u001b[32m    263\u001b[39m \u001b[33;43;03m            (SELECT COUNT(*) FROM pair_metadata) as total_pairs,\u001b[39;49;00m\n\u001b[32m    264\u001b[39m \u001b[33;43;03m            (SELECT COUNT(*) FROM block_metadata) as total_blocks\u001b[39;49;00m\n\u001b[32m    265\u001b[39m \u001b[33;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.fetchone()\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    269\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtotal_transfers\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[32m0\u001b[39m],\n\u001b[32m    270\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtotal_swaps\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[32m1\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    277\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtotal_blocks\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[32m8\u001b[39m],\n\u001b[32m    278\u001b[39m     }\n",
      "\u001b[31mCatalogException\u001b[39m: Catalog Error: Table with name transfer does not exist!\nDid you mean \"pg_attrdef\"?\n\nLINE 3:             (SELECT COUNT(*) FROM transfer) as total_transfers,\n                                          ^"
     ]
    }
   ],
   "source": [
    "token_filter = [\n",
    "    \"0xB4e16d0168e52d35CaCD2c6185b44281Ec28C9Dc\",\n",
    "    \"0x3139Ffc91B99aa94DA8A2dc13f1fC36F9BDc98eE\",\n",
    "    \"0x12EDE161c702D1494612d19f05992f43aa6A26FB\",\n",
    "    \"0xA478c2975Ab1Ea89e8196811F51A7B7Ade33eB11\",\n",
    "    \"0x07F068ca326a469Fc1d87d85d448990C8cBa7dF9\",\n",
    "    \"0xAE461cA67B15dc8dc81CE7615e0320dA1A9aB8D5\",\n",
    "    \"0xCe407CD7b95B39d3B4d53065E711e713dd5C5999\",\n",
    "    \"0x33C2d48Bc95FB7D0199C5C693e7a9F527145a9Af\",\n",
    "]\n",
    "\n",
    "START_BLOCK = 12369621\n",
    "END_BLOCK = w3.eth.block_number\n",
    "START_BLOCK = 10000000\n",
    "END_BLOCK = 10500000\n",
    "CHUNK_SIZE = 5000\n",
    "MAX_WORKERS = 4\n",
    "\n",
    "try:\n",
    "    logging.info(\"=\" * 80)\n",
    "    logging.info(\"UNISWAP V3 PIPELINE\")\n",
    "    logging.info(\"=\" * 80)\n",
    "    logging.info(f\"Blocks: {START_BLOCK:,} ‚Üí {END_BLOCK:,} (current)\")\n",
    "    logging.info(f\"Config: chunk={CHUNK_SIZE:,} | workers={MAX_WORKERS}\")\n",
    "\n",
    "    stats = get_database_stats(DB_PATH)\n",
    "    logging.info(\n",
    "        f\"DB: {stats['total_transfers']:,} transfers | {stats['total_swaps']:,} swaps | {stats['completed_ranges']} ranges done\"\n",
    "    )\n",
    "    logging.info(\"=\" * 80)\n",
    "\n",
    "    if input(\"Start? (y/n): \").strip().lower() != \"y\":\n",
    "        logging.info(\"Cancelled\")\n",
    "    else:\n",
    "        scan_blockchain_to_duckdb(\n",
    "            event_file=V3_EVENT_BY_CONTRACTS,\n",
    "            db_path=DB_PATH,\n",
    "            start_block=START_BLOCK,\n",
    "            end_block=END_BLOCK,\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            max_workers=MAX_WORKERS,\n",
    "        )\n",
    "\n",
    "        logging.info(\"\\nPost-processing...\")\n",
    "        collect_missing_pair_metadata(DB_PATH, batch_size=50, max_workers=4)\n",
    "        normalize_missing_pairs(DB_PATH, max_workers=4)\n",
    "\n",
    "        final = get_database_stats(DB_PATH)\n",
    "        logging.info(\"=\" * 80)\n",
    "        logging.info(\"COMPLETE\")\n",
    "        logging.info(\n",
    "            f\"Transfers: {final['total_transfers']:,} | Swaps: {final['total_swaps']:,}\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Mints: {final['total_mints']:,} | Burns: {final['total_burns']:,}\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Pairs: {final['total_pairs']:,} | Blocks: {final['total_blocks']:,}\"\n",
    "        )\n",
    "        logging.info(\"=\" * 80)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging.warning(\"\\n\" + \"=\" * 80)\n",
    "    logging.warning(\"INTERRUPTED - Progress saved\")\n",
    "    stats = get_database_stats(DB_PATH)\n",
    "    logging.warning(\n",
    "        f\"State: {stats['total_transfers']:,} transfers | {stats['completed_ranges']} ranges\"\n",
    "    )\n",
    "    logging.warning(\"Rerun to resume\")\n",
    "    logging.warning(\"=\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(\"\\n\" + \"=\" * 80)\n",
    "    logging.error(f\"ERROR: {e}\")\n",
    "    logging.error(traceback.format_exc())\n",
    "    logging.error(\"=\" * 80)\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    if hasattr(_thread_local, \"conn\") and _thread_local.conn:\n",
    "        _thread_local.conn.close()\n",
    "\n",
    "print(\"‚úì Main ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logs_with_chunking(\n",
    "    get_logs_fn,\n",
    "    from_block,\n",
    "    to_block,\n",
    "    argument_filters=None,\n",
    "    initial_chunk_size=10000,\n",
    "    max_retries=5,\n",
    "    base_delay=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch logs with automatic chunking and rate limit handling.\n",
    "    \"\"\"\n",
    "\n",
    "    if argument_filters is None:\n",
    "        argument_filters = {}\n",
    "\n",
    "    # Convert 'latest' to actual block number\n",
    "    if to_block == \"latest\":\n",
    "        to_block = w3.eth.block_number\n",
    "\n",
    "    def fetch_with_retry(start, end, retries=0):\n",
    "        \"\"\"Fetch logs with exponential backoff on rate limit errors.\"\"\"\n",
    "        try:\n",
    "            time.sleep(base_delay)\n",
    "\n",
    "            logs = get_logs_fn(\n",
    "                from_block=start, to_block=end, argument_filters=argument_filters\n",
    "            )\n",
    "            return logs\n",
    "\n",
    "        except HTTPError as e:\n",
    "            if \"429\" in str(e) or \"Too Many Requests\" in str(e):\n",
    "                if retries < max_retries:\n",
    "                    wait_time = base_delay * (2**retries)\n",
    "                    print(\n",
    "                        f\"  ‚ö† Rate limit hit, waiting {wait_time:.1f}s... (retry {retries + 1}/{max_retries})\"\n",
    "                    )\n",
    "                    time.sleep(wait_time)\n",
    "                    return fetch_with_retry(start, end, retries + 1)\n",
    "                else:\n",
    "                    print(f\"  ‚úó Max retries reached\")\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def fetch_range(start, end, chunk_size):\n",
    "        \"\"\"Recursive function to fetch a block range with dynamic chunking.\"\"\"\n",
    "\n",
    "        if end - start <= chunk_size:\n",
    "            try:\n",
    "                print(f\"Fetching blocks {start} to {end} ({end - start + 1} blocks)...\")\n",
    "                logs = fetch_with_retry(start, end)\n",
    "                print(f\"  ‚úì Got {len(logs)} logs\")\n",
    "                return logs\n",
    "\n",
    "            except HTTPError as e:\n",
    "                raise\n",
    "\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "\n",
    "                if \"-32005\" in error_str or \"more than 10000 results\" in error_str:\n",
    "                    print(f\"  ‚ö† Too many results, splitting...\")\n",
    "\n",
    "                    mid = (start + end) // 2\n",
    "\n",
    "                    if mid == start:\n",
    "                        print(f\"  ‚ö† Cannot split further\")\n",
    "                        try:\n",
    "                            if hasattr(e, \"args\") and len(e.args) > 0:\n",
    "                                error_data = e.args[0]\n",
    "                                if (\n",
    "                                    isinstance(error_data, dict)\n",
    "                                    and \"data\" in error_data\n",
    "                                ):\n",
    "                                    suggested_to = int(\n",
    "                                        error_data[\"data\"].get(\"to\", hex(end)), 16\n",
    "                                    )\n",
    "                                    if suggested_to < end:\n",
    "                                        print(f\"  Using RPC hint: {suggested_to}\")\n",
    "                                        return fetch_range(\n",
    "                                            start, suggested_to, chunk_size // 2\n",
    "                                        )\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        raise Exception(\n",
    "                            f\"Cannot split block range {start}-{end} further\"\n",
    "                        )\n",
    "\n",
    "                    left_logs = fetch_range(start, mid, chunk_size // 2)\n",
    "                    right_logs = fetch_range(mid + 1, end, chunk_size // 2)\n",
    "\n",
    "                    return left_logs + right_logs\n",
    "                else:\n",
    "                    print(f\"  ‚úó Error: {e}\")\n",
    "                    raise\n",
    "\n",
    "        else:\n",
    "            print(f\"Splitting range {start}-{end} into chunks of {chunk_size}...\")\n",
    "            current = start\n",
    "            logs = []\n",
    "\n",
    "            while current <= end:\n",
    "                chunk_end = min(current + chunk_size - 1, end)\n",
    "                chunk_logs = fetch_range(current, chunk_end, chunk_size)\n",
    "                logs.extend(chunk_logs)\n",
    "                current = chunk_end + 1\n",
    "\n",
    "            return logs\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fetching logs from block {from_block} to {to_block}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    all_logs = fetch_range(from_block, to_block, initial_chunk_size)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì Complete! Total logs fetched: {len(all_logs)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return all_logs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN CODE\n",
    "# ============================================================\n",
    "\n",
    "# 1. Get the factory contract\n",
    "address, abi, contract = get_address_abi_contract(UNISWAP_V2_CONTRACT)\n",
    "\n",
    "start_block = 0\n",
    "end_block = \"latest\"\n",
    "\n",
    "# 2. Fetch all PairCreated events\n",
    "print(\"Fetching all PairCreated events from Uniswap V2 Factory...\")\n",
    "pair_created_logs = get_logs_with_chunking(\n",
    "    get_logs_fn=contract.events.PairCreated().get_logs,\n",
    "    from_block=start_block,\n",
    "    to_block=end_block,\n",
    "    argument_filters={},\n",
    "    initial_chunk_size=10000,\n",
    "    max_retries=5,\n",
    "    base_delay=0.5,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Found {len(pair_created_logs)} pairs\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 3. Get event names from one sample pair (all pairs have same interface)\n",
    "if len(pair_created_logs) > 0:\n",
    "    sample_pair_address = pair_created_logs[0].args.pair\n",
    "    print(f\"Getting event list from sample pair: {sample_pair_address}\")\n",
    "\n",
    "    pair_address, pair_abi, pair_contract = get_address_abi_contract(\n",
    "        sample_pair_address\n",
    "    )\n",
    "    event_names = [ev.event_name for ev in pair_contract.events]\n",
    "\n",
    "    print(f\"All pairs have these events: {event_names}\\n\")\n",
    "\n",
    "    # 4. Build the dictionary structure\n",
    "    FULL_EVENT_BY_CONTRACTS = {}\n",
    "\n",
    "    print(f\"Building dictionary structure for {len(pair_created_logs)} pairs...\")\n",
    "\n",
    "    for idx, log in enumerate(pair_created_logs):\n",
    "        pair_addr = log.args.pair\n",
    "\n",
    "        # Create structure with empty dicts for each event\n",
    "        FULL_EVENT_BY_CONTRACTS[pair_addr] = {event: {} for event in event_names}\n",
    "\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(pair_created_logs)} pairs...\")\n",
    "\n",
    "    print(f\"  ‚úì Completed all {len(pair_created_logs)} pairs\\n\")\n",
    "\n",
    "    # 5. Save to disk\n",
    "    output_file = \"uniswap_v2_pairs_events.json\"\n",
    "\n",
    "    print(f\"Saving to {output_file}...\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(FULL_EVENT_BY_CONTRACTS, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"‚úì Saved successfully!\")\n",
    "\n",
    "    # 6. Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total pairs: {len(FULL_EVENT_BY_CONTRACTS)}\")\n",
    "    print(f\"Events per pair: {len(event_names)}\")\n",
    "    print(f\"Event types: {', '.join(event_names)}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Print first 3 pairs as sample\n",
    "    print(\"Sample (first 3 pairs):\")\n",
    "    for idx, (pair_addr, events) in enumerate(\n",
    "        list(FULL_EVENT_BY_CONTRACTS.items())[:3]\n",
    "    ):\n",
    "        print(f\"\\n{pair_addr}:\")\n",
    "        for event_name in events.keys():\n",
    "            print(f\"  - {event_name}: {{}}\")\n",
    "\n",
    "else:\n",
    "    print(\"No pairs found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3db616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important code\n",
    "# We look for the Genesis Uniswap factory, and we get all its events (Only 1 for the V1_factory: 'NewExchange', Only 1 for the V2_factory: PairCreated)\n",
    "# Then we scan from 0 to latest block every NexEchange created from this Factory\n",
    "# (We have the filter of events in case we are filtering events from contract that have multiple events to remove when we don't care)\n",
    "address, abi, contract = get_address_abi_contract(\n",
    "    UNISWAP_V2_CONTRACT\n",
    ")  # Uniswap Genesis Factory\n",
    "start_block = 0\n",
    "end_block = \"latest\"\n",
    "# list all event names\n",
    "event_names = [ev.event_name for ev in contract.events]\n",
    "print(event_names)\n",
    "# define which events you want and filters directly\n",
    "events_to_scan = [\n",
    "    contract.events.PairCreated().get_logs,\n",
    "    # contract.events.Transfer().get_logs,\n",
    "    # contract.events.Approval().get_logs,\n",
    "]\n",
    "L_LOGS = []  # IMPORTANT\n",
    "for get_logs_fn in events_to_scan:\n",
    "    logs = get_logs_fn(\n",
    "        from_block=start_block,\n",
    "        to_block=end_block,\n",
    "        argument_filters={},  # or {\"from\": some_address}, {\"to\": [addr1, addr2]}\n",
    "    )\n",
    "    for log in logs:\n",
    "        # print(log[\"transactionHash\"].hex(), log[\"blockNumber\"], log[\"event\"])\n",
    "        L_LOGS.append(log)\n",
    "\n",
    "# Important code we use in combination with the events filter\n",
    "# We created a list of Exchange created by the Uniswap V1 Factory Contract and we list all their Events\n",
    "# We create the Dictionnary\n",
    "# \"exchange_address_1\": {\"event_1\": {}, event_2: {}, event_3:{}}\n",
    "# This dict fed with the code allow us to retrieve every transactions with the events(logs) of this exchange\n",
    "# we can then sniff Liquidity out of it\n",
    "\n",
    "FULL_EVENT_BY_CONTRACTS = {}  # IMPORTANT\n",
    "for log in L_LOGS:\n",
    "    add, abi, contract = get_address_abi_contract(log.args.exchange)\n",
    "    event_names = [ev.event_name for ev in contract.events]\n",
    "    FULL_EVENT_BY_CONTRACTS[add] = {event: {} for event in event_names}\n",
    "    time.sleep(1)\n",
    "\n",
    "print(len(FULL_EVENT_BY_CONTRACTS))\n",
    "\n",
    "if not os.path.exists(V2_EVENT_BY_CONTRACTS):\n",
    "    with open(V2_EVENT_BY_CONTRACTS, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(V2_EVENT_BY_CONTRACTS, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Query specific pair data for analysis\n",
    "\n",
    "\n",
    "def load_pair_data_for_analysis(pair_address, block_start=None, block_end=None):\n",
    "    \"\"\"\n",
    "    Load normalized transfer data for a specific pair.\n",
    "    This is what you need for liquidity analysis.\n",
    "    \"\"\"\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "\n",
    "    block_filter = \"\"\n",
    "    if block_start and block_end:\n",
    "        block_filter = f\"AND block_number BETWEEN {block_start} AND {block_end}\"\n",
    "    elif block_start:\n",
    "        block_filter = f\"AND block_number >= {block_start}\"\n",
    "    elif block_end:\n",
    "        block_filter = f\"AND block_number <= {block_end}\"\n",
    "\n",
    "    conn = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "    # Get pair metadata\n",
    "    metadata = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT token0_symbol, token1_symbol, token0_decimals, token1_decimals\n",
    "        FROM pair_metadata\n",
    "        WHERE pair_address = ?\n",
    "    \"\"\",\n",
    "        (pair_address,),\n",
    "    ).fetchone()\n",
    "\n",
    "    if metadata:\n",
    "        pair_name = f\"{metadata[0]}/{metadata[1]}\"\n",
    "        print(f\"Loading data for {pair_name} pair ({pair_address[:10]}...)\")\n",
    "    else:\n",
    "        pair_name = \"Unknown\"\n",
    "        print(f\"‚ö†Ô∏è  No metadata found for {pair_address}\")\n",
    "\n",
    "    # Load transfer events (this is what you need for liquidity tracking)\n",
    "    df = conn.execute(\n",
    "        f\"\"\"\n",
    "        SELECT \n",
    "            block_number as block,\n",
    "            pair_address as address,\n",
    "            from_address,\n",
    "            to_address,\n",
    "            COALESCE(value_normalized, CAST(value AS DOUBLE) / 1e18) as value\n",
    "        FROM transfer\n",
    "        WHERE pair_address = ?\n",
    "        {block_filter}\n",
    "        ORDER BY block_number, log_index\n",
    "    \"\"\",\n",
    "        (pair_address,),\n",
    "    ).fetchdf()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è  No transfer events found\")\n",
    "        return df\n",
    "\n",
    "    # Add metadata\n",
    "    df[\"pair_name\"] = pair_name\n",
    "    df[\"address\"] = df[\"address\"].apply(w3.to_checksum_address)\n",
    "    df[\"from_address\"] = df[\"from_address\"].apply(w3.to_checksum_address)\n",
    "    df[\"to_address\"] = df[\"to_address\"].apply(w3.to_checksum_address)\n",
    "\n",
    "    print(f\"Loaded {len(df):,} transfer events\")\n",
    "    print(f\"Block range: {df['block'].min():,} to {df['block'].max():,}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_pair_summary_stats(pair_address):\n",
    "    \"\"\"\n",
    "    Get summary statistics for a pair.\n",
    "    \"\"\"\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "\n",
    "    conn = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "    stats = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT t.transaction_hash) as total_transactions,\n",
    "            COUNT(*) FILTER (WHERE t.from_address = '0x0000000000000000000000000000000000000000') as mints,\n",
    "            COUNT(*) FILTER (WHERE t.to_address = '0x0000000000000000000000000000000000000000') as burns,\n",
    "            COUNT(*) FILTER (WHERE t.from_address != '0x0000000000000000000000000000000000000000' \n",
    "                             AND t.to_address != '0x0000000000000000000000000000000000000000') as transfers,\n",
    "            (SELECT COUNT(*) FROM swap WHERE pair_address = ?) as swaps,\n",
    "            MIN(t.block_number) as first_block,\n",
    "            MAX(t.block_number) as last_block\n",
    "        FROM transfer t\n",
    "        WHERE t.pair_address = ?\n",
    "    \"\"\",\n",
    "        (pair_address, pair_address),\n",
    "    ).fetchone()\n",
    "\n",
    "    metadata = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT token0_symbol, token1_symbol\n",
    "        FROM pair_metadata\n",
    "        WHERE pair_address = ?\n",
    "    \"\"\",\n",
    "        (pair_address,),\n",
    "    ).fetchone()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    if metadata:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PAIR SUMMARY: {metadata[0]}/{metadata[1]}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    print(f\"Total transactions: {stats[0]:,}\")\n",
    "    print(f\"Mints (add liquidity): {stats[1]:,}\")\n",
    "    print(f\"Burns (remove liquidity): {stats[2]:,}\")\n",
    "    print(f\"Transfers (LP token): {stats[3]:,}\")\n",
    "    print(f\"Swaps: {stats[4]:,}\")\n",
    "    print(f\"Block range: {stats[5]:,} to {stats[6]:,}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "df = load_pair_data_for_analysis(token_filter[0], START_BLOCK, END_BLOCK)\n",
    "get_pair_summary_stats(token_filter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ac509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ST GRAPH, evolution of the UNISWAP v1 (UNI-V1) amount of token issued/burned (GLOBAL TOTAL over block)\n",
    "# Important to compare the size of every pool but we need to link \"value\" to either $ or something relevant for comparison\n",
    "# NEED: df\n",
    "totals = (\n",
    "    df.groupby([\"block\", \"address\"], as_index=False)[\"value\"]\n",
    "    .sum()\n",
    "    .sort_values([\"address\", \"block\"])\n",
    ")\n",
    "totals[\"cum_value\"] = totals.groupby(\"address\")[\"value\"].cumsum()\n",
    "\n",
    "# # 2) fill missing blocks only inside each address' span (min..max), then cumulate\n",
    "# totals = totals.groupby(\"address\", group_keys=False).apply(\n",
    "#     lambda g: (\n",
    "#         g.set_index(\"block\")\n",
    "#         .reindex(range(g[\"block\"].min(), g[\"block\"].max() + 1), fill_value=0)\n",
    "#         .rename_axis(\"block\")\n",
    "#         .reset_index()\n",
    "#         .assign(address=g.name)\n",
    "#     )\n",
    "# )\n",
    "# totals = (\n",
    "#     totals[[\"block\", \"address\", \"value\"]]\n",
    "#     .sort_values([\"address\", \"block\"])\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "pools_of_interest = [\n",
    "    w3.to_checksum_address(token_filter[0]),\n",
    "    w3.to_checksum_address(token_filter[1]),\n",
    "    w3.to_checksum_address(token_filter[2]),\n",
    "]\n",
    "\n",
    "# pools_of_interest = [\"add_1\",\"add_2\",\"add_3\"]\n",
    "cum_long_sub = totals[totals[\"address\"].isin(pools_of_interest)]\n",
    "\n",
    "fig = px.area(\n",
    "    cum_long_sub,\n",
    "    x=\"block\",\n",
    "    y=\"cum_value\",\n",
    "    color=\"address\",\n",
    "    line_group=\"address\",\n",
    "    title=\"Cumulative liquidity evolution per pool\",\n",
    "    labels={\"cum_value\": \"Cumulative liquidity\", \"address\": \"Pool address\"},\n",
    ")\n",
    "\n",
    "# Optionally, you can also do px.line instead of px.area if you prefer lines without fill\n",
    "fig = px.line(\n",
    "    cum_long_sub,\n",
    "    x=\"block\",\n",
    "    y=\"cum_value\",\n",
    "    color=\"address\",\n",
    "    title=\"Cumulative liquidity per pool\",\n",
    ")\n",
    "# You can also make it not stacked (i.e. overlayed) by doing:\n",
    "# fig = px.area(\n",
    "#     cum_long_sub,\n",
    "#     x=\"block\",\n",
    "#     y=\"cum_value\",\n",
    "#     color=\"address\",\n",
    "#     line_group=\"address\",\n",
    "#     facet_col=None,\n",
    "#     # maybe set `groupnorm=None` or other arguments\n",
    "# )\n",
    "\n",
    "fig.update_layout(legend_title=\"Pool address\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block_filter(block_start=None, block_end=None):\n",
    "    if block_start is not None and block_end is not None:\n",
    "        return f\"AND block_number BETWEEN {block_start} AND {block_end}\"\n",
    "    if block_start is not None:\n",
    "        return f\"AND block_number >= {block_start}\"\n",
    "    if block_end is not None:\n",
    "        return f\"AND block_number <= {block_end}\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def calculate_pool_liquidity_python_optimized(\n",
    "    db_path, pair_address, block_start=None, block_end=None\n",
    "):\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "    block_filter = build_block_filter(block_start, block_end)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        block_number AS block,\n",
    "        from_address AS provider,\n",
    "        -CAST(value AS DOUBLE) AS delta\n",
    "    FROM transfer\n",
    "    WHERE pair_address = '{pair_address}'\n",
    "        AND from_address != '0x0000000000000000000000000000000000000000'\n",
    "        {block_filter}\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        block_number AS block,\n",
    "        to_address AS provider,\n",
    "        CAST(value AS DOUBLE) AS delta\n",
    "    FROM transfer\n",
    "    WHERE pair_address = '{pair_address}'\n",
    "        AND to_address != '0x0000000000000000000000000000000000000000'\n",
    "        {block_filter}\n",
    "    ORDER BY block, provider\n",
    "    \"\"\"\n",
    "\n",
    "    with duckdb.connect(db_path, read_only=True) as conn:\n",
    "        df = conn.execute(query).fetch_df()\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_grouped = df.groupby([\"block\", \"provider\"], as_index=False)[\"delta\"].sum()\n",
    "    df_grouped = df_grouped.sort_values([\"provider\", \"block\"])\n",
    "    df_grouped[\"cum_provider\"] = df_grouped.groupby(\"provider\")[\"delta\"].cumsum()\n",
    "\n",
    "    all_blocks = np.sort(df_grouped[\"block\"].unique())\n",
    "    all_providers = df_grouped[\"provider\"].unique()\n",
    "\n",
    "    provider_histories = []\n",
    "\n",
    "    for provider in all_providers:\n",
    "        provider_data = df_grouped[df_grouped[\"provider\"] == provider].copy()\n",
    "        first_block = provider_data[\"block\"].min()\n",
    "\n",
    "        provider_blocks = provider_data[\"block\"].values\n",
    "        provider_balances = provider_data[\"cum_provider\"].values\n",
    "\n",
    "        for block in all_blocks:\n",
    "            if block >= first_block:\n",
    "                idx = np.searchsorted(provider_blocks, block, side=\"right\") - 1\n",
    "                if idx >= 0:\n",
    "                    balance = provider_balances[idx]\n",
    "                    if balance > 1e-8:\n",
    "                        provider_histories.append(\n",
    "                            {\n",
    "                                \"block\": block,\n",
    "                                \"provider\": provider,\n",
    "                                \"cum_provider\": balance,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "    df_full = pd.DataFrame(provider_histories)\n",
    "\n",
    "    if df_full.empty:\n",
    "        return df_full\n",
    "\n",
    "    pool_per_block = df_full.groupby(\"block\", as_index=False)[\"cum_provider\"].sum()\n",
    "    pool_per_block.rename(columns={\"cum_provider\": \"cum_pool\"}, inplace=True)\n",
    "\n",
    "    df_full = df_full.merge(pool_per_block, on=\"block\", how=\"left\")\n",
    "\n",
    "    df_full[\"share_pct\"] = np.where(\n",
    "        df_full[\"cum_pool\"].abs() < 1e-10,\n",
    "        0.0,\n",
    "        (df_full[\"cum_provider\"] / df_full[\"cum_pool\"] * 100),\n",
    "    )\n",
    "    df_full[\"share_pct\"] = df_full[\"share_pct\"].clip(0, 100)\n",
    "\n",
    "    df_full = df_full[df_full[\"share_pct\"] >= 0.1].copy()\n",
    "\n",
    "    df_full[\"provider_label\"] = df_full[\"provider\"].apply(create_provider_label)\n",
    "\n",
    "    return df_full.sort_values([\"block\", \"provider\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def validate_liquidity_data(db_path, pair_address, block_start=None, block_end=None):\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "    block_filter = build_block_filter(block_start, block_end)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH mints AS (\n",
    "        SELECT \n",
    "            block_number,\n",
    "            SUM(CAST(value AS DOUBLE)) AS minted\n",
    "        FROM transfer\n",
    "        WHERE pair_address = '{pair_address}'\n",
    "            AND from_address = '0x0000000000000000000000000000000000000000'\n",
    "            {block_filter}\n",
    "        GROUP BY block_number\n",
    "    ),\n",
    "    burns AS (\n",
    "        SELECT \n",
    "            block_number,\n",
    "            SUM(CAST(value AS DOUBLE)) AS burned\n",
    "        FROM transfer\n",
    "        WHERE pair_address = '{pair_address}'\n",
    "            AND to_address = '0x0000000000000000000000000000000000000000'\n",
    "            {block_filter}\n",
    "        GROUP BY block_number\n",
    "    ),\n",
    "    all_blocks AS (\n",
    "        SELECT DISTINCT block_number FROM mints\n",
    "        UNION\n",
    "        SELECT DISTINCT block_number FROM burns\n",
    "    ),\n",
    "    supply_tracking AS (\n",
    "        SELECT \n",
    "            ab.block_number,\n",
    "            COALESCE(m.minted, 0) AS minted,\n",
    "            COALESCE(b.burned, 0) AS burned,\n",
    "            SUM(COALESCE(m.minted, 0) - COALESCE(b.burned, 0)) \n",
    "                OVER (ORDER BY ab.block_number) AS cumulative_supply\n",
    "        FROM all_blocks ab\n",
    "        LEFT JOIN mints m ON ab.block_number = m.block_number\n",
    "        LEFT JOIN burns b ON ab.block_number = b.block_number\n",
    "    )\n",
    "    SELECT * FROM supply_tracking ORDER BY block_number\n",
    "    \"\"\"\n",
    "\n",
    "    with duckdb.connect(db_path, read_only=True) as conn:\n",
    "        validation_df = conn.execute(query).fetch_df()\n",
    "\n",
    "    return validation_df\n",
    "\n",
    "\n",
    "def print_liquidity_validation(db_path, pair_address, block_start=None, block_end=None):\n",
    "    validation_df = validate_liquidity_data(\n",
    "        db_path, pair_address, block_start, block_end\n",
    "    )\n",
    "\n",
    "    if validation_df.empty:\n",
    "        print(\"‚ö†Ô∏è  No liquidity events found\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LIQUIDITY VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    total_minted = validation_df[\"minted\"].sum()\n",
    "    total_burned = validation_df[\"burned\"].sum()\n",
    "    final_supply = validation_df[\"cumulative_supply\"].iloc[-1]\n",
    "\n",
    "    print(f\"Total LP Tokens Minted: {total_minted:,.6f}\")\n",
    "    print(f\"Total LP Tokens Burned: {total_burned:,.6f}\")\n",
    "    print(f\"Net Supply (Current): {final_supply:,.6f}\")\n",
    "    print(f\"First Event Block: {validation_df['block_number'].min()}\")\n",
    "    print(f\"Last Event Block: {validation_df['block_number'].max()}\")\n",
    "    print(f\"Total Events: {len(validation_df)}\")\n",
    "\n",
    "    if final_supply < 1000:\n",
    "        print(\n",
    "            \"‚ö†Ô∏è  Warning: Supply seems low. First 1000 wei should be permanently locked.\"\n",
    "        )\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def get_mint_burn_summary(db_path, pair_address, block_start=None, block_end=None):\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "    block_filter = build_block_filter(block_start, block_end)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH mint_events AS (\n",
    "        SELECT \n",
    "            t.block_number,\n",
    "            t.to_address AS provider,\n",
    "            CAST(t.value AS DOUBLE) AS lp_tokens,\n",
    "            m.amount0,\n",
    "            m.amount1\n",
    "        FROM transfer t\n",
    "        LEFT JOIN mint m ON t.transaction_hash = m.transaction_hash \n",
    "            AND t.pair_address = m.pair_address\n",
    "        WHERE t.pair_address = '{pair_address}'\n",
    "            AND t.from_address = '0x0000000000000000000000000000000000000000'\n",
    "            AND t.to_address != '0x0000000000000000000000000000000000000000'\n",
    "            {block_filter.replace('block_number', 't.block_number') if block_filter else ''}\n",
    "    ),\n",
    "    burn_events AS (\n",
    "        SELECT \n",
    "            t.block_number,\n",
    "            t.from_address AS provider,\n",
    "            CAST(t.value AS DOUBLE) AS lp_tokens,\n",
    "            b.amount0,\n",
    "            b.amount1\n",
    "        FROM transfer t\n",
    "        LEFT JOIN burn b ON t.transaction_hash = b.transaction_hash \n",
    "            AND t.pair_address = b.pair_address\n",
    "        WHERE t.pair_address = '{pair_address}'\n",
    "            AND t.to_address = '0x0000000000000000000000000000000000000000'\n",
    "            AND t.from_address != '0x0000000000000000000000000000000000000000'\n",
    "            {block_filter.replace('block_number', 't.block_number') if block_filter else ''}\n",
    "    )\n",
    "    SELECT \n",
    "        'MINT' AS event_type,\n",
    "        COUNT(*) AS event_count,\n",
    "        SUM(lp_tokens) AS total_lp_tokens,\n",
    "        SUM(CAST(amount0 AS DOUBLE)) AS total_amount0,\n",
    "        SUM(CAST(amount1 AS DOUBLE)) AS total_amount1\n",
    "    FROM mint_events\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'BURN' AS event_type,\n",
    "        COUNT(*) AS event_count,\n",
    "        SUM(lp_tokens) AS total_lp_tokens,\n",
    "        SUM(CAST(amount0 AS DOUBLE)) AS total_amount0,\n",
    "        SUM(CAST(amount1 AS DOUBLE)) AS total_amount1\n",
    "    FROM burn_events\n",
    "    \"\"\"\n",
    "\n",
    "    with duckdb.connect(db_path, read_only=True) as conn:\n",
    "        summary_df = conn.execute(query).fetch_df()\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def print_mint_burn_summary(db_path, pair_address, block_start=None, block_end=None):\n",
    "    summary_df = get_mint_burn_summary(db_path, pair_address, block_start, block_end)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MINT/BURN SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for _, row in summary_df.iterrows():\n",
    "        print(f\"\\n{row['event_type']} Events:\")\n",
    "        print(f\"  Count: {int(row['event_count'])}\")\n",
    "        print(f\"  Total LP Tokens: {row['total_lp_tokens']:,.6f}\")\n",
    "        if row[\"total_amount0\"] is not None:\n",
    "            print(f\"  Total Token0: {row['total_amount0']:,.6f}\")\n",
    "        if row[\"total_amount1\"] is not None:\n",
    "            print(f\"  Total Token1: {row['total_amount1']:,.6f}\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def analyze_pool_liquidity(\n",
    "    db_path,\n",
    "    pair_address,\n",
    "    block_start=None,\n",
    "    block_end=None,\n",
    "    show_plots=True,\n",
    "    show_validation=True,\n",
    "):\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "\n",
    "    if show_validation:\n",
    "        print_liquidity_validation(db_path, pair_address, block_start, block_end)\n",
    "        print_mint_burn_summary(db_path, pair_address, block_start, block_end)\n",
    "\n",
    "    print(\"\\nCalculating pool liquidity distribution...\")\n",
    "    liquidity_df = calculate_pool_liquidity_python_optimized(\n",
    "        db_path=db_path,\n",
    "        pair_address=pair_address,\n",
    "        block_start=block_start,\n",
    "        block_end=block_end,\n",
    "    )\n",
    "\n",
    "    if liquidity_df.empty:\n",
    "        print(\"‚ö†Ô∏è  No liquidity data found for this pair\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Total rows in liquidity data: {len(liquidity_df)}\")\n",
    "    print(\n",
    "        f\"Block range: {liquidity_df['block'].min()} to {liquidity_df['block'].max()}\"\n",
    "    )\n",
    "    print(f\"Number of unique providers: {liquidity_df['provider'].nunique()}\")\n",
    "\n",
    "    if show_plots:\n",
    "        print(\"\\nGenerating percentage ownership chart...\")\n",
    "        fig_pct = plot_staircase_ownership(liquidity_df)\n",
    "        fig_pct.show()\n",
    "\n",
    "        print(\"Generating absolute liquidity chart...\")\n",
    "        fig_abs = plot_absolute_liquidity_staircase(liquidity_df)\n",
    "        fig_abs.show()\n",
    "\n",
    "        print(\"Generating concentration analysis...\")\n",
    "        fig_conc, concentration_metrics = plot_ownership_concentration(liquidity_df)\n",
    "        fig_conc.show()\n",
    "\n",
    "        print(\"\\nGenerating current ownership snapshot (bubble chart)...\")\n",
    "        fig_bubble = plot_bubble_ownership_snapshot(liquidity_df)\n",
    "        fig_bubble.show()\n",
    "    else:\n",
    "        _, concentration_metrics = plot_ownership_concentration(liquidity_df)\n",
    "\n",
    "    print_liquidity_summary(liquidity_df)\n",
    "    print_concentration_summary(concentration_metrics)\n",
    "\n",
    "    return liquidity_df, concentration_metrics\n",
    "\n",
    "\n",
    "def create_provider_label(address):\n",
    "    checksum_addr = w3.to_checksum_address(address)\n",
    "    short_addr = f\"{checksum_addr[:6]}...{checksum_addr[-4:]}\"\n",
    "    return short_addr\n",
    "\n",
    "\n",
    "def add_million_block_markers(fig, min_block, max_block):\n",
    "    start = (min_block // 1_000_000) * 1_000_000\n",
    "    end = (max_block // 1_000_000 + 1) * 1_000_000 + 1\n",
    "\n",
    "    for million_block in range(start, end, 1_000_000):\n",
    "        if min_block <= million_block <= max_block:\n",
    "            fig.add_vline(\n",
    "                x=million_block,\n",
    "                line_width=2,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"black\",\n",
    "                opacity=0.4,\n",
    "                annotation_text=f\"{million_block / 1_000_000:.0f}M\",\n",
    "                annotation_position=\"top\",\n",
    "                annotation_font_size=12,\n",
    "            )\n",
    "\n",
    "\n",
    "def plot_staircase_ownership(df):\n",
    "    fig = go.Figure()\n",
    "    providers = sorted(df[\"provider_label\"].unique())\n",
    "\n",
    "    for provider in providers:\n",
    "        provider_data = df[df[\"provider_label\"] == provider].sort_values(\"block\")\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=provider_data[\"block\"],\n",
    "                y=provider_data[\"share_pct\"],\n",
    "                name=provider,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5, shape=\"hv\"),\n",
    "                stackgroup=\"one\",\n",
    "                groupnorm=\"\",\n",
    "                hovertemplate=\"<b>%{fullData.name}</b><br>Block: %{x}<br>Share: %{y:.4f}%<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    add_million_block_markers(fig, df[\"block\"].min(), df[\"block\"].max())\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Pool Ownership Distribution (Staircase View)\",\n",
    "        hovermode=\"x\",\n",
    "        yaxis_title=\"Ownership Share (%)\",\n",
    "        xaxis_title=\"Block Number\",\n",
    "        legend=dict(\n",
    "            title=\"Provider\",\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "        ),\n",
    "        yaxis=dict(range=[0, 100]),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_absolute_liquidity_staircase(df):\n",
    "    fig = go.Figure()\n",
    "    providers = sorted(df[\"provider_label\"].unique())\n",
    "\n",
    "    for provider in providers:\n",
    "        provider_data = df[df[\"provider_label\"] == provider].sort_values(\"block\")\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=provider_data[\"block\"],\n",
    "                y=provider_data[\"cum_provider\"],\n",
    "                name=provider,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5, shape=\"hv\"),\n",
    "                stackgroup=\"one\",\n",
    "                hovertemplate=\"<b>%{fullData.name}</b><br>Block: %{x}<br>Amount: %{y:.6f}<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    add_million_block_markers(fig, df[\"block\"].min(), df[\"block\"].max())\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Pool Liquidity by Provider (Absolute Values)\",\n",
    "        hovermode=\"x\",\n",
    "        yaxis_title=\"Liquidity Amount (Token Units)\",\n",
    "        xaxis_title=\"Block Number\",\n",
    "        legend=dict(\n",
    "            title=\"Provider\",\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def calculate_hhi_metrics(df):\n",
    "    share_clean = np.where(\n",
    "        np.isinf(df[\"share_pct\"]) | np.isnan(df[\"share_pct\"]), 0, df[\"share_pct\"]\n",
    "    )\n",
    "    df = df.assign(share_pct_clean=share_clean)\n",
    "\n",
    "    hhi_agg = (\n",
    "        df.groupby(\"block\")\n",
    "        .agg(\n",
    "            hhi=(\"share_pct_clean\", lambda x: (x**2).sum()),\n",
    "            active_providers=(\"share_pct_clean\", lambda x: (x > 0.01).sum()),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return hhi_agg\n",
    "\n",
    "\n",
    "def add_hhi_zones(fig):\n",
    "    zones = [\n",
    "        (0, 1500, \"green\", \"Competitive\"),\n",
    "        (1500, 2500, \"yellow\", \"Moderate\"),\n",
    "        (2500, 10000, \"red\", \"Concentrated\"),\n",
    "    ]\n",
    "\n",
    "    for y0, y1, color, label in zones:\n",
    "        fig.add_hrect(\n",
    "            y0=y0,\n",
    "            y1=y1,\n",
    "            fillcolor=color,\n",
    "            opacity=0.1,\n",
    "            annotation_text=label,\n",
    "            secondary_y=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_ownership_concentration(df):\n",
    "    hhi_df = calculate_hhi_metrics(df)\n",
    "\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hhi_df[\"block\"],\n",
    "            y=hhi_df[\"hhi\"],\n",
    "            name=\"HHI (Concentration)\",\n",
    "            line=dict(color=\"#F46821\", width=2),\n",
    "        ),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hhi_df[\"block\"],\n",
    "            y=hhi_df[\"active_providers\"],\n",
    "            name=\"Active Providers\",\n",
    "            line=dict(color=\"#29BEFD\", width=2),\n",
    "        ),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title=\"Pool Concentration Analysis\", hovermode=\"x unified\")\n",
    "    fig.update_xaxes(title_text=\"Block Number\")\n",
    "    fig.update_yaxes(title_text=\"HHI Score\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Number of Providers\", secondary_y=True)\n",
    "\n",
    "    add_hhi_zones(fig)\n",
    "\n",
    "    return fig, hhi_df\n",
    "\n",
    "\n",
    "def get_concentration_status(hhi):\n",
    "    if hhi < 1500:\n",
    "        return \"‚úÖ COMPETITIVE (Decentralized)\"\n",
    "    elif hhi < 2500:\n",
    "        return \"‚ö†Ô∏è  MODERATE CONCENTRATION\"\n",
    "    else:\n",
    "        return \"üî¥ HIGHLY CONCENTRATED\"\n",
    "\n",
    "\n",
    "def print_liquidity_summary(df):\n",
    "    max_block = df[\"block\"].max()\n",
    "\n",
    "    summary = (\n",
    "        df.groupby([\"provider\", \"provider_label\"])[\"cum_provider\"]\n",
    "        .last()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LIQUIDITY SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for (provider, label), amount in summary.items():\n",
    "        provider_checksum = w3.to_checksum_address(provider)\n",
    "        final_data = df[\n",
    "            (df[\"provider\"] == provider_checksum) & (df[\"block\"] == max_block)\n",
    "        ]\n",
    "\n",
    "        if not final_data.empty:\n",
    "            share = final_data[\"share_pct\"].values[0]\n",
    "            print(f\"{label}: {amount:.6f} tokens ({share:.2f}% of pool)\")\n",
    "        else:\n",
    "            print(f\"{label}: {amount:.6f} tokens (exited)\")\n",
    "\n",
    "\n",
    "def print_concentration_summary(hhi_df):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CONCENTRATION METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Average HHI: {hhi_df['hhi'].mean():.2f}\")\n",
    "    print(f\"Current HHI: {hhi_df['hhi'].iloc[-1]:.2f}\")\n",
    "    print(f\"Max providers at any block: {hhi_df['active_providers'].max()}\")\n",
    "    print(f\"Current active providers: {hhi_df['active_providers'].iloc[-1]}\")\n",
    "\n",
    "    current_hhi = hhi_df[\"hhi\"].iloc[-1]\n",
    "    status = get_concentration_status(current_hhi)\n",
    "    print(f\"Pool status: {status}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def plot_bubble_ownership_snapshot(df):\n",
    "    latest_block = df[\"block\"].max()\n",
    "    latest_data = df[df[\"block\"] == latest_block].copy()\n",
    "    latest_data = latest_data.sort_values(\"share_pct\", ascending=False)\n",
    "\n",
    "    n_providers = len(latest_data)\n",
    "    cols = int(np.ceil(np.sqrt(n_providers)))\n",
    "\n",
    "    latest_data[\"x_pos\"] = latest_data.index % cols\n",
    "    latest_data[\"y_pos\"] = latest_data.index // cols\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=latest_data[\"x_pos\"],\n",
    "            y=latest_data[\"y_pos\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=latest_data[\"share_pct\"] * 15,\n",
    "                sizemode=\"diameter\",\n",
    "                sizemin=30,\n",
    "                color=latest_data[\"share_pct\"],\n",
    "                colorscale=\"RdYlGn_r\",\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Share (%)\", thickness=20, len=0.7),\n",
    "                line=dict(color=\"darkgray\", width=3),\n",
    "                opacity=0.8,\n",
    "            ),\n",
    "            hovertemplate=(\n",
    "                \"<b>%{customdata[0]}</b><br>\"\n",
    "                \"Share: %{customdata[1]:.4f}%<br>\"\n",
    "                \"LP Tokens: %{customdata[2]:.6f}\"\n",
    "                \"<extra></extra>\"\n",
    "            ),\n",
    "            customdata=latest_data[\n",
    "                [\"provider_label\", \"share_pct\", \"cum_provider\"]\n",
    "            ].values,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Pool Ownership Snapshot at Block {latest_block:,}\",\n",
    "        xaxis=dict(visible=False, range=[-0.5, cols - 0.5]),\n",
    "        yaxis=dict(visible=False, scaleanchor=\"x\", scaleratio=1),\n",
    "        height=600,\n",
    "        width=800,\n",
    "        showlegend=False,\n",
    "        hovermode=\"closest\",\n",
    "        plot_bgcolor=\"white\",\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edeb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is all you need for the liquidity graphs we built earlier\n",
    "pair_address = token_filter[0]  # Your pair\n",
    "df = load_pair_data_for_analysis(pair_address, START_BLOCK, END_BLOCK)\n",
    "\n",
    "# Then use the existing analysis function\n",
    "liquidity_df, concentration_metrics = analyze_pool_liquidity(\n",
    "    db_path=DB_PATH,\n",
    "    pair_address=pair_address,\n",
    "    block_start=START_BLOCK,\n",
    "    block_end=END_BLOCK,\n",
    "    show_plots=True,\n",
    "    show_validation=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "closedblind-ylehWVqW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
