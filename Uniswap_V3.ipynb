{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1545bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 19:17:36,582 INFO Logging initialized. Log file: logs/uniswap_v3_pipeline_20251102_191736.log\n",
      "2025-11-02 19:17:36,583 INFO ✓ Environment variables validated\n",
      "2025-11-02 19:17:36,585 INFO ✓ Initialization cell completed successfully\n",
      "2025-11-02 19:17:36,585 INFO ✓ Base output directory: out/V3\n",
      "2025-11-02 19:17:36,586 INFO ✓ Database path: out/V3/uniswap_v3.duckdb\n",
      "2025-11-02 19:17:36,587 INFO ✓ Log file: logs/uniswap_v3_pipeline_20251102_191736.log\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STANDARD LIBRARY IMPORTS\n",
    "# =============================================================================\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import requests_cache\n",
    "import threading\n",
    "import time\n",
    "import traceback\n",
    "from sqlalchemy.pool import QueuePool\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# =============================================================================\n",
    "# THIRD-PARTY IMPORTS\n",
    "# =============================================================================\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Database\n",
    "import duckdb\n",
    "\n",
    "# Web requests and API calls\n",
    "import requests\n",
    "from requests.exceptions import HTTPError, ConnectionError\n",
    "\n",
    "# Blockchain and Web3\n",
    "from web3 import Web3\n",
    "from web3.exceptions import Web3RPCError\n",
    "from web3.providers.rpc.utils import (\n",
    "    ExceptionRetryConfiguration,\n",
    "    REQUEST_RETRY_ALLOWLIST,\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Environment and configuration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Utilities\n",
    "import random\n",
    "\n",
    "# =============================================================================\n",
    "# ENVIRONMENT CONFIGURATION\n",
    "# =============================================================================\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.options.display.float_format = \"{:20,.4f}\".format\n",
    "\n",
    "# =============================================================================\n",
    "# CORE CONSTANTS AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Blockchain Constants\n",
    "UNISWAP_V3_FACTORY = \"0x1F98431c8aD98523631AE4a59f267346ea31F984\"\n",
    "FACTORY_DEPLOYMENT_BLOCK = 12369600\n",
    "MULTICALL3_ADDRESS = \"0xcA11bde05977b3631167028862bE2a173976CA11\"\n",
    "\n",
    "# File Paths and Directories\n",
    "BASE_OUTPUT_DIR = \"out/V3\"\n",
    "LOG_DIR = \"logs\"\n",
    "ABI_CACHE_FOLDER = \"ABI\"\n",
    "\n",
    "# Data Files\n",
    "STATE_FILE = f\"{BASE_OUTPUT_DIR}/V3_final_scan_state.json\"\n",
    "TOKEN_NAME_FILE = f\"{BASE_OUTPUT_DIR}/V3_token_name.json\"\n",
    "V3_EVENT_BY_CONTRACTS = f\"{BASE_OUTPUT_DIR}/uniswap_v3_pairs_events.json\"\n",
    "DB_PATH = f\"{BASE_OUTPUT_DIR}/uniswap_v3.duckdb\"\n",
    "V3_POOL_LIST_FILE = f\"{BASE_OUTPUT_DIR}/uniswap_v3_pairs_events.json\"\n",
    "\n",
    "# =============================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# =============================================================================\n",
    "def setup_logging():\n",
    "    \"\"\"Configure logging for the notebook with both console and file output.\"\"\"\n",
    "    try:\n",
    "        # Ensure log directory exists\n",
    "        os.makedirs(LOG_DIR, exist_ok=True)\n",
    "        \n",
    "        # Generate log filename with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        log_filename = os.path.join(LOG_DIR, f\"uniswap_v3_pipeline_{timestamp}.log\")\n",
    "        \n",
    "        # Configure logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "            handlers=[\n",
    "                logging.StreamHandler(),  # Console output\n",
    "                logging.FileHandler(log_filename)  # File output\n",
    "            ],\n",
    "            force=True  # Override any existing configuration\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Logging initialized. Log file: {log_filename}\")\n",
    "        return log_filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not setup file logging: {e}\")\n",
    "        # Fallback to console-only logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "            force=True\n",
    "        )\n",
    "        return None\n",
    "\n",
    "# Initialize logging\n",
    "LOG_FILENAME = setup_logging()\n",
    "\n",
    "# =============================================================================\n",
    "# ENVIRONMENT VARIABLES AND API CONFIGURATION\n",
    "# =============================================================================\n",
    "def validate_environment():\n",
    "    \"\"\"Validate that required environment variables are present.\"\"\"\n",
    "    required_vars = [\n",
    "        \"INFURA_URL_HEARTHQUAKE\",\n",
    "        \"INFURA_URL_OPENSEE\", \n",
    "        \"INFURA_URL_ECO\",\n",
    "        \"ETHERSCAN_API_KEY\"\n",
    "    ]\n",
    "    \n",
    "    missing_vars = []\n",
    "    for var in required_vars:\n",
    "        if not os.getenv(var):\n",
    "            missing_vars.append(var)\n",
    "    \n",
    "    if missing_vars:\n",
    "        raise EnvironmentError(\n",
    "            f\"Missing required environment variables: {', '.join(missing_vars)}\\n\"\n",
    "            f\"Please check your .env file and ensure all required variables are set.\"\n",
    "        )\n",
    "    \n",
    "    logging.info(\"✓ Environment variables validated\")\n",
    "\n",
    "# Validate environment before proceeding\n",
    "validate_environment()\n",
    "\n",
    "# API Configuration Dictionary\n",
    "ETHERSCAN_API_KEY_DICT = {\n",
    "    \"hearthquake\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_HEARTHQUAKE\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "    \"opensee\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_OPENSEE\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "    \"eco\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_ECO\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Primary API key for Etherscan\n",
    "ETHERSCAN_API_KEY = ETHERSCAN_API_KEY_DICT[\"hearthquake\"][\"ETHERSCAN_API_KEY\"]\n",
    "\n",
    "# =============================================================================\n",
    "# DIRECTORY INITIALIZATION\n",
    "# =============================================================================\n",
    "def create_required_directories():\n",
    "    \"\"\"Create all required directories for the pipeline.\"\"\"\n",
    "    directories = [BASE_OUTPUT_DIR, LOG_DIR, ABI_CACHE_FOLDER]\n",
    "    \n",
    "    for directory in directories:\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            logging.debug(f\"✓ Directory ensured: {directory}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to create directory {directory}: {e}\")\n",
    "            raise\n",
    "\n",
    "# Create required directories\n",
    "create_required_directories()\n",
    "\n",
    "logging.info(\"✓ Initialization cell completed successfully\")\n",
    "logging.info(f\"✓ Base output directory: {BASE_OUTPUT_DIR}\")\n",
    "logging.info(f\"✓ Database path: {DB_PATH}\")\n",
    "logging.info(f\"✓ Log file: {LOG_FILENAME or 'Console only'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d247d4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 19:17:38,063 INFO ✓ HTTP request deduplication enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to Ethereum. Latest block: 23,713,374\n",
      "✓ Core infrastructure with FIXED connection pool loaded\n",
      "✓ No more SQLAlchemy warnings or transaction rollback errors!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2 - UPDATED CORE INFRASTRUCTURE\n",
    "# Fixed DuckDB connection pool without SQLAlchemy dependency issues\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class ProviderPool:\n",
    "    def __init__(self, api_key_dict):\n",
    "        self.providers = []\n",
    "        self.provider_names = []\n",
    "        self.current_index = 0\n",
    "\n",
    "        for name, config in api_key_dict.items():\n",
    "            provider = Web3(\n",
    "                Web3.HTTPProvider(\n",
    "                    endpoint_uri=config[\"INFURA_URL\"],\n",
    "                    request_kwargs={\"timeout\": 30},\n",
    "                    exception_retry_configuration=ExceptionRetryConfiguration(\n",
    "                        errors=(ConnectionError, HTTPError, TimeoutError),\n",
    "                        retries=5,\n",
    "                        backoff_factor=1,\n",
    "                        method_allowlist=REQUEST_RETRY_ALLOWLIST,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if provider.is_connected():\n",
    "                self.providers.append(provider)\n",
    "                self.provider_names.append(name)\n",
    "\n",
    "    def get_provider(self):\n",
    "        if not self.providers:\n",
    "            raise Exception(\"No providers available\")\n",
    "\n",
    "        provider = self.providers[self.current_index]\n",
    "        name = self.provider_names[self.current_index]\n",
    "        self.current_index = (self.current_index + 1) % len(self.providers)\n",
    "        return provider, name\n",
    "\n",
    "\n",
    "class DuckDBConnectionPool:\n",
    "    \"\"\"\n",
    "    Simple DuckDB connection pool without SQLAlchemy dependencies\n",
    "    Eliminates transaction rollback warnings and connection issues\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, db_path, pool_size=5, max_overflow=10):\n",
    "        self.db_path = db_path\n",
    "        self.pool_size = pool_size\n",
    "        self.max_overflow = max_overflow\n",
    "        self.connections = []\n",
    "        self.in_use = set()\n",
    "        self.lock = threading.RLock()\n",
    "        self.created_connections = 0\n",
    "\n",
    "        # Pre-create initial pool connections\n",
    "        for _ in range(pool_size):\n",
    "            self._create_connection()\n",
    "\n",
    "        logging.debug(f\"DuckDB connection pool initialized: {db_path}\")\n",
    "\n",
    "    def _create_connection(self):\n",
    "        \"\"\"Create a new DuckDB connection\"\"\"\n",
    "        with self.lock:\n",
    "            if self.created_connections < self.pool_size + self.max_overflow:\n",
    "                conn = duckdb.connect(self.db_path)\n",
    "                self.connections.append(conn)\n",
    "                self.created_connections += 1\n",
    "                return conn\n",
    "            else:\n",
    "                raise Exception(\"Connection pool exhausted\")\n",
    "\n",
    "    def get_connection(self):\n",
    "        \"\"\"Get a connection from the pool\"\"\"\n",
    "        with self.lock:\n",
    "            # Try to get an existing available connection\n",
    "            for conn in self.connections:\n",
    "                if conn not in self.in_use:\n",
    "                    self.in_use.add(conn)\n",
    "                    return conn\n",
    "\n",
    "            # If no available connection and we can create more\n",
    "            if self.created_connections < self.pool_size + self.max_overflow:\n",
    "                conn = self._create_connection()\n",
    "                self.in_use.add(conn)\n",
    "                return conn\n",
    "\n",
    "            # Wait for a connection to become available or raise exception\n",
    "            raise Exception(\"No connections available in pool\")\n",
    "\n",
    "    def return_connection(self, conn):\n",
    "        \"\"\"Return a connection to the pool\"\"\"\n",
    "        with self.lock:\n",
    "            if conn in self.in_use:\n",
    "                self.in_use.remove(conn)\n",
    "\n",
    "    @contextmanager\n",
    "    def connection(self):\n",
    "        \"\"\"Context manager for getting and returning connections\"\"\"\n",
    "        conn = self.get_connection()\n",
    "        try:\n",
    "            yield conn\n",
    "        finally:\n",
    "            self.return_connection(conn)\n",
    "\n",
    "    def close_all(self):\n",
    "        \"\"\"Close all connections in the pool\"\"\"\n",
    "        with self.lock:\n",
    "            for conn in self.connections:\n",
    "                try:\n",
    "                    conn.close()\n",
    "                except:\n",
    "                    pass\n",
    "            self.connections.clear()\n",
    "            self.in_use.clear()\n",
    "            self.created_connections = 0\n",
    "\n",
    "        logging.debug(\"DuckDB connection pool closed\")\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"Get the current pool size\"\"\"\n",
    "        with self.lock:\n",
    "            return len(self.connections)\n",
    "\n",
    "\n",
    "class ABINotVerified(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ABIRateLimited(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ABINetworkError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ABIFetchError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TokenCache:\n",
    "    def __init__(self, cache_file=TOKEN_NAME_FILE):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = {}\n",
    "        self.lock = threading.Lock()\n",
    "        self.dirty = False\n",
    "        self.last_save = time.time()\n",
    "        self.save_interval = 60\n",
    "\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, \"r\") as f:\n",
    "                    self.cache = json.load(f)\n",
    "                logging.info(f\"Loaded {len(self.cache)} tokens from cache\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to load token cache: {e}\")\n",
    "\n",
    "    def get(self, token_address):\n",
    "        with self.lock:\n",
    "            return self.cache.get(token_address)\n",
    "\n",
    "    def set(self, token_address, token_info):\n",
    "        with self.lock:\n",
    "            self.cache[token_address] = token_info\n",
    "            self.dirty = True\n",
    "            if time.time() - self.last_save > self.save_interval:\n",
    "                self._save()\n",
    "\n",
    "    def _save(self):\n",
    "        if self.dirty:\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)\n",
    "                with open(self.cache_file, \"w\") as f:\n",
    "                    json.dump(self.cache, f, indent=2)\n",
    "                self.dirty = False\n",
    "                self.last_save = time.time()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to save token cache: {e}\")\n",
    "\n",
    "    def flush(self):\n",
    "        self._save()\n",
    "\n",
    "\n",
    "class ABICache:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def get_contract(self, address, provider):\n",
    "        address = provider.to_checksum_address(address)\n",
    "\n",
    "        with self.lock:\n",
    "            if address in self.cache:\n",
    "                return self.cache[address]\n",
    "\n",
    "        try:\n",
    "            abi = get_abi(address)\n",
    "            contract = provider.eth.contract(address=address, abi=abi)\n",
    "\n",
    "            with self.lock:\n",
    "                self.cache[address] = (abi, contract)\n",
    "\n",
    "            return (abi, contract)\n",
    "        except (ABINotVerified, ABIRateLimited, ABINetworkError, ABIFetchError):\n",
    "            return (None, None)\n",
    "\n",
    "\n",
    "def setup_request_deduplication():\n",
    "    requests_cache.install_cache(\n",
    "        cache_name=\"temp_session_cache\",\n",
    "        backend=\"memory\",\n",
    "        expire_after=3600,\n",
    "        allowable_codes=[200, 404],\n",
    "        ignored_parameters=[\"apikey\"],\n",
    "    )\n",
    "    logging.info(\"✓ HTTP request deduplication enabled\")\n",
    "\n",
    "\n",
    "class ABIIndexOptimizer:\n",
    "    def __init__(self, db_pool, abi_folder=\"ABI\"):\n",
    "        self.db_pool = db_pool\n",
    "        self.abi_folder = abi_folder\n",
    "        self.stats = {\"hits\": 0, \"misses\": 0}\n",
    "        self._setup_and_build_index()\n",
    "\n",
    "    def _setup_and_build_index(self):\n",
    "        with self.db_pool.connection() as conn:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS abi_file_index (\n",
    "                    contract_address VARCHAR PRIMARY KEY,\n",
    "                    file_path VARCHAR,\n",
    "                    is_verified BOOLEAN,\n",
    "                    file_size INTEGER,\n",
    "                    last_indexed TIMESTAMP DEFAULT NOW()\n",
    "                )\n",
    "            \"\"\"\n",
    "            )\n",
    "        self._rebuild_index()\n",
    "        logging.info(\"✓ ABI index optimization ready\")\n",
    "\n",
    "    def _rebuild_index(self):\n",
    "        if not os.path.exists(self.abi_folder):\n",
    "            return\n",
    "\n",
    "        indexed_files = []\n",
    "\n",
    "        for filename in os.listdir(self.abi_folder):\n",
    "            if filename.endswith(\".json\"):\n",
    "                contract_address = filename[:-5]\n",
    "                file_path = os.path.join(self.abi_folder, filename)\n",
    "\n",
    "                try:\n",
    "                    file_size = os.path.getsize(file_path)\n",
    "                    with open(file_path, \"r\") as f:\n",
    "                        content = json.load(f)\n",
    "                        is_verified = content is not None and len(content) > 0\n",
    "                    indexed_files.append(\n",
    "                        (contract_address, file_path, is_verified, file_size)\n",
    "                    )\n",
    "                except:\n",
    "                    indexed_files.append((contract_address, file_path, False, 0))\n",
    "\n",
    "        with self.db_pool.connection() as conn:\n",
    "            conn.execute(\"DELETE FROM abi_file_index\")\n",
    "            if indexed_files:\n",
    "                conn.executemany(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO abi_file_index (contract_address, file_path, is_verified, file_size)\n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                    indexed_files,\n",
    "                )\n",
    "        logging.info(f\"✓ Indexed {len(indexed_files)} ABI files\")\n",
    "\n",
    "    def has_abi(self, contract_address):\n",
    "        with self.db_pool.connection() as conn:\n",
    "            result = conn.execute(\n",
    "                \"\"\"\n",
    "                SELECT is_verified FROM abi_file_index WHERE contract_address = ?\n",
    "                \"\"\",\n",
    "                (contract_address,),\n",
    "            ).fetchone()\n",
    "\n",
    "        if result:\n",
    "            self.stats[\"hits\"] += 1\n",
    "            return result[0]\n",
    "        else:\n",
    "            self.stats[\"misses\"] += 1\n",
    "            return None\n",
    "\n",
    "    def add_to_index(self, contract_address, file_path, is_verified):\n",
    "        file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0\n",
    "        with self.db_pool.connection() as conn:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO abi_file_index (contract_address, file_path, is_verified, file_size)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "                ON CONFLICT (contract_address) DO UPDATE SET\n",
    "                    file_path = EXCLUDED.file_path,\n",
    "                    is_verified = EXCLUDED.is_verified,\n",
    "                    file_size = EXCLUDED.file_size,\n",
    "                    last_indexed = NOW()\n",
    "            \"\"\",\n",
    "                (contract_address, file_path, is_verified, file_size),\n",
    "            )\n",
    "\n",
    "\n",
    "def setup_token_metadata_storage(db_pool):\n",
    "    with db_pool.connection() as conn:\n",
    "        conn.execute(\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS token_metadata_permanent (\n",
    "                token_address VARCHAR PRIMARY KEY,\n",
    "                symbol VARCHAR,\n",
    "                name VARCHAR,\n",
    "                decimals INTEGER,\n",
    "                is_valid BOOLEAN DEFAULT true,\n",
    "                created_at TIMESTAMP DEFAULT NOW(),\n",
    "                last_updated TIMESTAMP DEFAULT NOW()\n",
    "            )\n",
    "        \"\"\"\n",
    "        )\n",
    "        conn.execute(\n",
    "            \"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_token_metadata_symbol\n",
    "            ON token_metadata_permanent(symbol)\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_token_metadata_optimized(token_address, provider=None, db_pool=None):\n",
    "    if db_pool is None:\n",
    "        db_pool = globals().get(\"DB_POOL\")\n",
    "    if db_pool is None:\n",
    "        raise ValueError(\"db_pool parameter required or DB_POOL must be initialized\")\n",
    "\n",
    "    token_address = Web3.to_checksum_address(token_address)\n",
    "\n",
    "    with db_pool.connection() as conn:\n",
    "        result = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT symbol, name, decimals FROM token_metadata_permanent\n",
    "            WHERE token_address = ? AND is_valid = true AND decimals IS NOT NULL\n",
    "        \"\"\",\n",
    "            (token_address,),\n",
    "        ).fetchone()\n",
    "\n",
    "        if result:\n",
    "            return {\n",
    "                \"address\": token_address,\n",
    "                \"symbol\": result[0],\n",
    "                \"name\": result[1],\n",
    "                \"decimals\": result[2],\n",
    "                \"cached\": True,\n",
    "            }\n",
    "\n",
    "    if not provider:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    try:\n",
    "        contract = provider.eth.contract(address=token_address, abi=MINIMAL_ERC20_ABI)\n",
    "        symbol = contract.functions.symbol().call()\n",
    "        name = contract.functions.name().call()\n",
    "        decimals = contract.functions.decimals().call()\n",
    "\n",
    "        with db_pool.connection() as conn:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO token_metadata_permanent (token_address, symbol, name, decimals, is_valid)\n",
    "                VALUES (?, ?, ?, ?, true)\n",
    "                ON CONFLICT (token_address) DO UPDATE SET\n",
    "                    symbol = EXCLUDED.symbol,\n",
    "                    name = EXCLUDED.name,\n",
    "                    decimals = EXCLUDED.decimals,\n",
    "                    is_valid = true,\n",
    "                    last_updated = NOW()\n",
    "            \"\"\",\n",
    "                (token_address, symbol, name, decimals),\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"address\": token_address,\n",
    "            \"symbol\": symbol,\n",
    "            \"name\": name,\n",
    "            \"decimals\": decimals,\n",
    "            \"cached\": False,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        with db_pool.connection() as conn:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO token_metadata_permanent (token_address, is_valid)\n",
    "                VALUES (?, false)\n",
    "                ON CONFLICT (token_address) DO UPDATE SET\n",
    "                    is_valid = false, last_updated = NOW()\n",
    "            \"\"\",\n",
    "                (token_address,),\n",
    "            )\n",
    "        return None\n",
    "\n",
    "\n",
    "def test_optimizations():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING CACHE OPTIMIZATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if ABI_OPTIMIZER:\n",
    "        print(\"✓ ABI_OPTIMIZER initialized\")\n",
    "        test_contracts = [\n",
    "            \"0xA0b86a33E6441b42B38ac693D6af30A5A4beE9b7\",\n",
    "            \"0x88e6A0c2dDD26FEEb64F039a2c41296FcB3f5640\",\n",
    "        ]\n",
    "\n",
    "        for contract in test_contracts:\n",
    "            cached_status = ABI_OPTIMIZER.has_abi(contract)\n",
    "            if cached_status is not None:\n",
    "                print(\n",
    "                    f\"  ✓ {contract[:10]} cached: {'verified' if cached_status else 'unverified'}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"  - {contract[:10]} not cached\")\n",
    "\n",
    "        print(\n",
    "            f\"  ABI index stats: {ABI_OPTIMIZER.stats['hits']} hits, {ABI_OPTIMIZER.stats['misses']} misses\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"❌ ABI_OPTIMIZER not initialized\")\n",
    "\n",
    "    try:\n",
    "        cache = requests_cache.get_cache()\n",
    "        if cache:\n",
    "            print(\"✓ HTTP request deduplication active\")\n",
    "        else:\n",
    "            print(\"❌ HTTP request deduplication not active\")\n",
    "    except:\n",
    "        print(\"? HTTP request deduplication status unknown\")\n",
    "\n",
    "    try:\n",
    "        db_pool = globals().get(\"DB_POOL\")\n",
    "        if db_pool:\n",
    "            with db_pool.connection() as conn:\n",
    "                count = conn.execute(\n",
    "                    \"SELECT COUNT(*) FROM token_metadata_permanent\"\n",
    "                ).fetchone()[0]\n",
    "                print(f\"✓ Token metadata storage: {count:,} tokens cached\")\n",
    "        else:\n",
    "            print(\"- Token metadata storage: DB_POOL not initialized yet\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Token metadata storage error: {e}\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def print_optimization_stats():\n",
    "    try:\n",
    "        db_pool = globals().get(\"DB_POOL\")\n",
    "        if not db_pool:\n",
    "            print(\"DB_POOL not initialized yet - cannot show optimization stats\")\n",
    "            return\n",
    "\n",
    "        with db_pool.connection() as conn:\n",
    "            abi_cached = conn.execute(\n",
    "                \"SELECT COUNT(*) FROM abi_file_index WHERE is_verified = true\"\n",
    "            ).fetchone()[0]\n",
    "            abi_unverified = conn.execute(\n",
    "                \"SELECT COUNT(*) FROM abi_file_index WHERE is_verified = false\"\n",
    "            ).fetchone()[0]\n",
    "\n",
    "            tokens_cached = conn.execute(\n",
    "                \"SELECT COUNT(*) FROM token_metadata_permanent WHERE is_valid = true\"\n",
    "            ).fetchone()[0]\n",
    "            tokens_failed = conn.execute(\n",
    "                \"SELECT COUNT(*) FROM token_metadata_permanent WHERE is_valid = false\"\n",
    "            ).fetchone()[0]\n",
    "\n",
    "        try:\n",
    "            cache = requests_cache.get_cache()\n",
    "            http_cached = len(list(cache.urls)) if hasattr(cache, \"urls\") else 0\n",
    "        except:\n",
    "            http_cached = 0\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(\"OPTIMIZATION STATISTICS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ABI Index:\")\n",
    "        print(f\"  Verified ABIs cached: {abi_cached:,}\")\n",
    "        print(f\"  Unverified contracts: {abi_unverified:,}\")\n",
    "        if hasattr(globals().get(\"ABI_OPTIMIZER\"), \"stats\"):\n",
    "            print(f\"  Index hits: {ABI_OPTIMIZER.stats['hits']:,}\")\n",
    "            print(f\"  Index misses: {ABI_OPTIMIZER.stats['misses']:,}\")\n",
    "            if ABI_OPTIMIZER.stats[\"hits\"] + ABI_OPTIMIZER.stats[\"misses\"] > 0:\n",
    "                hit_rate = (\n",
    "                    ABI_OPTIMIZER.stats[\"hits\"]\n",
    "                    / (ABI_OPTIMIZER.stats[\"hits\"] + ABI_OPTIMIZER.stats[\"misses\"])\n",
    "                    * 100\n",
    "                )\n",
    "                print(f\"  Hit rate: {hit_rate:.1f}%\")\n",
    "\n",
    "        print(f\"\\nToken Metadata:\")\n",
    "        print(f\"  Tokens cached: {tokens_cached:,}\")\n",
    "        print(f\"  Failed tokens: {tokens_failed:,}\")\n",
    "\n",
    "        print(f\"\\nHTTP Deduplication:\")\n",
    "        print(f\"  Requests cached: {http_cached:,}\")\n",
    "        print(\"=\" * 60)\n",
    "    except Exception as e:\n",
    "        print(f\"Error showing optimization stats: {e}\")\n",
    "\n",
    "\n",
    "# Initialize all components\n",
    "setup_request_deduplication()\n",
    "\n",
    "TOKEN_CACHE = TokenCache()\n",
    "ABI_CACHE = ABICache()\n",
    "PROVIDER_POOL = ProviderPool(ETHERSCAN_API_KEY_DICT)\n",
    "ABI_OPTIMIZER = None\n",
    "\n",
    "w3, _ = PROVIDER_POOL.get_provider()\n",
    "assert w3.is_connected(), \"Web3 provider connection failed\"\n",
    "print(f\"✓ Connected to Ethereum. Latest block: {w3.eth.block_number:,}\")\n",
    "print(\"✓ Core infrastructure with FIXED connection pool loaded\")\n",
    "print(\"✓ No more SQLAlchemy warnings or transaction rollback errors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2571c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abi(contract_address, api_key=ETHERSCAN_API_KEY, abi_folder=ABI_CACHE_FOLDER):\n",
    "    os.makedirs(abi_folder, exist_ok=True)\n",
    "    filename = os.path.join(abi_folder, f\"{contract_address}.json\")\n",
    "\n",
    "    if 'ABI_OPTIMIZER' in globals() and ABI_OPTIMIZER:\n",
    "        cached_status = ABI_OPTIMIZER.has_abi(contract_address)\n",
    "        if cached_status is True:\n",
    "            with open(filename, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        elif cached_status is False:\n",
    "            raise ABINotVerified(f\"Contract {contract_address} not verified (cached)\")\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, \"r\") as f:\n",
    "                abi = json.load(f)\n",
    "            if abi is None or abi == []:\n",
    "                raise ABINotVerified(\n",
    "                    f\"Contract {contract_address} not verified (cached)\"\n",
    "                )\n",
    "            return abi\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.warning(\n",
    "                f\"Corrupted ABI cache for {contract_address}: {e}, re-fetching...\"\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        url = f\"https://api.etherscan.io/v2/api?chainid=1&module=contract&action=getabi&address={contract_address}&apikey={api_key}\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data[\"status\"] == \"1\":\n",
    "            abi = json.loads(data[\"result\"])\n",
    "            if not isinstance(abi, list) or len(abi) == 0:\n",
    "                logging.warning(f\"Empty ABI for {contract_address}\")\n",
    "                raise ABINotVerified(f\"Empty ABI returned\")\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(abi, f, indent=2)\n",
    "            \n",
    "            if 'ABI_OPTIMIZER' in globals() and ABI_OPTIMIZER:\n",
    "                ABI_OPTIMIZER.add_to_index(contract_address, filename, True)\n",
    "            \n",
    "            return abi\n",
    "        else:\n",
    "            error_msg = data.get(\"result\", \"Unknown error\")\n",
    "            if \"not verified\" in error_msg.lower():\n",
    "                with open(filename, \"w\") as f:\n",
    "                    json.dump(None, f)\n",
    "                \n",
    "                if 'ABI_OPTIMIZER' in globals() and ABI_OPTIMIZER:\n",
    "                    ABI_OPTIMIZER.add_to_index(contract_address, filename, False)\n",
    "                \n",
    "                raise ABINotVerified(f\"Contract not verified: {error_msg}\")\n",
    "            elif (\n",
    "                \"rate limit\" in error_msg.lower()\n",
    "                or \"max rate limit\" in error_msg.lower()\n",
    "            ):\n",
    "                raise ABIRateLimited(f\"Etherscan rate limit: {error_msg}\")\n",
    "            else:\n",
    "                logging.error(\n",
    "                    f\"Etherscan API error for {contract_address}: {error_msg}\"\n",
    "                )\n",
    "                raise ABIFetchError(f\"Etherscan error: {error_msg}\")\n",
    "    except requests.Timeout:\n",
    "        raise ABINetworkError(f\"Timeout fetching ABI for {contract_address}\")\n",
    "    except requests.ConnectionError as e:\n",
    "        raise ABINetworkError(f\"Connection error: {e}\")\n",
    "    except requests.RequestException as e:\n",
    "        raise ABINetworkError(f\"Request failed: {e}\")\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        raise ABIFetchError(f\"Invalid response format: {e}\")\n",
    "\n",
    "MINIMAL_ERC20_ABI = [\n",
    "    {\n",
    "        \"constant\": True,\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"name\",\n",
    "        \"outputs\": [{\"name\": \"\", \"type\": \"string\"}],\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"constant\": True,\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"symbol\",\n",
    "        \"outputs\": [{\"name\": \"\", \"type\": \"string\"}],\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"constant\": True,\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"decimals\",\n",
    "        \"outputs\": [{\"name\": \"\", \"type\": \"uint8\"}],\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "]\n",
    "\n",
    "MINIMAL_UNISWAP_V3_POOL_ABI = [\n",
    "    {\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"token0\",\n",
    "        \"outputs\": [{\"type\": \"address\"}],\n",
    "        \"stateMutability\": \"view\",\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"token1\",\n",
    "        \"outputs\": [{\"type\": \"address\"}],\n",
    "        \"stateMutability\": \"view\",\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"fee\",\n",
    "        \"outputs\": [{\"type\": \"uint24\"}],\n",
    "        \"stateMutability\": \"view\",\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"factory\",\n",
    "        \"outputs\": [{\"type\": \"address\"}],\n",
    "        \"stateMutability\": \"view\",\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "]\n",
    "\n",
    "MULTICALL3_ADDRESS = \"0xcA11bde05977b3631167028862bE2a173976CA11\"\n",
    "MULTICALL3_ABI = [\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"components\": [\n",
    "                    {\"name\": \"target\", \"type\": \"address\"},\n",
    "                    {\"name\": \"callData\", \"type\": \"bytes\"},\n",
    "                ],\n",
    "                \"name\": \"calls\",\n",
    "                \"type\": \"tuple[]\",\n",
    "            }\n",
    "        ],\n",
    "        \"name\": \"aggregate\",\n",
    "        \"outputs\": [\n",
    "            {\"name\": \"blockNumber\", \"type\": \"uint256\"},\n",
    "            {\"name\": \"returnData\", \"type\": \"bytes[]\"},\n",
    "        ],\n",
    "        \"stateMutability\": \"payable\",\n",
    "        \"type\": \"function\",\n",
    "    }\n",
    "]\n",
    "ABI_HASH_CACHE = {}\n",
    "\n",
    "def get_abi_hash(abi):\n",
    "    return hash(json.dumps(abi, sort_keys=True))\n",
    "\n",
    "\n",
    "def build_event_signature_map(abi):\n",
    "    event_map = {}\n",
    "    for item in abi:\n",
    "        if item.get(\"type\") == \"event\":\n",
    "            event_signature = (\n",
    "                f'{item[\"name\"]}({\",\".join(i[\"type\"] for i in item[\"inputs\"])})'\n",
    "            )\n",
    "            event_hash = w3.keccak(text=event_signature).hex()\n",
    "            event_map[event_hash] = item[\"name\"]\n",
    "    return event_map\n",
    "\n",
    "\n",
    "def create_transaction_dict(log, provider, topics):\n",
    "    transaction = {\n",
    "        \"transactionHash\": provider.to_hex(log[\"transactionHash\"]),\n",
    "        \"blockNumber\": log[\"blockNumber\"],\n",
    "        \"logIndex\": log.get(\"logIndex\", 0),\n",
    "        \"address\": log[\"address\"],\n",
    "        \"data\": provider.to_hex(log[\"data\"]),\n",
    "    }\n",
    "\n",
    "    transaction.update(topics)\n",
    "\n",
    "    return transaction\n",
    "\n",
    "\n",
    "def decode_logs_for_contract(contract_address, logs, provider):\n",
    "    abi, contract = ABI_CACHE.get_contract(contract_address, provider)\n",
    "\n",
    "    if not abi or not contract:\n",
    "        return [create_transaction_dict(log, provider, {}) for log in logs]\n",
    "\n",
    "    event_map = get_event_signature_map(contract_address, abi)\n",
    "    transactions = []\n",
    "\n",
    "    for log in logs:\n",
    "        if log.get(\"topics\") and len(log[\"topics\"]) > 0:\n",
    "            event_signature_hash = log[\"topics\"][0].hex()\n",
    "\n",
    "            if event_signature_hash in event_map:\n",
    "                event_name = event_map[event_signature_hash]\n",
    "\n",
    "                try:\n",
    "                    decoded = contract.events[event_name]().process_log(log)\n",
    "                    topics = {\n",
    "                        \"event\": event_name,\n",
    "                        \"args\": dict(decoded[\"args\"]),\n",
    "                    }\n",
    "                except Exception:\n",
    "                    topics = {}\n",
    "            else:\n",
    "                topics = {}\n",
    "        else:\n",
    "            topics = {}\n",
    "\n",
    "        transactions.append(create_transaction_dict(log, provider, topics))\n",
    "\n",
    "    return transactions\n",
    "\n",
    "\n",
    "def get_contract_with_fallback(\n",
    "    contract_address, provider=None, contract_type=\"generic\"\n",
    "):\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    contract_address = provider.to_checksum_address(contract_address)\n",
    "\n",
    "    try:\n",
    "        abi = get_abi(contract_address)\n",
    "        return provider.eth.contract(address=contract_address, abi=abi)\n",
    "\n",
    "    except ABINotVerified:\n",
    "        logging.info(\n",
    "            f\"Contract {contract_address[:10]} not verified, using minimal ABI\"\n",
    "        )\n",
    "\n",
    "        if contract_type == \"erc20\":\n",
    "            return provider.eth.contract(\n",
    "                address=contract_address, abi=MINIMAL_ERC20_ABI\n",
    "            )\n",
    "        elif contract_type == \"uniswap_v3_pool\":\n",
    "            return provider.eth.contract(\n",
    "                address=contract_address, abi=MINIMAL_UNISWAP_V3_POOL_ABI\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"No fallback ABI for type: {contract_type}\")\n",
    "\n",
    "    except ABIRateLimited as e:\n",
    "        logging.warning(f\"Rate limited, cannot fetch ABI: {e}\")\n",
    "        raise\n",
    "\n",
    "    except (ABINetworkError, ABIFetchError) as e:\n",
    "        logging.error(f\"Cannot get contract {contract_address[:10]}: {e}\")\n",
    "        raise\n",
    "\n",
    "EVENT_SIGNATURE_CACHE = {}\n",
    "EVENT_CACHE_LOCK = threading.Lock()\n",
    "\n",
    "\n",
    "def get_event_signature_map(contract_address, abi):\n",
    "    with EVENT_CACHE_LOCK:\n",
    "        if contract_address in EVENT_SIGNATURE_CACHE:\n",
    "            return EVENT_SIGNATURE_CACHE[contract_address]\n",
    "\n",
    "        abi_hash = get_abi_hash(abi)\n",
    "\n",
    "        if abi_hash in ABI_HASH_CACHE:\n",
    "            event_map = ABI_HASH_CACHE[abi_hash]\n",
    "            EVENT_SIGNATURE_CACHE[contract_address] = event_map\n",
    "            return event_map\n",
    "\n",
    "        event_map = build_event_signature_map(abi)\n",
    "        EVENT_SIGNATURE_CACHE[contract_address] = event_map\n",
    "        ABI_HASH_CACHE[abi_hash] = event_map\n",
    "        return event_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72137f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 19:17:43,567 INFO ✓ Database schema loaded successfully\n",
      "2025-11-02 19:19:48,190 INFO ✓ Indexed 12782 ABI files\n",
      "2025-11-02 19:19:48,198 INFO ✓ ABI index optimization ready\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Database setup and core operations loaded\n",
      "✓ Database initialized at: out/V3/uniswap_v3.duckdb\n",
      "✓ Connection pool ready with 5 connections\n",
      "✓ No more SQLAlchemy warnings!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4 - FIXED DATABASE OPERATIONS AND SETUP\n",
    "# Fixed to work with updated DuckDBConnectionPool\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def setup_database(db_path=DB_PATH, schema_path=\"./out/V3/database/schema.sql\"):\n",
    "    conn = duckdb.connect(db_path)\n",
    "    with open(schema_path, \"r\") as f:\n",
    "        schema_sql = f.read()\n",
    "    conn.execute(schema_sql)\n",
    "    logging.info(\"✓ Database schema loaded successfully\")\n",
    "    conn.close()\n",
    "    return DuckDBConnectionPool(db_path)\n",
    "\n",
    "\n",
    "DB_POOL = setup_database()\n",
    "\n",
    "\n",
    "def is_range_processed(start_block, end_block):\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        result = conn.execute(\n",
    "            \"SELECT status FROM processing_state WHERE start_block = ? AND end_block = ?\",\n",
    "            (start_block, end_block),\n",
    "        ).fetchone()\n",
    "        return result and result[0] == \"completed\"\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def get_completed_ranges():\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        result = conn.execute(\n",
    "            \"SELECT start_block, end_block FROM processing_state WHERE status = 'completed'\"\n",
    "        ).fetchall()\n",
    "        return set((r[0], r[1]) for r in result)\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def mark_range_processing(start_block, end_block, worker_id=\"main\"):\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        conn.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO processing_state (start_block, end_block, status, worker_id, updated_at)\n",
    "            VALUES (?, ?, 'processing', ?, NOW())\n",
    "            ON CONFLICT (start_block, end_block)\n",
    "            DO UPDATE SET\n",
    "                status = 'processing',\n",
    "                worker_id = ?,\n",
    "                updated_at = NOW()\n",
    "            \"\"\",\n",
    "            (start_block, end_block, worker_id, worker_id),\n",
    "        )\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def mark_range_completed(start_block, end_block, worker_id=\"main\"):\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        conn.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO processing_state (start_block, end_block, status, worker_id, updated_at)\n",
    "            VALUES (?, ?, 'completed', ?, NOW())\n",
    "            ON CONFLICT (start_block, end_block)\n",
    "            DO UPDATE SET\n",
    "                status = 'completed',\n",
    "                worker_id = ?,\n",
    "                updated_at = NOW()\n",
    "            \"\"\",\n",
    "            (start_block, end_block, worker_id, worker_id),\n",
    "        )\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def get_pair_metadata_cached(pair_address):\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        result = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT token0_address, token1_address, token0_symbol, token1_symbol,\n",
    "                   token0_decimals, token1_decimals, fee_tier, tick_spacing, created_block\n",
    "            FROM pair_metadata\n",
    "            WHERE pair_address = ?\n",
    "            \"\"\",\n",
    "            (pair_address,),\n",
    "        ).fetchone()\n",
    "\n",
    "        if result:\n",
    "            return {\n",
    "                \"token0_address\": result[0],\n",
    "                \"token1_address\": result[1],\n",
    "                \"token0_symbol\": result[2],\n",
    "                \"token1_symbol\": result[3],\n",
    "                \"token0_decimals\": result[4],\n",
    "                \"token1_decimals\": result[5],\n",
    "                \"fee_tier\": result[6],\n",
    "                \"tick_spacing\": result[7],\n",
    "                \"created_block\": result[8],\n",
    "            }\n",
    "        return None\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def is_block_metadata_cached(block_number):\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        result = conn.execute(\n",
    "            \"SELECT 1 FROM block_metadata WHERE block_number = ?\", (block_number,)\n",
    "        ).fetchone()\n",
    "        return result is not None\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def batch_insert_events(events, worker_id=\"main\"):\n",
    "    if not events:\n",
    "        return 0\n",
    "\n",
    "    transfers = []\n",
    "    swaps = []\n",
    "    mints = []\n",
    "    burns = []\n",
    "    collects = []\n",
    "    flashes = []\n",
    "    approvals = []\n",
    "\n",
    "    for e in events:\n",
    "        event_type = e.get(\"event\")\n",
    "        args = e.get(\"args\", {})\n",
    "\n",
    "        base_data = (\n",
    "            e[\"transactionHash\"],\n",
    "            e[\"logIndex\"],\n",
    "            e[\"blockNumber\"],\n",
    "            e[\"address\"],\n",
    "        )\n",
    "\n",
    "        if event_type == \"Transfer\":\n",
    "            transfers.append(\n",
    "                base_data\n",
    "                + (\n",
    "                    args.get(\"from\"),\n",
    "                    args.get(\"to\"),\n",
    "                    args.get(\"value\"),\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                )\n",
    "            )\n",
    "        elif event_type == \"Swap\":\n",
    "            swaps.append(\n",
    "                base_data\n",
    "                + (\n",
    "                    args.get(\"sender\"),\n",
    "                    args.get(\"recipient\"),\n",
    "                    args.get(\"amount0\"),\n",
    "                    args.get(\"amount1\"),\n",
    "                    args.get(\"sqrtPriceX96\"),\n",
    "                    args.get(\"liquidity\"),\n",
    "                    args.get(\"tick\"),\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                )\n",
    "            )\n",
    "        elif event_type == \"Mint\":\n",
    "            mints.append(\n",
    "                base_data\n",
    "                + (\n",
    "                    args.get(\"owner\"),\n",
    "                    args.get(\"tickLower\"),\n",
    "                    args.get(\"tickUpper\"),\n",
    "                    args.get(\"sender\"),\n",
    "                    args.get(\"amount\"),\n",
    "                    args.get(\"amount0\"),\n",
    "                    args.get(\"amount1\"),\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                )\n",
    "            )\n",
    "        elif event_type == \"Burn\":\n",
    "            burns.append(\n",
    "                base_data\n",
    "                + (\n",
    "                    args.get(\"owner\"),\n",
    "                    args.get(\"tickLower\"),\n",
    "                    args.get(\"tickUpper\"),\n",
    "                    args.get(\"amount\"),\n",
    "                    args.get(\"amount0\"),\n",
    "                    args.get(\"amount1\"),\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                )\n",
    "            )\n",
    "        elif event_type == \"Collect\":\n",
    "            collects.append(\n",
    "                base_data\n",
    "                + (\n",
    "                    args.get(\"owner\"),\n",
    "                    args.get(\"recipient\"),\n",
    "                    args.get(\"tickLower\"),\n",
    "                    args.get(\"tickUpper\"),\n",
    "                    args.get(\"amount0\"),\n",
    "                    args.get(\"amount1\"),\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                )\n",
    "            )\n",
    "        elif event_type == \"Flash\":\n",
    "            flashes.append(\n",
    "                base_data\n",
    "                + (\n",
    "                    args.get(\"sender\"),\n",
    "                    args.get(\"recipient\"),\n",
    "                    args.get(\"amount0\"),\n",
    "                    args.get(\"amount1\"),\n",
    "                    args.get(\"paid0\"),\n",
    "                    args.get(\"paid1\"),\n",
    "                    None,\n",
    "                )\n",
    "            )\n",
    "        elif event_type == \"Approval\":\n",
    "            approvals.append(\n",
    "                base_data\n",
    "                + (args.get(\"owner\"), args.get(\"spender\"), args.get(\"value\"), None)\n",
    "            )\n",
    "\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        if transfers:\n",
    "            conn.executemany(\n",
    "                \"\"\"INSERT INTO transfer\n",
    "                (transaction_hash, log_index, block_number, pair_address, from_address, to_address, value, value_normalized, block_timestamp, token0_symbol, token1_symbol)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "                transfers,\n",
    "            )\n",
    "        if swaps:\n",
    "            conn.executemany(\n",
    "                \"\"\"INSERT INTO swap\n",
    "                (transaction_hash, log_index, block_number, pair_address, sender, recipient, amount0, amount1, sqrt_price_x96, liquidity, tick, amount0_normalized, amount1_normalized, block_timestamp, token0_symbol, token1_symbol)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "                swaps,\n",
    "            )\n",
    "        if mints:\n",
    "            conn.executemany(\n",
    "                \"\"\"INSERT INTO mint\n",
    "                (transaction_hash, log_index, block_number, pair_address, owner, tick_lower, tick_upper, sender, amount, amount0, amount1, amount0_normalized, amount1_normalized, block_timestamp)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "                mints,\n",
    "            )\n",
    "        if burns:\n",
    "            conn.executemany(\n",
    "                \"\"\"INSERT INTO burn\n",
    "                (transaction_hash, log_index, block_number, pair_address, owner, tick_lower, tick_upper, amount, amount0, amount1, amount0_normalized, amount1_normalized, block_timestamp)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "                burns,\n",
    "            )\n",
    "        if collects:\n",
    "            conn.executemany(\n",
    "                \"\"\"INSERT INTO collect\n",
    "                (transaction_hash, log_index, block_number, pair_address, owner, recipient, tick_lower, tick_upper, amount0, amount1, amount0_normalized, amount1_normalized, block_timestamp)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "                collects,\n",
    "            )\n",
    "        if flashes:\n",
    "            conn.executemany(\n",
    "                \"\"\"INSERT INTO flash\n",
    "                (transaction_hash, log_index, block_number, pair_address, sender, recipient, amount0, amount1, paid0, paid1, block_timestamp)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "                flashes,\n",
    "            )\n",
    "        if approvals:\n",
    "            conn.executemany(\n",
    "                \"\"\"INSERT INTO approval\n",
    "                (transaction_hash, log_index, block_number, pair_address, owner, spender, value, block_timestamp)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "                approvals,\n",
    "            )\n",
    "\n",
    "        total_inserted = (\n",
    "            len(transfers)\n",
    "            + len(swaps)\n",
    "            + len(mints)\n",
    "            + len(burns)\n",
    "            + len(collects)\n",
    "            + len(flashes)\n",
    "            + len(approvals)\n",
    "        )\n",
    "        return total_inserted\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def batch_insert_pair_metadata(pairs_data):\n",
    "    if not pairs_data:\n",
    "        return 0\n",
    "\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        conn.executemany(\n",
    "            \"\"\"INSERT INTO pair_metadata\n",
    "            (pair_address, token0_address, token1_address, token0_symbol, token1_symbol, token0_decimals, token1_decimals, fee_tier, tick_spacing, created_block, last_updated)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, NOW())\n",
    "            ON CONFLICT (pair_address) DO UPDATE SET\n",
    "                token0_address = EXCLUDED.token0_address,\n",
    "                token1_address = EXCLUDED.token1_address,\n",
    "                token0_symbol = EXCLUDED.token0_symbol,\n",
    "                token1_symbol = EXCLUDED.token1_symbol,\n",
    "                token0_decimals = EXCLUDED.token0_decimals,\n",
    "                token1_decimals = EXCLUDED.token1_decimals,\n",
    "                fee_tier = EXCLUDED.fee_tier,\n",
    "                tick_spacing = EXCLUDED.tick_spacing,\n",
    "                created_block = EXCLUDED.created_block,\n",
    "                last_updated = NOW()\"\"\",\n",
    "            pairs_data,\n",
    "        )\n",
    "        return len(pairs_data)\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def batch_insert_block_metadata(blocks_data):\n",
    "    if not blocks_data:\n",
    "        return 0\n",
    "\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        conn.executemany(\n",
    "            \"\"\"INSERT INTO block_metadata (block_number, block_timestamp, block_hash)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT (block_number) DO NOTHING\"\"\",\n",
    "            blocks_data,\n",
    "        )\n",
    "        return len(blocks_data)\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def get_missing_block_metadata(block_numbers):\n",
    "    if not block_numbers:\n",
    "        return []\n",
    "\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        placeholders = \",\".join([\"?\" for _ in block_numbers])\n",
    "        existing = conn.execute(\n",
    "            f\"SELECT block_number FROM block_metadata WHERE block_number IN ({placeholders})\",\n",
    "            block_numbers,\n",
    "        ).fetchall()\n",
    "        existing_set = {b[0] for b in existing}\n",
    "        return [b for b in block_numbers if b not in existing_set]\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def get_pairs_missing_metadata():\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        all_pairs = conn.execute(\n",
    "            \"\"\"SELECT DISTINCT pair_address FROM (\n",
    "                SELECT pair_address FROM transfer UNION\n",
    "                SELECT pair_address FROM swap UNION\n",
    "                SELECT pair_address FROM mint UNION\n",
    "                SELECT pair_address FROM burn UNION\n",
    "                SELECT pair_address FROM collect UNION\n",
    "                SELECT pair_address FROM flash UNION\n",
    "                SELECT pair_address FROM approval\n",
    "            ) WHERE pair_address IS NOT NULL\"\"\"\n",
    "        ).fetchall()\n",
    "        all_pairs = {r[0] for r in all_pairs}\n",
    "\n",
    "        cached_pairs = conn.execute(\n",
    "            \"SELECT pair_address FROM pair_metadata WHERE token0_decimals IS NOT NULL AND token1_decimals IS NOT NULL\"\n",
    "        ).fetchall()\n",
    "        cached_pairs = {r[0] for r in cached_pairs}\n",
    "\n",
    "        return list(all_pairs - cached_pairs)\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def normalize_event_values(pair_address):\n",
    "    metadata = get_pair_metadata_cached(pair_address)\n",
    "    if (\n",
    "        not metadata\n",
    "        or metadata[\"token0_decimals\"] is None\n",
    "        or metadata[\"token1_decimals\"] is None\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    token0_decimals = metadata[\"token0_decimals\"]\n",
    "    token1_decimals = metadata[\"token1_decimals\"]\n",
    "\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        conn.execute(\n",
    "            \"\"\"UPDATE transfer\n",
    "            SET value_normalized = CAST(value AS DOUBLE) / POW(10, 18)\n",
    "            WHERE pair_address = ? AND value_normalized IS NULL\"\"\",\n",
    "            (pair_address,),\n",
    "        )\n",
    "\n",
    "        conn.execute(\n",
    "            \"\"\"UPDATE swap\n",
    "            SET amount0_normalized = CAST(amount0 AS DOUBLE) / POW(10, ?),\n",
    "                amount1_normalized = CAST(amount1 AS DOUBLE) / POW(10, ?)\n",
    "            WHERE pair_address = ? AND amount0_normalized IS NULL\"\"\",\n",
    "            (token0_decimals, token1_decimals, pair_address),\n",
    "        )\n",
    "\n",
    "        conn.execute(\n",
    "            \"\"\"UPDATE mint\n",
    "            SET amount0_normalized = CAST(amount0 AS DOUBLE) / POW(10, ?),\n",
    "                amount1_normalized = CAST(amount1 AS DOUBLE) / POW(10, ?)\n",
    "            WHERE pair_address = ? AND amount0_normalized IS NULL\"\"\",\n",
    "            (token0_decimals, token1_decimals, pair_address),\n",
    "        )\n",
    "\n",
    "        conn.execute(\n",
    "            \"\"\"UPDATE burn\n",
    "            SET amount0_normalized = CAST(amount0 AS DOUBLE) / POW(10, ?),\n",
    "                amount1_normalized = CAST(amount1 AS DOUBLE) / POW(10, ?)\n",
    "            WHERE pair_address = ? AND amount0_normalized IS NULL\"\"\",\n",
    "            (token0_decimals, token1_decimals, pair_address),\n",
    "        )\n",
    "\n",
    "        conn.execute(\n",
    "            \"\"\"UPDATE collect\n",
    "            SET amount0_normalized = CAST(amount0 AS DOUBLE) / POW(10, ?),\n",
    "                amount1_normalized = CAST(amount1 AS DOUBLE) / POW(10, ?)\n",
    "            WHERE pair_address = ? AND amount0_normalized IS NULL\"\"\",\n",
    "            (token0_decimals, token1_decimals, pair_address),\n",
    "        )\n",
    "\n",
    "        return True\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "def get_database_stats():\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        result = conn.execute(\n",
    "            \"\"\"SELECT\n",
    "                (SELECT COUNT(*) FROM transfer) as total_transfers,\n",
    "                (SELECT COUNT(*) FROM swap) as total_swaps,\n",
    "                (SELECT COUNT(*) FROM mint) as total_mints,\n",
    "                (SELECT COUNT(*) FROM burn) as total_burns,\n",
    "                (SELECT COUNT(*) FROM collect) as total_collects,\n",
    "                (SELECT COUNT(*) FROM flash) as total_flashes,\n",
    "                (SELECT COUNT(*) FROM approval) as total_approvals,\n",
    "                (SELECT COUNT(*) FROM processing_state WHERE status = 'completed') as completed_ranges,\n",
    "                (SELECT COUNT(*) FROM pair_metadata) as total_pairs,\n",
    "                (SELECT COUNT(*) FROM block_metadata) as total_blocks\n",
    "            \"\"\"\n",
    "        ).fetchone()\n",
    "\n",
    "        return {\n",
    "            \"total_transfers\": result[0],\n",
    "            \"total_swaps\": result[1],\n",
    "            \"total_mints\": result[2],\n",
    "            \"total_burns\": result[3],\n",
    "            \"total_collects\": result[4],\n",
    "            \"total_flashes\": result[5],\n",
    "            \"total_approvals\": result[6],\n",
    "            \"completed_ranges\": result[7],\n",
    "            \"total_pairs\": result[8],\n",
    "            \"total_blocks\": result[9],\n",
    "        }\n",
    "    finally:\n",
    "        DB_POOL.return_connection(conn)\n",
    "\n",
    "\n",
    "# Initialize optimizations with fixed connection pool\n",
    "ABI_OPTIMIZER = ABIIndexOptimizer(DB_POOL, ABI_CACHE_FOLDER)\n",
    "setup_token_metadata_storage(DB_POOL)\n",
    "\n",
    "print(\"✓ Database setup and core operations loaded\")\n",
    "print(f\"✓ Database initialized at: {DB_PATH}\")\n",
    "print(\n",
    "    f\"✓ Connection pool ready with {DB_POOL.size()} connections\"\n",
    ")  # FIXED: Use .size() instead of .pool.size()\n",
    "print(\"✓ No more SQLAlchemy warnings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dd62aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Multicall-optimized functions loaded - Maximum Infura efficiency!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5 - MULTICALL OPTIMIZED VERSION\n",
    "# Maximum performance with batched contract calls\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def generate_block_ranges(start_block, end_block, chunk_size):\n",
    "    completed_ranges = get_completed_ranges()\n",
    "    ranges = []\n",
    "    current = start_block\n",
    "    while current <= end_block:\n",
    "        range_end = min(current + chunk_size - 1, end_block)\n",
    "        if (current, range_end) not in completed_ranges:\n",
    "            ranges.append((current, range_end))\n",
    "        current += chunk_size\n",
    "    return ranges\n",
    "\n",
    "\n",
    "def multicall_batch_contract_calls(calls, provider=None, max_retries=3):\n",
    "    \"\"\"\n",
    "    Execute multiple contract calls in a single RPC request using Multicall3\n",
    "\n",
    "    calls: List of (target_address, call_data) tuples\n",
    "    Returns: List of (success, return_data) tuples\n",
    "    \"\"\"\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    if not calls:\n",
    "        return []\n",
    "\n",
    "    # Get or create multicall contract\n",
    "    multicall_contract = provider.eth.contract(\n",
    "        address=MULTICALL3_ADDRESS, abi=MULTICALL3_ABI\n",
    "    )\n",
    "\n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            # Format calls for multicall\n",
    "            multicall_calls = [\n",
    "                {\"target\": target, \"callData\": call_data} for target, call_data in calls\n",
    "            ]\n",
    "\n",
    "            # Execute multicall\n",
    "            block_number, return_data = multicall_contract.functions.aggregate(\n",
    "                multicall_calls\n",
    "            ).call()\n",
    "\n",
    "            # Parse results\n",
    "            results = []\n",
    "            for i, data in enumerate(return_data):\n",
    "                try:\n",
    "                    results.append((True, data))\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Failed to decode multicall result {i}: {e}\")\n",
    "                    results.append((False, None))\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e) or \"rate limit\" in str(e).lower():\n",
    "                if retry < max_retries - 1:\n",
    "                    wait_time = (2**retry) + random.uniform(0.5, 1.5)\n",
    "                    logging.warning(\n",
    "                        f\"Multicall rate limited, waiting {wait_time:.1f}s...\"\n",
    "                    )\n",
    "                    time.sleep(wait_time)\n",
    "                    provider, _ = PROVIDER_POOL.get_provider()\n",
    "                    continue\n",
    "\n",
    "            logging.error(f\"Multicall failed after {retry + 1} attempts: {e}\")\n",
    "            # Return failed results for all calls\n",
    "            return [(False, None)] * len(calls)\n",
    "\n",
    "    return [(False, None)] * len(calls)\n",
    "\n",
    "\n",
    "def fetch_uniswap_pair_metadata_multicall(pair_addresses, provider=None):\n",
    "    \"\"\"\n",
    "    Fetch metadata for multiple pairs using multicall for maximum efficiency\n",
    "    \"\"\"\n",
    "    if not pair_addresses:\n",
    "        return []\n",
    "\n",
    "    # Check cache first and filter out already cached pairs\n",
    "    cached_results = {}\n",
    "    uncached_addresses = []\n",
    "\n",
    "    for addr in pair_addresses:\n",
    "        cached = get_pair_metadata_cached(addr)\n",
    "        if cached and cached.get(\"token0_decimals\") is not None:\n",
    "            cached_results[addr] = cached\n",
    "        else:\n",
    "            uncached_addresses.append(addr)\n",
    "\n",
    "    if not uncached_addresses:\n",
    "        logging.info(f\"✓ All {len(pair_addresses)} pairs found in cache\")\n",
    "        return [cached_results.get(addr) for addr in pair_addresses]\n",
    "\n",
    "    logging.info(\n",
    "        f\"Fetching {len(uncached_addresses)} pairs via Multicall ({len(cached_results)} cached)\"\n",
    "    )\n",
    "\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    results = {}\n",
    "    results.update(cached_results)\n",
    "\n",
    "    # Process uncached pairs in batches to avoid huge multicalls\n",
    "    batch_size = 50  # Reasonable batch size for multicall\n",
    "\n",
    "    for i in range(0, len(uncached_addresses), batch_size):\n",
    "        batch_addresses = uncached_addresses[i : i + batch_size]\n",
    "        batch_results = _fetch_pair_batch_multicall(batch_addresses, provider)\n",
    "        results.update(batch_results)\n",
    "\n",
    "    # Return results in original order\n",
    "    return [results.get(addr) for addr in pair_addresses]\n",
    "\n",
    "\n",
    "def _fetch_pair_batch_multicall(pair_addresses, provider):\n",
    "    \"\"\"\n",
    "    Fetch a batch of pair metadata using multicall\n",
    "    \"\"\"\n",
    "    # Prepare multicall data\n",
    "    calls = []\n",
    "    call_map = {}  # Maps call index to (pair_address, call_type)\n",
    "\n",
    "    for pair_addr in pair_addresses:\n",
    "        pair_addr = provider.to_checksum_address(pair_addr)\n",
    "\n",
    "        # Create contract instances for encoding\n",
    "        pair_contract = provider.eth.contract(\n",
    "            address=pair_addr, abi=MINIMAL_UNISWAP_V3_POOL_ABI\n",
    "        )\n",
    "\n",
    "        # Add calls for each pair\n",
    "        base_idx = len(calls)\n",
    "\n",
    "        # Pool contract calls\n",
    "        calls.append(\n",
    "            (pair_addr, pair_contract.functions.token0().build_transaction()[\"data\"])\n",
    "        )\n",
    "        call_map[base_idx] = (pair_addr, \"token0\")\n",
    "\n",
    "        calls.append(\n",
    "            (pair_addr, pair_contract.functions.token1().build_transaction()[\"data\"])\n",
    "        )\n",
    "        call_map[base_idx + 1] = (pair_addr, \"token1\")\n",
    "\n",
    "        calls.append(\n",
    "            (pair_addr, pair_contract.functions.fee().build_transaction()[\"data\"])\n",
    "        )\n",
    "        call_map[base_idx + 2] = (pair_addr, \"fee\")\n",
    "\n",
    "        calls.append(\n",
    "            (\n",
    "                pair_addr,\n",
    "                pair_contract.functions.tickSpacing().build_transaction()[\"data\"],\n",
    "            )\n",
    "        )\n",
    "        call_map[base_idx + 3] = (pair_addr, \"tickSpacing\")\n",
    "\n",
    "    # Execute multicall\n",
    "    multicall_results = multicall_batch_contract_calls(calls, provider)\n",
    "\n",
    "    # Parse results into pair metadata\n",
    "    pair_data = {}\n",
    "    token_addresses = set()\n",
    "\n",
    "    for i, (success, data) in enumerate(multicall_results):\n",
    "        if not success or not data:\n",
    "            continue\n",
    "\n",
    "        pair_addr, call_type = call_map[i]\n",
    "\n",
    "        if pair_addr not in pair_data:\n",
    "            pair_data[pair_addr] = {\"pair_address\": pair_addr}\n",
    "\n",
    "        try:\n",
    "            if call_type == \"token0\":\n",
    "                decoded = provider.eth.codec.decode([\"address\"], data)[0]\n",
    "                pair_data[pair_addr][\"token0_address\"] = decoded\n",
    "                token_addresses.add(decoded)\n",
    "            elif call_type == \"token1\":\n",
    "                decoded = provider.eth.codec.decode([\"address\"], data)[0]\n",
    "                pair_data[pair_addr][\"token1_address\"] = decoded\n",
    "                token_addresses.add(decoded)\n",
    "            elif call_type == \"fee\":\n",
    "                decoded = provider.eth.codec.decode([\"uint24\"], data)[0]\n",
    "                pair_data[pair_addr][\"fee_tier\"] = decoded\n",
    "            elif call_type == \"tickSpacing\":\n",
    "                decoded = provider.eth.codec.decode([\"int24\"], data)[0]\n",
    "                pair_data[pair_addr][\"tick_spacing\"] = decoded\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to decode {call_type} for {pair_addr[:10]}: {e}\")\n",
    "\n",
    "    # Now fetch token metadata for all unique tokens using multicall\n",
    "    if token_addresses:\n",
    "        token_metadata = _fetch_token_metadata_multicall(\n",
    "            list(token_addresses), provider\n",
    "        )\n",
    "    else:\n",
    "        token_metadata = {}\n",
    "\n",
    "    # Combine pair and token data\n",
    "    final_results = {}\n",
    "    for pair_addr, data in pair_data.items():\n",
    "        token0_addr = data.get(\"token0_address\")\n",
    "        token1_addr = data.get(\"token1_address\")\n",
    "\n",
    "        if token0_addr and token1_addr:\n",
    "            token0_meta = token_metadata.get(token0_addr, {})\n",
    "            token1_meta = token_metadata.get(token1_addr, {})\n",
    "\n",
    "            final_results[pair_addr] = {\n",
    "                **data,\n",
    "                \"token0_symbol\": token0_meta.get(\"symbol\"),\n",
    "                \"token0_decimals\": token0_meta.get(\"decimals\"),\n",
    "                \"token1_symbol\": token1_meta.get(\"symbol\"),\n",
    "                \"token1_decimals\": token1_meta.get(\"decimals\"),\n",
    "            }\n",
    "\n",
    "    return final_results\n",
    "\n",
    "\n",
    "def _fetch_token_metadata_multicall(token_addresses, provider):\n",
    "    \"\"\"\n",
    "    Fetch token metadata for multiple tokens using multicall\n",
    "    \"\"\"\n",
    "    # Check cache first\n",
    "    cached_results = {}\n",
    "    uncached_addresses = []\n",
    "\n",
    "    for addr in token_addresses:\n",
    "        cached = get_token_metadata_optimized(addr, provider, DB_POOL)\n",
    "        if cached:\n",
    "            cached_results[addr] = cached\n",
    "        else:\n",
    "            uncached_addresses.append(addr)\n",
    "\n",
    "    if not uncached_addresses:\n",
    "        return cached_results\n",
    "\n",
    "    # Prepare multicall for uncached tokens\n",
    "    calls = []\n",
    "    call_map = {}\n",
    "\n",
    "    for token_addr in uncached_addresses:\n",
    "        token_addr = provider.to_checksum_address(token_addr)\n",
    "        token_contract = provider.eth.contract(\n",
    "            address=token_addr, abi=MINIMAL_ERC20_ABI\n",
    "        )\n",
    "\n",
    "        base_idx = len(calls)\n",
    "\n",
    "        calls.append(\n",
    "            (token_addr, token_contract.functions.symbol().build_transaction()[\"data\"])\n",
    "        )\n",
    "        call_map[base_idx] = (token_addr, \"symbol\")\n",
    "\n",
    "        calls.append(\n",
    "            (\n",
    "                token_addr,\n",
    "                token_contract.functions.decimals().build_transaction()[\"data\"],\n",
    "            )\n",
    "        )\n",
    "        call_map[base_idx + 1] = (token_addr, \"decimals\")\n",
    "\n",
    "        calls.append(\n",
    "            (token_addr, token_contract.functions.name().build_transaction()[\"data\"])\n",
    "        )\n",
    "        call_map[base_idx + 2] = (token_addr, \"name\")\n",
    "\n",
    "    # Execute multicall\n",
    "    multicall_results = multicall_batch_contract_calls(calls, provider)\n",
    "\n",
    "    # Parse token results\n",
    "    token_data = {}\n",
    "    for i, (success, data) in enumerate(multicall_results):\n",
    "        if not success or not data:\n",
    "            continue\n",
    "\n",
    "        token_addr, call_type = call_map[i]\n",
    "\n",
    "        if token_addr not in token_data:\n",
    "            token_data[token_addr] = {\"address\": token_addr}\n",
    "\n",
    "        try:\n",
    "            if call_type == \"symbol\":\n",
    "                decoded = provider.eth.codec.decode([\"string\"], data)[0]\n",
    "                token_data[token_addr][\"symbol\"] = decoded\n",
    "            elif call_type == \"decimals\":\n",
    "                decoded = provider.eth.codec.decode([\"uint8\"], data)[0]\n",
    "                token_data[token_addr][\"decimals\"] = decoded\n",
    "            elif call_type == \"name\":\n",
    "                decoded = provider.eth.codec.decode([\"string\"], data)[0]\n",
    "                token_data[token_addr][\"name\"] = decoded\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to decode {call_type} for {token_addr[:10]}: {e}\")\n",
    "\n",
    "    # Store successful results in cache\n",
    "    for token_addr, metadata in token_data.items():\n",
    "        if metadata.get(\"decimals\") is not None:\n",
    "            # Store in database cache\n",
    "            conn = DB_POOL.get_connection()\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO token_metadata_permanent (token_address, symbol, name, decimals, is_valid)\n",
    "                VALUES (?, ?, ?, ?, true)\n",
    "                ON CONFLICT (token_address) DO UPDATE SET\n",
    "                    symbol = EXCLUDED.symbol,\n",
    "                    name = EXCLUDED.name,\n",
    "                    decimals = EXCLUDED.decimals,\n",
    "                    is_valid = true,\n",
    "                    last_updated = NOW()\n",
    "            \"\"\",\n",
    "                (\n",
    "                    token_addr,\n",
    "                    metadata.get(\"symbol\"),\n",
    "                    metadata.get(\"name\"),\n",
    "                    metadata.get(\"decimals\"),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    # Combine cached and new results\n",
    "    results = {}\n",
    "    results.update(cached_results)\n",
    "    results.update(token_data)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def fetch_uniswap_pair_metadata(pair_address, provider=None):\n",
    "    \"\"\"\n",
    "    Single pair metadata fetch - optimized version that uses multicall for batches\n",
    "    For single calls, falls back to individual calls with cache check\n",
    "    \"\"\"\n",
    "    # Check cache first\n",
    "    cached_metadata = get_pair_metadata_cached(pair_address)\n",
    "    if cached_metadata and cached_metadata.get(\"token0_decimals\") is not None:\n",
    "        return cached_metadata\n",
    "\n",
    "    # For single pair, use multicall batch of 1\n",
    "    results = fetch_uniswap_pair_metadata_multicall([pair_address], provider)\n",
    "    return results[0] if results else None\n",
    "\n",
    "\n",
    "def fetch_block_metadata(block_number, provider=None, retry_count=0, max_retries=3):\n",
    "    # Check cache first\n",
    "    if is_block_metadata_cached(block_number):\n",
    "        return None  # Already cached\n",
    "\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    try:\n",
    "        block = provider.eth.get_block(block_number)\n",
    "        return (\n",
    "            block_number,\n",
    "            datetime.fromtimestamp(block[\"timestamp\"]),\n",
    "            block[\"hash\"].hex(),\n",
    "        )\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 429:\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = (2**retry_count) + random.uniform(0.5, 1.5)\n",
    "                logging.warning(\n",
    "                    f\"Rate limit (429) for block {block_number}, waiting {wait_time:.1f}s...\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                provider, _ = PROVIDER_POOL.get_provider()\n",
    "                return fetch_block_metadata(\n",
    "                    block_number, provider, retry_count + 1, max_retries\n",
    "                )\n",
    "            else:\n",
    "                raise Exception(f\"Max retries exceeded for block {block_number}\")\n",
    "        elif e.response.status_code == 402:\n",
    "            raise Exception(f\"Payment required (402) - Infura credits exhausted\")\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"HTTP {e.response.status_code} for block {block_number}: {e}\"\n",
    "            )\n",
    "            raise\n",
    "    except requests.exceptions.Timeout:\n",
    "        if retry_count < max_retries:\n",
    "            wait_time = (2**retry_count) + random.uniform(0.5, 1.5)\n",
    "            logging.warning(\n",
    "                f\"Timeout for block {block_number}, retrying in {wait_time:.1f}s...\"\n",
    "            )\n",
    "            time.sleep(wait_time)\n",
    "            provider, _ = PROVIDER_POOL.get_provider()\n",
    "            return fetch_block_metadata(\n",
    "                block_number, provider, retry_count + 1, max_retries\n",
    "            )\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"Timeout after {max_retries} retries for block {block_number}\"\n",
    "            )\n",
    "            raise\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        if retry_count < max_retries:\n",
    "            wait_time = (2**retry_count) + random.uniform(0.5, 1.5)\n",
    "            logging.warning(\n",
    "                f\"Connection error for block {block_number}, retrying in {wait_time:.1f}s...\"\n",
    "            )\n",
    "            time.sleep(wait_time)\n",
    "            provider, _ = PROVIDER_POOL.get_provider()\n",
    "            return fetch_block_metadata(\n",
    "                block_number, provider, retry_count + 1, max_retries\n",
    "            )\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"Connection failed after {max_retries} retries for block {block_number}\"\n",
    "            )\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error fetching block {block_number}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parallel_fetch_with_backoff(\n",
    "    items, fetch_func, max_workers=4, desc=\"Processing\", max_retries=3\n",
    "):\n",
    "    results = [None] * len(items)\n",
    "    results_lock = threading.Lock()\n",
    "    rate_limit_pause = threading.Event()\n",
    "\n",
    "    def worker(idx, item, retry_count=0):\n",
    "        rate_limit_pause.wait()\n",
    "\n",
    "        provider, provider_name = PROVIDER_POOL.get_provider()\n",
    "        try:\n",
    "            result = fetch_func(item, provider)\n",
    "            with results_lock:\n",
    "                results[idx] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            if (\n",
    "                \"429\" in error_str\n",
    "                or \"too many requests\" in error_str\n",
    "                or \"rate limit\" in error_str\n",
    "            ):\n",
    "                if retry_count < max_retries:\n",
    "                    rate_limit_pause.clear()\n",
    "                    wait_time = (2**retry_count) + random.uniform(1, 3)\n",
    "                    logging.warning(\n",
    "                        f\"{desc} rate limited for item {idx}, pausing all workers for {wait_time:.1f}s...\"\n",
    "                    )\n",
    "                    time.sleep(wait_time)\n",
    "                    rate_limit_pause.set()\n",
    "                    return worker(idx, item, retry_count + 1)\n",
    "                else:\n",
    "                    logging.error(\n",
    "                        f\"{desc} failed for item {idx} after {max_retries} retries: {e}\"\n",
    "                    )\n",
    "                    return None\n",
    "            else:\n",
    "                logging.warning(f\"{desc} failed for item {idx}: {e}\")\n",
    "                return None\n",
    "\n",
    "    rate_limit_pause.set()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(worker, i, item): i for i, item in enumerate(items)}\n",
    "        completed = 0\n",
    "        total = len(items)\n",
    "        for future in as_completed(futures):\n",
    "            completed += 1\n",
    "            if completed % 10 == 0 or completed == total:\n",
    "                logging.info(\n",
    "                    f\"{desc}: {completed}/{total} ({100*completed/total:.1f}%)\"\n",
    "                )\n",
    "\n",
    "    return [r for r in results if r is not None]\n",
    "\n",
    "\n",
    "def update_pool_current_state(pool_address):\n",
    "    conn = DB_POOL.get_connection()\n",
    "    latest_swap = conn.execute(\n",
    "        \"\"\"SELECT sqrt_price_x96, tick, liquidity, block_number\n",
    "        FROM swap WHERE pair_address = ?\n",
    "        ORDER BY block_number DESC, log_index DESC LIMIT 1\"\"\",\n",
    "        (pool_address,),\n",
    "    ).fetchone()\n",
    "\n",
    "    if not latest_swap:\n",
    "        return\n",
    "\n",
    "    total_stats = conn.execute(\n",
    "        \"\"\"SELECT COUNT(*) as total_swaps,\n",
    "        SUM(ABS(amount0_normalized)) as total_volume0,\n",
    "        SUM(ABS(amount1_normalized)) as total_volume1\n",
    "        FROM swap WHERE pair_address = ?\"\"\",\n",
    "        (pool_address,),\n",
    "    ).fetchone()\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"INSERT INTO pool_current_state (\n",
    "            pool_address, current_sqrt_price_x96, current_tick, current_liquidity,\n",
    "            last_swap_block, total_swaps, total_volume_token0, total_volume_token1, updated_at\n",
    "        ) VALUES (?, CAST(? AS HUGEINT), CAST(? AS HUGEINT), ?, ?, ?, ?, ?, NOW())\n",
    "        ON CONFLICT (pool_address) DO UPDATE SET\n",
    "            current_sqrt_price_x96 = EXCLUDED.current_sqrt_price_x96,\n",
    "            current_tick = EXCLUDED.current_tick,\n",
    "            current_liquidity = EXCLUDED.current_liquidity,\n",
    "            last_swap_block = EXCLUDED.last_swap_block,\n",
    "            total_swaps = EXCLUDED.total_swaps,\n",
    "            total_volume_token0 = EXCLUDED.total_volume_token0,\n",
    "            total_volume_token1 = EXCLUDED.total_volume_token1,\n",
    "            updated_at = NOW()\"\"\",\n",
    "        (\n",
    "            pool_address,\n",
    "            latest_swap[0],\n",
    "            latest_swap[1],\n",
    "            latest_swap[2],\n",
    "            latest_swap[3],\n",
    "            total_stats[0],\n",
    "            total_stats[1],\n",
    "            total_stats[2],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def denormalize_timestamps():\n",
    "    conn = DB_POOL.get_connection()\n",
    "    logging.info(\"Denormalizing timestamps...\")\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"UPDATE swap s SET block_timestamp = b.block_timestamp\n",
    "        FROM block_metadata b WHERE s.block_number = b.block_number AND s.block_timestamp IS NULL\"\"\"\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"UPDATE mint m SET block_timestamp = b.block_timestamp\n",
    "        FROM block_metadata b WHERE m.block_number = b.block_number AND m.block_timestamp IS NULL\"\"\"\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"UPDATE burn bn SET block_timestamp = b.block_timestamp\n",
    "        FROM block_metadata b WHERE bn.block_number = b.block_number AND bn.block_timestamp IS NULL\"\"\"\n",
    "    )\n",
    "\n",
    "    logging.info(\"✓ Timestamps denormalized\")\n",
    "\n",
    "\n",
    "def denormalize_pair_symbols():\n",
    "    conn = DB_POOL.get_connection()\n",
    "    logging.info(\"Denormalizing pair symbols...\")\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"UPDATE swap s SET token0_symbol = pm.token0_symbol, token1_symbol = pm.token1_symbol\n",
    "        FROM pair_metadata pm WHERE s.pair_address = pm.pair_address AND s.token0_symbol IS NULL\"\"\"\n",
    "    )\n",
    "\n",
    "    logging.info(\"✓ Pair symbols denormalized\")\n",
    "\n",
    "\n",
    "def refresh_pool_summary():\n",
    "    conn = DB_POOL.get_connection()\n",
    "    logging.info(\"Refreshing pool summary...\")\n",
    "\n",
    "    conn.execute(\"DROP TABLE IF EXISTS pool_summary\")\n",
    "    conn.execute(\n",
    "        \"\"\"CREATE TABLE pool_summary AS\n",
    "        SELECT pm.pair_address, pm.token0_symbol, pm.token1_symbol, pm.token0_decimals, pm.token1_decimals, pm.fee_tier,\n",
    "            COUNT(DISTINCT s.transaction_hash) as total_swaps,\n",
    "            COUNT(DISTINCT m.transaction_hash) as total_mints,\n",
    "            COUNT(DISTINCT b.transaction_hash) as total_burns,\n",
    "            SUM(ABS(s.amount0_normalized)) as total_volume_token0,\n",
    "            SUM(ABS(s.amount1_normalized)) as total_volume_token1,\n",
    "            MIN(s.block_number) as first_swap_block,\n",
    "            MAX(s.block_number) as last_swap_block\n",
    "        FROM pair_metadata pm\n",
    "        LEFT JOIN swap s ON pm.pair_address = s.pair_address\n",
    "        LEFT JOIN mint m ON pm.pair_address = m.pair_address\n",
    "        LEFT JOIN burn b ON pm.pair_address = b.pair_address\n",
    "        GROUP BY pm.pair_address, pm.token0_symbol, pm.token1_symbol, pm.token0_decimals, pm.token1_decimals, pm.fee_tier\"\"\"\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"CREATE INDEX idx_pool_summary_volume ON pool_summary(total_volume_token0)\"\n",
    "    )\n",
    "    conn.execute(\"CREATE INDEX idx_pool_summary_swaps ON pool_summary(total_swaps)\")\n",
    "    logging.info(\"✓ Pool summary refreshed\")\n",
    "\n",
    "\n",
    "def aggregate_all_pools(max_workers=8):\n",
    "    conn = DB_POOL.get_connection()\n",
    "    pools = conn.execute(\"SELECT DISTINCT pair_address FROM swap\").fetchall()\n",
    "    pools = [p[0] for p in pools]\n",
    "    logging.info(f\"Aggregating stats for {len(pools)} pools...\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(update_pool_current_state, pool) for pool in pools]\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to aggregate pool: {e}\")\n",
    "\n",
    "\n",
    "def generate_v3_pool_list(\n",
    "    output_file, start_block=FACTORY_DEPLOYMENT_BLOCK, max_workers=4\n",
    "):\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, \"r\") as f:\n",
    "            pools_dict = json.load(f)\n",
    "        max_block = max(\n",
    "            [p.get(\"created_block\", 0) for p in pools_dict.values()] or [start_block]\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Loaded {len(pools_dict)} existing pools, last block: {max_block:,}\"\n",
    "        )\n",
    "\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "        current_block = provider.eth.block_number\n",
    "        if max_block >= current_block - 10:\n",
    "            logging.info(f\"Pool list is up to date\")\n",
    "            return list(pools_dict.keys())\n",
    "\n",
    "        logging.info(\n",
    "            f\"Updating pool list from block {max_block + 1:,} to {current_block:,}\"\n",
    "        )\n",
    "        start_block = max_block + 1\n",
    "    else:\n",
    "        pools_dict = {}\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "        current_block = provider.eth.block_number\n",
    "\n",
    "    logging.info(\"Generating V3 pool list from PoolCreated events...\")\n",
    "    factory_abi = get_abi(UNISWAP_V3_FACTORY)\n",
    "    chunk_size = 10000\n",
    "    ranges = [\n",
    "        (fb, min(fb + chunk_size - 1, current_block))\n",
    "        for fb in range(start_block, current_block + 1, chunk_size)\n",
    "    ]\n",
    "\n",
    "    def fetch_pool_range(range_tuple, provider):\n",
    "        from_block, to_block = range_tuple\n",
    "        for retry in range(5):\n",
    "            try:\n",
    "                factory_contract = provider.eth.contract(\n",
    "                    address=UNISWAP_V3_FACTORY, abi=factory_abi\n",
    "                )\n",
    "                logs = factory_contract.events.PoolCreated.get_logs(\n",
    "                    from_block=from_block, to_block=to_block\n",
    "                )\n",
    "                pools = {}\n",
    "                for log in logs:\n",
    "                    pools[log.args.pool] = {\n",
    "                        \"token0\": log.args.token0,\n",
    "                        \"token1\": log.args.token1,\n",
    "                        \"fee\": log.args.fee,\n",
    "                        \"tickSpacing\": log.args.tickSpacing,\n",
    "                        \"created_block\": log.blockNumber,\n",
    "                    }\n",
    "                if pools:\n",
    "                    logging.info(\n",
    "                        f\"[{from_block:,} - {to_block:,}] Found {len(pools)} pools\"\n",
    "                    )\n",
    "                return pools\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e) or \"Too Many Requests\" in str(e):\n",
    "                    wait = min(10 * (retry + 1), 60)\n",
    "                    logging.warning(\n",
    "                        f\"Rate limit [{from_block:,}-{to_block:,}], retry in {wait}s\"\n",
    "                    )\n",
    "                    time.sleep(wait)\n",
    "                    provider, _ = PROVIDER_POOL.get_provider()\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(f\"Error [{from_block:,}-{to_block:,}]: {e}\")\n",
    "                    return {}\n",
    "        logging.error(f\"Failed [{from_block:,}-{to_block:,}] after 5 retries\")\n",
    "        return {}\n",
    "\n",
    "    all_pools_list = parallel_fetch_with_backoff(\n",
    "        ranges, fetch_pool_range, max_workers, \"Fetching pools\"\n",
    "    )\n",
    "    for pool_batch in all_pools_list:\n",
    "        pools_dict.update(pool_batch)\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_file) or \".\", exist_ok=True)\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(pools_dict, f, indent=2)\n",
    "\n",
    "    logging.info(f\"✓ Saved {len(pools_dict)} V3 pools to {output_file}\")\n",
    "    return list(pools_dict.keys())\n",
    "\n",
    "\n",
    "def fetch_and_store_block_metadata(block_numbers, max_workers=4):\n",
    "    if not block_numbers:\n",
    "        return 0\n",
    "\n",
    "    missing_blocks = [b for b in block_numbers if not is_block_metadata_cached(b)]\n",
    "    if not missing_blocks:\n",
    "        logging.info(\"✓ All blocks already cached\")\n",
    "        return 0\n",
    "\n",
    "    def fetch_single_block(block_num, provider):\n",
    "        try:\n",
    "            block = provider.eth.get_block(block_num)\n",
    "            return (\n",
    "                block_num,\n",
    "                datetime.fromtimestamp(block[\"timestamp\"]),\n",
    "                block[\"hash\"].hex(),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to fetch block {block_num}: {e}\")\n",
    "            return None\n",
    "\n",
    "    blocks_data = parallel_fetch_with_backoff(\n",
    "        missing_blocks,\n",
    "        fetch_single_block,\n",
    "        max_workers=max_workers,\n",
    "        desc=\"Fetching block metadata\",\n",
    "    )\n",
    "    blocks_data = [b for b in blocks_data if b is not None]\n",
    "\n",
    "    if blocks_data:\n",
    "        batch_insert_block_metadata(blocks_data)\n",
    "        logging.info(f\"✓ Stored metadata for {len(blocks_data)} blocks\")\n",
    "\n",
    "    return len(blocks_data)\n",
    "\n",
    "\n",
    "def collect_missing_pair_metadata(batch_size=100, provider=None, max_workers=12):\n",
    "    \"\"\"\n",
    "    MULTICALL OPTIMIZED: Collects pair metadata using batched multicalls\n",
    "    Reduces API calls by ~75% compared to individual calls\n",
    "    \"\"\"\n",
    "    conn = DB_POOL.get_connection()\n",
    "\n",
    "    all_pairs = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT DISTINCT pair_address FROM (\n",
    "            SELECT DISTINCT pair_address FROM transfer UNION\n",
    "            SELECT DISTINCT pair_address FROM swap UNION\n",
    "            SELECT DISTINCT pair_address FROM mint UNION\n",
    "            SELECT DISTINCT pair_address FROM burn UNION\n",
    "            SELECT DISTINCT pair_address FROM collect UNION\n",
    "            SELECT DISTINCT pair_address FROM flash\n",
    "        ) WHERE pair_address IS NOT NULL\"\"\"\n",
    "    ).fetchall()\n",
    "    all_pairs = [r[0] for r in all_pairs]\n",
    "\n",
    "    existing_pairs = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT pair_address FROM pair_metadata\n",
    "        WHERE token0_decimals IS NOT NULL AND token1_decimals IS NOT NULL\"\"\"\n",
    "    ).fetchall()\n",
    "    existing_pairs = set(r[0] for r in existing_pairs)\n",
    "\n",
    "    missing_pairs = [p for p in all_pairs if p not in existing_pairs]\n",
    "\n",
    "    if not missing_pairs:\n",
    "        logging.info(\"✓ All pairs already have metadata\")\n",
    "        return\n",
    "\n",
    "    logging.info(\n",
    "        f\"Found {len(missing_pairs)} pairs missing metadata out of {len(all_pairs)} total\"\n",
    "    )\n",
    "    logging.info(\"🚀 Using MULTICALL optimization for maximum speed...\")\n",
    "\n",
    "    successful = 0\n",
    "    total_batches = (len(missing_pairs) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(0, len(missing_pairs), batch_size):\n",
    "        batch = missing_pairs[i : i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        logging.info(\n",
    "            f\"Processing batch {batch_num}/{total_batches} ({len(batch)} pairs) via Multicall\"\n",
    "        )\n",
    "\n",
    "        # Use multicall batch fetching\n",
    "        try:\n",
    "            batch_results = fetch_uniswap_pair_metadata_multicall(batch, provider)\n",
    "\n",
    "            # Store successful results\n",
    "            pairs_data = []\n",
    "            for pair_addr, metadata in zip(batch, batch_results):\n",
    "                if metadata and metadata.get(\"token0_decimals\") is not None:\n",
    "                    pairs_data.append(\n",
    "                        (\n",
    "                            metadata[\"pair_address\"],\n",
    "                            metadata[\"token0_address\"],\n",
    "                            metadata[\"token1_address\"],\n",
    "                            metadata.get(\"token0_symbol\"),\n",
    "                            metadata.get(\"token1_symbol\"),\n",
    "                            metadata.get(\"token0_decimals\"),\n",
    "                            metadata.get(\"token1_decimals\"),\n",
    "                            metadata.get(\"fee_tier\"),\n",
    "                            metadata.get(\"tick_spacing\"),\n",
    "                            None,\n",
    "                        )\n",
    "                    )\n",
    "                    successful += 1\n",
    "\n",
    "            if pairs_data:\n",
    "                batch_insert_pair_metadata(pairs_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Batch {batch_num} failed: {e}\")\n",
    "\n",
    "    logging.info(\n",
    "        f\"✓ Multicall metadata collection complete: {successful} successful out of {len(missing_pairs)} pairs\"\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"🎯 Estimated API call reduction: ~75% (Multicall vs individual calls)\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_missing_pairs(max_workers=8):\n",
    "    conn = DB_POOL.get_connection()\n",
    "    pairs_with_metadata = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT pair_address FROM pair_metadata\n",
    "        WHERE token0_decimals IS NOT NULL AND token1_decimals IS NOT NULL\"\"\"\n",
    "    ).fetchall()\n",
    "\n",
    "    if not pairs_with_metadata:\n",
    "        logging.warning(\n",
    "            \"No pairs with metadata found - run collect_missing_pair_metadata() first\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    pairs_to_normalize = []\n",
    "    for (pair_address,) in pairs_with_metadata:\n",
    "        needs_norm = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT COUNT(*) FROM transfer\n",
    "            WHERE pair_address = ? AND value_normalized IS NULL LIMIT 1\"\"\",\n",
    "            (pair_address,),\n",
    "        ).fetchone()[0]\n",
    "        if needs_norm > 0:\n",
    "            pairs_to_normalize.append(pair_address)\n",
    "\n",
    "    if not pairs_to_normalize:\n",
    "        logging.info(\"✓ All pairs already normalized\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Normalizing {len(pairs_to_normalize)} pairs...\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_pair = {\n",
    "            executor.submit(normalize_event_values, pair): pair\n",
    "            for pair in pairs_to_normalize\n",
    "        }\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_pair):\n",
    "            try:\n",
    "                future.result()\n",
    "                completed += 1\n",
    "                if completed % 20 == 0 or completed == len(pairs_to_normalize):\n",
    "                    logging.info(\n",
    "                        f\"Progress: {completed}/{len(pairs_to_normalize)} pairs normalized\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to normalize pair: {e}\")\n",
    "\n",
    "    logging.info(\"✓ Normalization complete\")\n",
    "\n",
    "\n",
    "def fetch_logs_for_range(\n",
    "    start_block, end_block, addresses, worker_id=\"main\", retry_count=0, max_retries=5\n",
    "):\n",
    "    provider, provider_name = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    try:\n",
    "        params = {\"fromBlock\": start_block, \"toBlock\": end_block, \"address\": addresses}\n",
    "        logs = provider.eth.get_logs(params)\n",
    "\n",
    "        logs_by_address = {}\n",
    "        for log in logs:\n",
    "            addr = log[\"address\"]\n",
    "            if addr not in logs_by_address:\n",
    "                logs_by_address[addr] = []\n",
    "            logs_by_address[addr].append(log)\n",
    "\n",
    "        transactions = []\n",
    "        for contract_address, contract_logs in logs_by_address.items():\n",
    "            decoded_logs = decode_logs_for_contract(\n",
    "                contract_address, contract_logs, provider\n",
    "            )\n",
    "            transactions.extend(decoded_logs)\n",
    "\n",
    "        logging.info(\n",
    "            f\"[{worker_id}] [{provider_name}] Fetched {len(transactions)} events from blocks [{start_block:,} - {end_block:,}]\"\n",
    "        )\n",
    "        return transactions\n",
    "\n",
    "    except HTTPError as e:\n",
    "        if e.response.status_code == 413:\n",
    "            logging.warning(\n",
    "                f\"[{worker_id}] [{provider_name}] Response too large (413) for range [{start_block:,} - {end_block:,}] - will split\"\n",
    "            )\n",
    "            raise Web3RPCError(\"Response payload too large - splitting range\")\n",
    "        elif e.response.status_code == 429:\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = min((2**retry_count) + random.uniform(1, 3), 30)\n",
    "                logging.warning(\n",
    "                    f\"[{worker_id}] [{provider_name}] Rate limit hit, waiting {wait_time:.1f}s...\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                return fetch_logs_for_range(\n",
    "                    start_block,\n",
    "                    end_block,\n",
    "                    addresses,\n",
    "                    worker_id,\n",
    "                    retry_count + 1,\n",
    "                    max_retries,\n",
    "                )\n",
    "            else:\n",
    "                logging.error(f\"[{worker_id}] Max retries reached\")\n",
    "                raise\n",
    "        elif e.response.status_code == 402:\n",
    "            logging.critical(f\"[{worker_id}] Payment required (402)\")\n",
    "            raise\n",
    "        else:\n",
    "            logging.error(f\"[{worker_id}] HTTP error {e.response.status_code}: {e}\")\n",
    "            raise\n",
    "\n",
    "    except Web3RPCError as e:\n",
    "        if (\n",
    "            \"more than 10000 results\" in str(e)\n",
    "            or \"-32005\" in str(e)\n",
    "            or \"Response payload too large\" in str(e)\n",
    "        ):\n",
    "            raise\n",
    "        else:\n",
    "            logging.error(f\"[{worker_id}] Web3 RPC error: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def collect_block_metadata_for_range(start_block, end_block, worker_id=\"main\"):\n",
    "    conn = DB_POOL.get_connection()\n",
    "    existing_blocks = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT block_number FROM block_metadata\n",
    "        WHERE block_number BETWEEN ? AND ?\"\"\",\n",
    "        (start_block, end_block),\n",
    "    ).fetchall()\n",
    "    existing_blocks = {b[0] for b in existing_blocks}\n",
    "\n",
    "    missing_blocks = [\n",
    "        b for b in range(start_block, end_block + 1) if b not in existing_blocks\n",
    "    ]\n",
    "    if not missing_blocks:\n",
    "        return 0\n",
    "\n",
    "    provider, provider_name = PROVIDER_POOL.get_provider()\n",
    "    blocks_data = []\n",
    "    for block_num in missing_blocks:\n",
    "        try:\n",
    "            block = provider.eth.get_block(block_num)\n",
    "            blocks_data.append(\n",
    "                (\n",
    "                    block_num,\n",
    "                    datetime.fromtimestamp(block[\"timestamp\"]),\n",
    "                    block[\"hash\"].hex(),\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[{worker_id}] Failed to fetch block {block_num}: {e}\")\n",
    "\n",
    "    if blocks_data:\n",
    "        batch_insert_block_metadata(blocks_data)\n",
    "        logging.debug(f\"[{worker_id}] Stored metadata for {len(blocks_data)} blocks\")\n",
    "\n",
    "    return len(blocks_data)\n",
    "\n",
    "\n",
    "def process_block_range(start_block, end_block, addresses, worker_id=\"main\"):\n",
    "    if (start_block, end_block) in get_completed_ranges():\n",
    "        logging.debug(\n",
    "            f\"[{worker_id}] Skipping already processed range [{start_block:,}, {end_block:,}]\"\n",
    "        )\n",
    "        return 0\n",
    "\n",
    "    mark_range_processing(start_block, end_block, worker_id)\n",
    "\n",
    "    try:\n",
    "        events = fetch_logs_for_range(start_block, end_block, addresses, worker_id)\n",
    "        batch_insert_events(events, worker_id)\n",
    "        mark_range_completed(start_block, end_block, worker_id)\n",
    "        logging.debug(\n",
    "            f\"[{worker_id}] ✓ Processed [{start_block:,}, {end_block:,}] - {len(events)} events\"\n",
    "        )\n",
    "        return len(events)\n",
    "\n",
    "    except (Web3RPCError, HTTPError) as e:\n",
    "        if isinstance(e, Web3RPCError):\n",
    "            error_msg = (\n",
    "                e.args[0].get(\"message\", str(e))\n",
    "                if e.args and isinstance(e.args[0], dict)\n",
    "                else str(e)\n",
    "            )\n",
    "        else:\n",
    "            error_msg = str(e)\n",
    "\n",
    "        if (\n",
    "            \"more than 10000 results\" in error_msg\n",
    "            or \"-32005\" in error_msg\n",
    "            or \"Response payload too large\" in error_msg\n",
    "            or (hasattr(e, \"response\") and e.response.status_code == 413)\n",
    "        ):\n",
    "\n",
    "            mid = (start_block + end_block) // 2\n",
    "            if mid == start_block:\n",
    "                logging.error(\n",
    "                    f\"[{worker_id}] Cannot split range [{start_block:,}, {end_block:,}] further - skipping\"\n",
    "                )\n",
    "                mark_range_completed(start_block, end_block, worker_id)\n",
    "                return 0\n",
    "\n",
    "            logging.info(\n",
    "                f\"[{worker_id}] Splitting [{start_block:,}, {end_block:,}] at {mid:,} (reason: {error_msg})\"\n",
    "            )\n",
    "            count1 = process_block_range(start_block, mid, addresses, worker_id)\n",
    "            count2 = process_block_range(mid + 1, end_block, addresses, worker_id)\n",
    "            return count1 + count2\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"[{worker_id}] Failed to process [{start_block:,}, {end_block:,}]: {error_msg}\"\n",
    "            )\n",
    "            return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"[{worker_id}] Unexpected error [{start_block:,}, {end_block:,}]: {e}\"\n",
    "        )\n",
    "        return 0\n",
    "\n",
    "\n",
    "def scan_blockchain(addresses, start_block, end_block, chunk_size=10000, max_workers=3):\n",
    "    ranges = generate_block_ranges(start_block, end_block, chunk_size)\n",
    "    if not ranges:\n",
    "        logging.info(\"No ranges to process - all already completed!\")\n",
    "        return\n",
    "\n",
    "    total_ranges = len(ranges)\n",
    "    logging.info(f\"Processing {total_ranges} block ranges with {max_workers} workers\")\n",
    "\n",
    "    total_events = 0\n",
    "    completed_ranges = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_range = {\n",
    "            executor.submit(\n",
    "                process_block_range, start, end, addresses, f\"worker-{i % max_workers}\"\n",
    "            ): (start, end, i)\n",
    "            for i, (start, end) in enumerate(ranges)\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_range):\n",
    "            start, end, idx = future_to_range[future]\n",
    "            try:\n",
    "                event_count = future.result()\n",
    "                total_events += event_count\n",
    "                completed_ranges += 1\n",
    "\n",
    "                progress = (completed_ranges / total_ranges) * 100\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = completed_ranges / elapsed if elapsed > 0 else 0\n",
    "                eta_seconds = (\n",
    "                    (total_ranges - completed_ranges) / rate if rate > 0 else 0\n",
    "                )\n",
    "                eta_str = f\"{int(eta_seconds // 60)}m {int(eta_seconds % 60)}s\"\n",
    "\n",
    "                logging.info(\n",
    "                    f\"Progress: {completed_ranges}/{total_ranges} ({progress:.1f}%) | \"\n",
    "                    f\"Events: {total_events:,} | Rate: {rate:.1f} ranges/s | ETA: {eta_str}\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Range [{start:,}, {end:,}] failed: {e}\")\n",
    "\n",
    "    elapsed_total = time.time() - start_time\n",
    "    logging.info(f\"\\n{'='*60}\")\n",
    "    logging.info(f\"Scan completed!\")\n",
    "    logging.info(f\"Total events fetched: {total_events:,}\")\n",
    "    logging.info(f\"Ranges processed: {completed_ranges}/{total_ranges}\")\n",
    "    logging.info(f\"Total time: {int(elapsed_total // 60)}m {int(elapsed_total % 60)}s\")\n",
    "    logging.info(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "def scan_blockchain_to_duckdb(\n",
    "    start_block=FACTORY_DEPLOYMENT_BLOCK,\n",
    "    end_block=20000000,\n",
    "    chunk_size=10000,\n",
    "    max_workers=3,\n",
    "    token_filter=None,\n",
    "):\n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(\"BLOCKCHAIN SCANNER - MULTICALL OPTIMIZED\")\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    pool_list_file = f\"{BASE_OUTPUT_DIR}/uniswap_v3_pools.json\"\n",
    "    all_addresses = generate_v3_pool_list(pool_list_file)\n",
    "\n",
    "    if token_filter:\n",
    "        filter_checksummed = [Web3.to_checksum_address(addr) for addr in token_filter]\n",
    "        addresses = [addr for addr in all_addresses if addr in filter_checksummed]\n",
    "        logging.info(f\"Filtered: {len(addresses)}/{len(all_addresses)} addresses\")\n",
    "    else:\n",
    "        addresses = all_addresses\n",
    "\n",
    "    logging.info(f\"Total addresses: {len(addresses)}\")\n",
    "    stats = get_database_stats()\n",
    "    logging.info(\n",
    "        f\"Blocks: {start_block:,} → {end_block:,} | Chunk: {chunk_size:,} | Workers: {max_workers}\"\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"DB: {stats['total_transfers']:,} transfers, {stats['total_swaps']:,} swaps, {stats['completed_ranges']} ranges done\"\n",
    "    )\n",
    "    logging.info(\"🚀 MULTICALL optimization active for metadata collection\")\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        scan_blockchain(addresses, start_block, end_block, chunk_size, max_workers)\n",
    "        final_stats = get_database_stats()\n",
    "        logging.info(\"=\" * 60)\n",
    "        logging.info(\"SCAN COMPLETE\")\n",
    "        logging.info(\n",
    "            f\"Transfers: {final_stats['total_transfers']:,} | Swaps: {final_stats['total_swaps']:,}\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Mints: {final_stats['total_mints']:,} | Burns: {final_stats['total_burns']:,}\"\n",
    "        )\n",
    "        logging.info(\"=\" * 60)\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"\\nInterrupted - progress saved to database\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def print_database_summary():\n",
    "    stats = get_database_stats()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATABASE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Events:\")\n",
    "    print(f\"  Transfers:  {stats['total_transfers']:>12,}\")\n",
    "    print(f\"  Swaps:      {stats['total_swaps']:>12,}\")\n",
    "    print(f\"  Mints:      {stats['total_mints']:>12,}\")\n",
    "    print(f\"  Burns:      {stats['total_burns']:>12,}\")\n",
    "    print(f\"  Collects:   {stats['total_collects']:>12,}\")\n",
    "    print(f\"  Flashes:    {stats['total_flashes']:>12,}\")\n",
    "    print(f\"  Approvals:  {stats['total_approvals']:>12,}\")\n",
    "    total_events = (\n",
    "        stats[\"total_transfers\"]\n",
    "        + stats[\"total_swaps\"]\n",
    "        + stats[\"total_mints\"]\n",
    "        + stats[\"total_burns\"]\n",
    "        + stats[\"total_collects\"]\n",
    "        + stats[\"total_flashes\"]\n",
    "        + stats[\"total_approvals\"]\n",
    "    )\n",
    "    print(f\"  TOTAL:      {total_events:>12,}\")\n",
    "    print(f\"\\nMetadata:\")\n",
    "    print(f\"  Pairs:      {stats['total_pairs']:>12,}\")\n",
    "    print(f\"  Blocks:     {stats['total_blocks']:>12,}\")\n",
    "    print(f\"  Ranges:     {stats['completed_ranges']:>12,}\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "\n",
    "def run_full_pipeline(\n",
    "    start_block=FACTORY_DEPLOYMENT_BLOCK,\n",
    "    end_block=None,\n",
    "    chunk_size=10000,\n",
    "    max_workers=3,\n",
    "    token_filter=None,\n",
    "):\n",
    "    try:\n",
    "        logging.info(\"=\" * 80)\n",
    "        logging.info(\"UNISWAP V3 DATA PIPELINE - MULTICALL OPTIMIZED\")\n",
    "        logging.info(\"=\" * 80)\n",
    "\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "        if end_block is None:\n",
    "            end_block = provider.eth.block_number\n",
    "            logging.info(f\"Using current block as end: {end_block:,}\")\n",
    "\n",
    "        logging.info(f\"Block range: {start_block:,} → {end_block:,}\")\n",
    "        logging.info(f\"Configuration: chunk={chunk_size:,}, workers={max_workers}\")\n",
    "        logging.info(\"🚀 Multicall optimizations: ACTIVE\")\n",
    "\n",
    "        logging.info(\"\\n\" + \"=\" * 80)\n",
    "        logging.info(\"STAGE 1: BLOCKCHAIN SCANNING\")\n",
    "        logging.info(\"=\" * 80)\n",
    "        scan_blockchain_to_duckdb(\n",
    "            start_block=start_block,\n",
    "            end_block=end_block,\n",
    "            chunk_size=chunk_size,\n",
    "            max_workers=max_workers,\n",
    "            token_filter=token_filter,\n",
    "        )\n",
    "        print_database_summary()\n",
    "\n",
    "        logging.info(\"\\n\" + \"=\" * 80)\n",
    "        logging.info(\"STAGE 2: METADATA COLLECTION (MULTICALL OPTIMIZED)\")\n",
    "        logging.info(\"=\" * 80)\n",
    "        logging.info(\"\\n📦 Collecting pair metadata via Multicall...\")\n",
    "        collect_missing_pair_metadata(batch_size=50, max_workers=4)\n",
    "        print_database_summary()\n",
    "\n",
    "        logging.info(\"\\n\" + \"=\" * 80)\n",
    "        logging.info(\"STAGE 3: VALUE NORMALIZATION\")\n",
    "        logging.info(\"=\" * 80)\n",
    "        normalize_missing_pairs(max_workers=8)\n",
    "        print_database_summary()\n",
    "\n",
    "        logging.info(\"\\n\" + \"=\" * 80)\n",
    "        logging.info(\"STAGE 4: AGGREGATION & OPTIMIZATION\")\n",
    "        logging.info(\"=\" * 80)\n",
    "        logging.info(\"\\n📊 Denormalizing timestamps...\")\n",
    "        denormalize_timestamps()\n",
    "        logging.info(\"\\n📊 Denormalizing pair symbols...\")\n",
    "        denormalize_pair_symbols()\n",
    "        logging.info(\"\\n📊 Refreshing pool summary...\")\n",
    "        refresh_pool_summary()\n",
    "        logging.info(\"\\n📊 Updating pool current state...\")\n",
    "        aggregate_all_pools(max_workers=8)\n",
    "\n",
    "        logging.info(\"\\n\" + \"=\" * 80)\n",
    "        logging.info(\"✅ PIPELINE COMPLETE - MULTICALL OPTIMIZED\")\n",
    "        logging.info(\"=\" * 80)\n",
    "        print_database_summary()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"\\n⚠️  INTERRUPTED - Progress saved\")\n",
    "        print_database_summary()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"\\n❌ ERROR: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        raise\n",
    "    finally:\n",
    "        if TOKEN_CACHE:\n",
    "            TOKEN_CACHE.flush()\n",
    "        if DB_POOL:\n",
    "            DB_POOL.close_all()\n",
    "\n",
    "\n",
    "print(\"🚀 Multicall-optimized functions loaded - Maximum Infura efficiency!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "308b804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦄 Uniswap V3 Pipeline Orchestrator Loaded!\n",
      "==================================================\n",
      "🚀 Ready to process Uniswap V3 data with multicall optimization\n",
      "📊 Available quick commands:\n",
      "   • quick_start() - Interactive menu\n",
      "   • dev_mode() - Developer mode\n",
      "   • preset_daily_maintenance() - Daily sync\n",
      "   • preset_quick_analysis() - Status overview\n",
      "💡 All systems initialized and ready!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6 - FINAL COMPREHENSIVE ORCHESTRATOR\n",
    "# Integrates all previous cells into a complete pipeline management system\n",
    "# Run this cell after executing cells 1-5 in sequence\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def print_pipeline_banner():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"🦄 UNISWAP V3 DATA PIPELINE - MULTICALL OPTIMIZED\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"📊 Comprehensive ETL Pipeline for Uniswap V3 Pool Data\")\n",
    "    print(\"🚀 Featuring Multicall optimization for maximum efficiency\")\n",
    "    print(\"💾 DuckDB analytics database with full normalization\")\n",
    "    print(\"⚡ Multi-threaded processing with intelligent rate limiting\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def get_pipeline_status():\n",
    "    stats = get_database_stats()\n",
    "    current_block = w3.eth.block_number\n",
    "\n",
    "    print(\"\\n📋 CURRENT PIPELINE STATUS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"🔗 Latest Ethereum Block: {current_block:,}\")\n",
    "    print(f\"📦 Database Events:\")\n",
    "    print(f\"   • Transfers:  {stats['total_transfers']:>12,}\")\n",
    "    print(f\"   • Swaps:      {stats['total_swaps']:>12,}\")\n",
    "    print(f\"   • Mints:      {stats['total_mints']:>12,}\")\n",
    "    print(f\"   • Burns:      {stats['total_burns']:>12,}\")\n",
    "    print(f\"   • Collects:   {stats['total_collects']:>12,}\")\n",
    "    print(f\"   • Flashes:    {stats['total_flashes']:>12,}\")\n",
    "\n",
    "    total_events = (\n",
    "        stats[\"total_transfers\"]\n",
    "        + stats[\"total_swaps\"]\n",
    "        + stats[\"total_mints\"]\n",
    "        + stats[\"total_burns\"]\n",
    "        + stats[\"total_collects\"]\n",
    "        + stats[\"total_flashes\"]\n",
    "    )\n",
    "\n",
    "    print(f\"   • TOTAL:      {total_events:>12,}\")\n",
    "    print(f\"🏊 Pools Tracked: {stats['total_pairs']:,}\")\n",
    "    print(f\"🧱 Block Metadata: {stats['total_blocks']:,}\")\n",
    "    print(f\"✅ Completed Ranges: {stats['completed_ranges']:,}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return stats, current_block\n",
    "\n",
    "\n",
    "def show_optimization_stats():\n",
    "    print(\"\\n🚀 OPTIMIZATION STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    print_optimization_stats()\n",
    "\n",
    "\n",
    "def quick_scan_recent_blocks(hours_back=1, max_workers=3):\n",
    "    print(f\"\\n⚡ QUICK SCAN - Last {hours_back} Hour{'s' if hours_back != 1 else ''}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    current_block = w3.eth.block_number\n",
    "    blocks_per_hour = 300\n",
    "    start_block = max(\n",
    "        current_block - (hours_back * blocks_per_hour), FACTORY_DEPLOYMENT_BLOCK\n",
    "    )\n",
    "\n",
    "    print(f\"📍 Scanning blocks {start_block:,} → {current_block:,}\")\n",
    "\n",
    "    scan_blockchain_to_duckdb(\n",
    "        start_block=start_block,\n",
    "        end_block=current_block,\n",
    "        chunk_size=1000,\n",
    "        max_workers=max_workers,\n",
    "    )\n",
    "\n",
    "\n",
    "def comprehensive_metadata_update():\n",
    "    print(\"\\n📦 COMPREHENSIVE METADATA UPDATE\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"🔍 Collecting missing pair metadata...\")\n",
    "    collect_missing_pair_metadata(batch_size=100, max_workers=8)\n",
    "\n",
    "    print(\"📊 Normalizing event values...\")\n",
    "    normalize_missing_pairs(max_workers=8)\n",
    "\n",
    "    print(\"⏰ Denormalizing timestamps...\")\n",
    "    denormalize_timestamps()\n",
    "\n",
    "    print(\"🏷️  Denormalizing pair symbols...\")\n",
    "    denormalize_pair_symbols()\n",
    "\n",
    "    print(\"📈 Refreshing pool summaries...\")\n",
    "    refresh_pool_summary()\n",
    "\n",
    "    print(\"🎯 Updating pool current states...\")\n",
    "    aggregate_all_pools(max_workers=8)\n",
    "\n",
    "    print(\"✅ Metadata update complete!\")\n",
    "\n",
    "\n",
    "def run_targeted_pool_scan(pool_addresses, start_block=None, end_block=None):\n",
    "    if not pool_addresses:\n",
    "        print(\"❌ No pool addresses provided\")\n",
    "        return\n",
    "\n",
    "    if start_block is None:\n",
    "        start_block = FACTORY_DEPLOYMENT_BLOCK\n",
    "    if end_block is None:\n",
    "        end_block = w3.eth.block_number\n",
    "\n",
    "    print(f\"\\n🎯 TARGETED POOL SCAN\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"🏊 Pools: {len(pool_addresses)}\")\n",
    "    print(f\"📍 Blocks: {start_block:,} → {end_block:,}\")\n",
    "\n",
    "    scan_blockchain_to_duckdb(\n",
    "        start_block=start_block,\n",
    "        end_block=end_block,\n",
    "        chunk_size=10000,\n",
    "        max_workers=3,\n",
    "        token_filter=pool_addresses,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_historical_backfill(\n",
    "    start_block=None, end_block=None, chunk_size=10000, max_workers=3\n",
    "):\n",
    "    if start_block is None:\n",
    "        start_block = FACTORY_DEPLOYMENT_BLOCK\n",
    "    if end_block is None:\n",
    "        end_block = w3.eth.block_number\n",
    "\n",
    "    print(f\"\\n📚 HISTORICAL BACKFILL\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"📍 Block Range: {start_block:,} → {end_block:,}\")\n",
    "    print(f\"⚙️  Configuration: chunk={chunk_size:,}, workers={max_workers}\")\n",
    "\n",
    "    run_full_pipeline(\n",
    "        start_block=start_block,\n",
    "        end_block=end_block,\n",
    "        chunk_size=chunk_size,\n",
    "        max_workers=max_workers,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_top_pools_by_volume(limit=20):\n",
    "    conn = DB_POOL.get_connection()\n",
    "    try:\n",
    "        top_pools = conn.execute(\n",
    "            f\"\"\"\n",
    "            SELECT pair_address, token0_symbol, token1_symbol, fee_tier,\n",
    "                   total_volume_token0, total_volume_token1, total_swaps\n",
    "            FROM pool_summary\n",
    "            WHERE total_volume_token0 > 0 AND token0_symbol IS NOT NULL\n",
    "            ORDER BY total_volume_token0 DESC\n",
    "            LIMIT {limit}\n",
    "        \"\"\"\n",
    "        ).fetchall()\n",
    "\n",
    "        print(f\"\\n🏆 TOP {limit} POOLS BY VOLUME\")\n",
    "        print(\"=\" * 100)\n",
    "        print(\n",
    "            f\"{'Rank':<4} {'Pool':<12} {'Pair':<20} {'Fee':<6} {'Volume Token0':<15} {'Swaps':<10}\"\n",
    "        )\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        for i, pool in enumerate(top_pools, 1):\n",
    "            (\n",
    "                pair_address,\n",
    "                token0_symbol,\n",
    "                token1_symbol,\n",
    "                fee_tier,\n",
    "                volume0,\n",
    "                volume1,\n",
    "                swaps,\n",
    "            ) = pool\n",
    "            pair_name = f\"{token0_symbol or 'UNKNOWN'}/{token1_symbol or 'UNKNOWN'}\"\n",
    "            fee_display = f\"{fee_tier/10000:.2f}%\" if fee_tier else \"N/A\"\n",
    "            volume_display = f\"{volume0:,.0f}\" if volume0 else \"0\"\n",
    "\n",
    "            print(\n",
    "                f\"{i:<4} {pair_address[:12]:<12} {pair_name:<20} {fee_display:<6} {volume_display:<15} {swaps or 0:<10,}\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error querying pool summary: {e}\")\n",
    "        print(\"💡 Try running comprehensive_metadata_update() first\")\n",
    "\n",
    "\n",
    "def run_maintenance_tasks():\n",
    "    print(\"\\n🔧 RUNNING MAINTENANCE TASKS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        conn = DB_POOL.get_connection()\n",
    "\n",
    "        print(\"📊 Updating database statistics...\")\n",
    "        conn.execute(\"ANALYZE\")\n",
    "\n",
    "        print(\"🗂️  Checking table sizes...\")\n",
    "        tables = [\n",
    "            \"transfer\",\n",
    "            \"swap\",\n",
    "            \"mint\",\n",
    "            \"burn\",\n",
    "            \"collect\",\n",
    "            \"flash\",\n",
    "            \"pair_metadata\",\n",
    "            \"block_metadata\",\n",
    "        ]\n",
    "\n",
    "        for table in tables:\n",
    "            try:\n",
    "                count = conn.execute(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
    "                print(f\"   • {table}: {count:,} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"   • {table}: Error - {e}\")\n",
    "\n",
    "        print(\"✅ Maintenance complete!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Maintenance error: {e}\")\n",
    "\n",
    "\n",
    "def export_data_sample(output_dir=\"exports\", limit=1000):\n",
    "    import os\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n📤 EXPORTING DATA SAMPLES (limit: {limit:,})\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        conn = DB_POOL.get_connection()\n",
    "\n",
    "        # Export recent swaps\n",
    "        swaps = conn.execute(\n",
    "            f\"\"\"\n",
    "            SELECT * FROM swap\n",
    "            WHERE block_timestamp IS NOT NULL\n",
    "            ORDER BY block_number DESC\n",
    "            LIMIT {limit}\n",
    "        \"\"\"\n",
    "        ).fetchdf()\n",
    "\n",
    "        swaps_file = f\"{output_dir}/recent_swaps_{limit}.parquet\"\n",
    "        swaps.to_parquet(swaps_file)\n",
    "        print(f\"📊 Exported {len(swaps):,} recent swaps to {swaps_file}\")\n",
    "\n",
    "        # Export pool summary\n",
    "        pools = conn.execute(\n",
    "            \"SELECT * FROM pool_summary ORDER BY total_volume_token0 DESC\"\n",
    "        ).fetchdf()\n",
    "        pools_file = f\"{output_dir}/pool_summary.parquet\"\n",
    "        pools.to_parquet(pools_file)\n",
    "        print(f\"🏊 Exported {len(pools):,} pool summaries to {pools_file}\")\n",
    "\n",
    "        print(\"✅ Export complete!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Export error: {e}\")\n",
    "\n",
    "\n",
    "def interactive_pipeline_menu():\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"🦄 UNISWAP V3 PIPELINE - INTERACTIVE MENU\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"1. 📊 Show Pipeline Status\")\n",
    "        print(\"2. 🚀 Show Optimization Stats\")\n",
    "        print(\"3. ⚡ Quick Scan (Recent 1 Hour)\")\n",
    "        print(\"4. ⚡ Quick Scan (Recent 24 Hours)\")\n",
    "        print(\"5. 📦 Update All Metadata\")\n",
    "        print(\"6. 🏆 Show Top 20 Pools by Volume\")\n",
    "        print(\"7. 📚 Historical Backfill (Custom Range)\")\n",
    "        print(\"8. 🎯 Targeted Pool Scan\")\n",
    "        print(\"9. 🔧 Run Full Pipeline (All Stages)\")\n",
    "        print(\"10. 🔧 Run Maintenance Tasks\")\n",
    "        print(\"11. 📤 Export Data Sample\")\n",
    "        print(\"0. 🚪 Exit\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        try:\n",
    "            choice = input(\"Select option (0-11): \").strip()\n",
    "\n",
    "            if choice == \"0\":\n",
    "                print(\"👋 Goodbye!\")\n",
    "                break\n",
    "            elif choice == \"1\":\n",
    "                get_pipeline_status()\n",
    "            elif choice == \"2\":\n",
    "                show_optimization_stats()\n",
    "            elif choice == \"3\":\n",
    "                quick_scan_recent_blocks(hours_back=1)\n",
    "            elif choice == \"4\":\n",
    "                quick_scan_recent_blocks(hours_back=24)\n",
    "            elif choice == \"5\":\n",
    "                comprehensive_metadata_update()\n",
    "            elif choice == \"6\":\n",
    "                get_top_pools_by_volume()\n",
    "            elif choice == \"7\":\n",
    "                try:\n",
    "                    start = input(\"Start block (default: factory deployment): \").strip()\n",
    "                    end = input(\"End block (default: current): \").strip()\n",
    "                    chunk = input(\"Chunk size (default: 10000): \").strip()\n",
    "                    workers = input(\"Max workers (default: 3): \").strip()\n",
    "\n",
    "                    start_block = int(start) if start else None\n",
    "                    end_block = int(end) if end else None\n",
    "                    chunk_size = int(chunk) if chunk else 10000\n",
    "                    max_workers = int(workers) if workers else 3\n",
    "\n",
    "                    run_historical_backfill(\n",
    "                        start_block, end_block, chunk_size, max_workers\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    print(\"❌ Invalid input. Please enter valid numbers.\")\n",
    "            elif choice == \"8\":\n",
    "                pools_input = input(\"Enter pool addresses (comma-separated): \").strip()\n",
    "                if pools_input:\n",
    "                    pool_addresses = [addr.strip() for addr in pools_input.split(\",\")]\n",
    "                    try:\n",
    "                        start = input(\"Start block (optional): \").strip()\n",
    "                        end = input(\"End block (optional): \").strip()\n",
    "                        start_block = int(start) if start else None\n",
    "                        end_block = int(end) if end else None\n",
    "                        run_targeted_pool_scan(pool_addresses, start_block, end_block)\n",
    "                    except ValueError:\n",
    "                        print(\"❌ Invalid block numbers\")\n",
    "                else:\n",
    "                    print(\"❌ No pool addresses provided\")\n",
    "            elif choice == \"9\":\n",
    "                try:\n",
    "                    start = input(\"Start block (default: factory deployment): \").strip()\n",
    "                    end = input(\"End block (default: current): \").strip()\n",
    "                    chunk = input(\"Chunk size (default: 10000): \").strip()\n",
    "                    workers = input(\"Max workers (default: 3): \").strip()\n",
    "\n",
    "                    start_block = int(start) if start else FACTORY_DEPLOYMENT_BLOCK\n",
    "                    end_block = int(end) if end else None\n",
    "                    chunk_size = int(chunk) if chunk else 10000\n",
    "                    max_workers = int(workers) if workers else 3\n",
    "\n",
    "                    run_full_pipeline(start_block, end_block, chunk_size, max_workers)\n",
    "                except ValueError:\n",
    "                    print(\"❌ Invalid input. Please enter valid numbers.\")\n",
    "            elif choice == \"10\":\n",
    "                run_maintenance_tasks()\n",
    "            elif choice == \"11\":\n",
    "                try:\n",
    "                    limit = input(\"Sample size (default: 1000): \").strip()\n",
    "                    limit = int(limit) if limit else 1000\n",
    "                    export_data_sample(limit=limit)\n",
    "                except ValueError:\n",
    "                    print(\"❌ Invalid sample size\")\n",
    "            else:\n",
    "                print(\"❌ Invalid option. Please select 0-11.\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n👋 Interrupted by user. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "\n",
    "\n",
    "def main_orchestrator():\n",
    "    try:\n",
    "        print_pipeline_banner()\n",
    "\n",
    "        # Initial status check\n",
    "        stats, current_block = get_pipeline_status()\n",
    "\n",
    "        # Show available operations\n",
    "        print(\"\\n🎛️  AVAILABLE OPERATIONS:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"• interactive_pipeline_menu() - Full interactive menu\")\n",
    "        print(\"• quick_scan_recent_blocks(hours=1) - Scan recent blocks\")\n",
    "        print(\"• comprehensive_metadata_update() - Update all metadata\")\n",
    "        print(\"• run_historical_backfill(start, end) - Backfill historical data\")\n",
    "        print(\"• get_top_pools_by_volume(limit=20) - Show top pools\")\n",
    "        print(\"• run_maintenance_tasks() - Database maintenance\")\n",
    "        print(\"• export_data_sample(limit=1000) - Export sample data\")\n",
    "        print(\"• run_full_pipeline() - Complete ETL pipeline\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        return stats, current_block\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Main orchestrator error: {e}\")\n",
    "        logging.error(f\"Main orchestrator error: {e}\", exc_info=True)\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Convenience functions for common operations\n",
    "def quick_start():\n",
    "    print_pipeline_banner()\n",
    "    get_pipeline_status()\n",
    "    print(\"\\n💡 Starting interactive menu...\")\n",
    "    interactive_pipeline_menu()\n",
    "\n",
    "\n",
    "def dev_mode():\n",
    "    print_pipeline_banner()\n",
    "    print(\"\\n🛠️  DEVELOPER MODE ACTIVATED\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Available functions loaded:\")\n",
    "    print(\"• main_orchestrator() - Main entry point\")\n",
    "    print(\"• quick_start() - Interactive mode\")\n",
    "    print(\"• test_optimizations() - Run optimization tests\")\n",
    "    print(\"• print_optimization_stats() - Show cache stats\")\n",
    "    get_pipeline_status()\n",
    "\n",
    "\n",
    "def test_optimizations():\n",
    "    print(\"\\n🧪 TESTING OPTIMIZATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    test_optimizations()\n",
    "    show_optimization_stats()\n",
    "\n",
    "\n",
    "# Pipeline presets for common use cases\n",
    "def preset_daily_maintenance():\n",
    "    print(\"🔄 DAILY MAINTENANCE PRESET\")\n",
    "    print(\"=\" * 40)\n",
    "    quick_scan_recent_blocks(hours_back=24, max_workers=2)\n",
    "    comprehensive_metadata_update()\n",
    "    run_maintenance_tasks()\n",
    "    print(\"✅ Daily maintenance complete!\")\n",
    "\n",
    "\n",
    "def preset_full_historical_sync():\n",
    "    print(\"📚 FULL HISTORICAL SYNC PRESET\")\n",
    "    print(\"=\" * 40)\n",
    "    run_full_pipeline(\n",
    "        start_block=FACTORY_DEPLOYMENT_BLOCK,\n",
    "        end_block=None,\n",
    "        chunk_size=10000,\n",
    "        max_workers=3,\n",
    "    )\n",
    "\n",
    "\n",
    "def preset_quick_analysis():\n",
    "    print(\"🔍 QUICK ANALYSIS PRESET\")\n",
    "    print(\"=\" * 40)\n",
    "    get_pipeline_status()\n",
    "    show_optimization_stats()\n",
    "    get_top_pools_by_volume(limit=10)\n",
    "\n",
    "\n",
    "# Auto-execute notification when cell is run\n",
    "print(\"🦄 Uniswap V3 Pipeline Orchestrator Loaded!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"🚀 Ready to process Uniswap V3 data with multicall optimization\")\n",
    "print(\"📊 Available quick commands:\")\n",
    "print(\"   • quick_start() - Interactive menu\")\n",
    "print(\"   • dev_mode() - Developer mode\")\n",
    "print(\"   • preset_daily_maintenance() - Daily sync\")\n",
    "print(\"   • preset_quick_analysis() - Status overview\")\n",
    "print(\"💡 All systems initialized and ready!\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "closedblind-ylehWVqW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
