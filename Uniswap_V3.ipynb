{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import datetime\n",
    "from datetime import timezone\n",
    "import signal\n",
    "import tempfile\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "from threading import Lock, Event as ThreadEvent\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from decimal import Decimal\n",
    "\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from hexbytes import HexBytes\n",
    "from requests.exceptions import HTTPError, ConnectionError\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "\n",
    "from web3 import Web3\n",
    "from web3.providers.rpc.utils import (\n",
    "    ExceptionRetryConfiguration,\n",
    "    REQUEST_RETRY_ALLOWLIST,\n",
    ")\n",
    "from web3.exceptions import Web3RPCError\n",
    "\n",
    "print(\"‚úì All imports loaded successfully\")\n",
    "\n",
    "# Configuration\n",
    "load_dotenv()\n",
    "pd.options.display.float_format = \"{:20,.4f}\".format\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "ETHERSCAN_API_KEY_DICT = {\n",
    "    \"hearthquake\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_HEARTHQUAKE\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "    \"opensee\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_OPENSEE\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "    \"eco\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_ECO\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "ETHERSCAN_API_KEY = ETHERSCAN_API_KEY_DICT[\"hearthquake\"][\"ETHERSCAN_API_KEY\"]\n",
    "\n",
    "STATE_FILE = \"out/V3/V3_final_scan_state.json\"\n",
    "TOKEN_NAME_FILE = \"out/V3/V3_token_name.json\"\n",
    "V3_EVENT_BY_CONTRACTS = \"out/V3/uniswap_v3_pairs_events.json\"\n",
    "DB_PATH = \"out/V3/uniswap_v3.duckdb\"\n",
    "ABI_CACHE_FOLDER = \"ABI\"\n",
    "\n",
    "GLOBAL_DICT_TOKEN_SYMBOL = {}\n",
    "if os.path.exists(TOKEN_NAME_FILE):\n",
    "    with open(TOKEN_NAME_FILE, \"r\") as f:\n",
    "        GLOBAL_DICT_TOKEN_SYMBOL = json.load(f)\n",
    "\n",
    "\n",
    "class ProviderPool:\n",
    "    def __init__(self, api_key_dict):\n",
    "        self.providers = []\n",
    "        self.provider_names = []\n",
    "        self.index = 0\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "        for name, config in api_key_dict.items():\n",
    "            provider = Web3(\n",
    "                Web3.HTTPProvider(\n",
    "                    endpoint_uri=config[\"INFURA_URL\"],\n",
    "                    request_kwargs={\"timeout\": 30},\n",
    "                    exception_retry_configuration=ExceptionRetryConfiguration(\n",
    "                        errors=(ConnectionError, HTTPError, TimeoutError),\n",
    "                        retries=5,\n",
    "                        backoff_factor=1,\n",
    "                        method_allowlist=REQUEST_RETRY_ALLOWLIST,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "            if provider.is_connected():\n",
    "                self.providers.append(provider)\n",
    "                self.provider_names.append(name)\n",
    "                logging.info(f\"‚úì Provider '{name}' connected\")\n",
    "            else:\n",
    "                logging.warning(f\"‚úó Provider '{name}' failed to connect\")\n",
    "\n",
    "        if not self.providers:\n",
    "            raise Exception(\"No providers connected!\")\n",
    "\n",
    "    def get_provider(self):\n",
    "        with self.lock:\n",
    "            provider = self.providers[self.index]\n",
    "            name = self.provider_names[self.index]\n",
    "            self.index = (self.index + 1) % len(self.providers)\n",
    "            return provider, name\n",
    "\n",
    "\n",
    "PROVIDER_POOL = ProviderPool(ETHERSCAN_API_KEY_DICT)\n",
    "w3 = PROVIDER_POOL.get_provider()\n",
    "assert w3.is_connected(), \"Web3 provider connection failed\"\n",
    "print(f\"‚úì Connected to Ethereum. Latest block: {w3.eth.block_number:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0df9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Helper Function: Get ABI from Etherscan or Disk\n",
    "# --------------------\n",
    "def get_abi(contract_address, api_key):\n",
    "    abi_folder = \"ABI\"\n",
    "    if not os.path.exists(abi_folder):\n",
    "        os.makedirs(abi_folder)\n",
    "\n",
    "    filename = os.path.join(abi_folder, f\"{contract_address}.json\")\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\") as file:\n",
    "            abi = json.load(file)\n",
    "    else:\n",
    "        abi = None  # ‚Üê INITIALIZE abi BEFORE try block\n",
    "        try:\n",
    "            url = f\"https://api.etherscan.io/v2/api?chainid=1&module=contract&action=getabi&address={contract_address}&apikey={api_key}\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            if data[\"status\"] == \"1\":\n",
    "                abi = json.loads(data[\"result\"])\n",
    "                with open(filename, \"w\") as file:\n",
    "                    json.dump(abi, file)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error fetching ABI for contract {contract_address}: {e}\")\n",
    "\n",
    "    return abi\n",
    "\n",
    "\n",
    "def get_abi(contract_address, api_key=ETHERSCAN_API_KEY, abi_folder=ABI_CACHE_FOLDER):\n",
    "    os.makedirs(abi_folder, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(abi_folder, f\"{contract_address}.json\")\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.warning(\n",
    "                f\"Corrupted ABI cache for {contract_address}: {e}, re-fetching...\"\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        url = f\"https://api.etherscan.io/api?module=contract&action=getabi&address={contract_address}&apikey={api_key}\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        if data[\"status\"] != \"1\":\n",
    "            logging.warning(\n",
    "                f\"Etherscan error for {contract_address}: {data.get('result', 'Unknown')}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        abi = json.loads(data[\"result\"])\n",
    "\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(abi, f, indent=2)\n",
    "\n",
    "        return abi\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Network error fetching ABI for {contract_address}: {e}\")\n",
    "        return None\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        logging.error(f\"Invalid response format for {contract_address}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_contract(contract_address, provider=None, api_key=ETHERSCAN_API_KEY):\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    contract_address = provider.to_checksum_address(contract_address)\n",
    "    abi = get_abi(contract_address, api_key)\n",
    "\n",
    "    if abi is None:\n",
    "        raise ValueError(f\"Could not retrieve ABI for {contract_address}\")\n",
    "\n",
    "    return provider.eth.contract(address=contract_address, abi=abi)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Helper: Convert event to dict\n",
    "# -----------------------\n",
    "def event_to_dict(event):\n",
    "    d = dict(event)\n",
    "    if \"args\" in d:\n",
    "        d[\"args\"] = dict(d[\"args\"])\n",
    "    if \"transactionHash\" in d:\n",
    "        d[\"transactionHash\"] = d[\"transactionHash\"].hex()\n",
    "    if \"blockHash\" in d:\n",
    "        d[\"blockHash\"] = d[\"blockHash\"].hex()\n",
    "    return d\n",
    "\n",
    "\n",
    "class Web3JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        # HexBytes ‚Üí hex string\n",
    "        if isinstance(obj, HexBytes):\n",
    "            return obj.hex()\n",
    "        # Peel off any other web3-specific types here as needed...\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# ETHERSCAN VERSION\n",
    "# Used to find at which block 1 contract has been deployed\n",
    "# Might be useful later, put it in JSON in the end\n",
    "# -----------------------\n",
    "def get_contract_creation_block_etherscan(\n",
    "    contract_address: str, etherscan_api_key: str\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Retrieves the contract creation block from Etherscan.\n",
    "    Returns the block number as an integer.\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        f\"https://api.etherscan.io/api?module=contract&action=getcontractcreation\"\n",
    "        f\"&contractaddresses={contract_address}&apikey={etherscan_api_key}\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if data.get(\"status\") == \"1\":\n",
    "        results = data.get(\"result\", [])\n",
    "        if results and len(results) > 0:\n",
    "            return int(results[0][\"blockNumber\"])\n",
    "        else:\n",
    "            raise Exception(\"No contract creation data found.\")\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Error fetching creation block: \" + data.get(\"result\", \"Unknown error\")\n",
    "        )\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Used to find at which block 1 contract has been deployed\n",
    "# Might be useful later, put it in JSON in the end\n",
    "# -----------------------\n",
    "def get_contract_creation_block_custom(start_block=0, end_block=100000):\n",
    "\n",
    "    def get_contract_deployments(start_block, end_block, max_workers=8):\n",
    "        deployments = []\n",
    "\n",
    "        def process_block(block_number):\n",
    "            block = w3.eth.get_block(block_number, full_transactions=True)\n",
    "            block_deployments = []\n",
    "            for tx in block.transactions:\n",
    "                if tx.to is None:\n",
    "                    try:\n",
    "                        receipt = w3.eth.get_transaction_receipt(tx.hash)\n",
    "                        contract_address = receipt.contractAddress\n",
    "                        if contract_address:\n",
    "                            block_deployments.append(\n",
    "                                {\n",
    "                                    \"block_number\": block_number,\n",
    "                                    \"contract_address\": contract_address,\n",
    "                                }\n",
    "                            )\n",
    "                    except:\n",
    "                        print(tx.hash)\n",
    "            return block_deployments\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_block = {\n",
    "                executor.submit(process_block, bn): bn\n",
    "                for bn in range(start_block, end_block + 1)\n",
    "            }\n",
    "            for future in as_completed(future_to_block):\n",
    "                block_deployments = future.result()\n",
    "                deployments.extend(block_deployments)\n",
    "\n",
    "        return deployments\n",
    "\n",
    "    deployments = get_contract_deployments(start_block, end_block)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    with open(\"contract_deployments.json\", \"w\") as f:\n",
    "        json.dump(deployments, f, indent=4)\n",
    "\n",
    "\n",
    "# -- Step 2: Reconstruct an Event‚Äôs Signature --\n",
    "def get_event_signature(event_name: str, abi: list) -> str:\n",
    "    \"\"\"\n",
    "    Given an event name and an ABI, find the event definition and reconstruct its signature.\n",
    "    For example, for event Transfer(address,address,uint256) this returns its keccak256 hash.\n",
    "    \"\"\"\n",
    "    from eth_utils import keccak, encode_hex\n",
    "\n",
    "    for item in abi:\n",
    "        if item.get(\"type\") == \"event\" and item.get(\"name\") == event_name:\n",
    "            # Build the signature string: \"Transfer(address,address,uint256)\"\n",
    "            types = \",\".join([inp[\"type\"] for inp in item.get(\"inputs\", [])])\n",
    "            signature = f\"{event_name}({types})\"\n",
    "            return encode_hex(keccak(text=signature))\n",
    "    raise ValueError(f\"Event {event_name} not found in ABI.\")\n",
    "\n",
    "\n",
    "def block_to_utc(block_number):\n",
    "    \"\"\"\n",
    "    Convert a block number into its UTC timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        w3 (Web3): A Web3 instance\n",
    "        block_number (int): The block number\n",
    "\n",
    "    Returns:\n",
    "        datetime: The block timestamp in UTC\n",
    "    \"\"\"\n",
    "    block = w3.eth.get_block(block_number)\n",
    "    timestamp = block[\"timestamp\"]\n",
    "    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "def read_and_sort_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file, each line being a JSON object with a field `blockNumber`,\n",
    "    and returns a list of those objects sorted by blockNumber (ascending).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                # Handle bad JSON if needed, e.g., log or skip\n",
    "                print(line)\n",
    "                print(f\"Skipping bad JSON line: {e}\")\n",
    "                continue\n",
    "            # Optionally, you could check that 'blockNumber' exists, is int, etc.\n",
    "            if \"blockNumber\" not in obj:\n",
    "                print(f\"Skipping line with no blockNumber: {obj}\")\n",
    "                continue\n",
    "            data.append(obj)\n",
    "    # Now sort by blockNumber ascending\n",
    "    # If blockNumber in file is already int, fine; else convert\n",
    "    sorted_data = sorted(data, key=lambda o: int(o[\"blockNumber\"]))\n",
    "    return sorted_data\n",
    "\n",
    "\n",
    "def get_address_abi_contract(contract_address, etherscan_api_key=ETHERSCAN_API_KEY):\n",
    "    address = w3.to_checksum_address(contract_address)\n",
    "    contract_abi = get_abi(address, etherscan_api_key)\n",
    "    contract = w3.eth.contract(address=contract_address, abi=contract_abi)\n",
    "\n",
    "    return address, contract_abi, contract\n",
    "\n",
    "\n",
    "# Find the amount of token depending on the contract at the very specific block_number\n",
    "# but it use ETHERSCAN API (to go further: explorer the reconstruct from all the Transfer event but slow)\n",
    "# Not super useful for the moment\n",
    "def get_erc20_balance_at_block(user_address, token_address, block_number):\n",
    "    \"\"\"\n",
    "    Query ERC-20 balance of an address at a specific block.\n",
    "\n",
    "    user_address = \"0xe2dFC8F41DB4169A24e7B44095b9E92E20Ed57eD\"\n",
    "    token_address = \"0x514910771AF9Ca656af840dff83E8264EcF986CA\"\n",
    "    block_number = 23405236\n",
    "    balance = get_erc20_balance_at_block(user_address, token_address, block_number)\n",
    "\n",
    "    Parameters:\n",
    "        user_address: string, account to check\n",
    "        token_address: Web3 contract instance for the ERC-20 token\n",
    "        block_number: int, historical block\n",
    "\n",
    "    Returns:\n",
    "        int: token balance\n",
    "        None if contract is a proxy\n",
    "    \"\"\"\n",
    "    token_address, token_abi, token_contract = get_address_abi_contract(token_address)\n",
    "    user_address = w3.to_checksum_address(user_address)\n",
    "    token_name = None\n",
    "    token_symbol = None\n",
    "    try:\n",
    "        token_name = token_contract.functions.name().call()\n",
    "        token_symbol = token_contract.functions.symbol().call()\n",
    "    except Exception as e:\n",
    "        print(f\"Error {e}\")\n",
    "        print(f\"{token_address}\")\n",
    "        return None\n",
    "    balance = token_contract.functions.balanceOf(user_address).call(\n",
    "        block_identifier=block_number\n",
    "    )\n",
    "    print(\n",
    "        f\"Address {user_address} had {w3.from_wei(balance, \"ether\")} of {token_symbol} at block {block_number}\"\n",
    "    )\n",
    "    return balance\n",
    "\n",
    "\n",
    "def get_token_name_by_contract(\n",
    "    token_address,\n",
    "    TOKEN_NAME_FILE=TOKEN_NAME_FILE,\n",
    "    proxy_address=None,\n",
    "    global_cache=GLOBAL_DICT_TOKEN_SYMBOL,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the token name for `token_address`, using a local JSON cache.\n",
    "    If not in cache, will call get_token_name_by_contract (your ABI/Web3 function),\n",
    "    store the result (or None) in the cache file, and return it.\n",
    "    \"\"\"\n",
    "    # 1. Load cache\n",
    "    cache = global_cache\n",
    "    # if os.path.exists(TOKEN_NAME_FILE):\n",
    "    #     try:\n",
    "    #         with open(TOKEN_NAME_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    #             cache = json.load(f)\n",
    "    #     except Exception as e:\n",
    "    #         # If file is corrupted, proceed with empty cache\n",
    "    #         print(f\"Warning: cannot read token name cache: {e}\")\n",
    "\n",
    "    # 2. Check cache\n",
    "    if token_address in cache:\n",
    "        return cache[token_address]\n",
    "\n",
    "    # Not in cache ‚Üí fetch from contract\n",
    "    name = None\n",
    "    symbol = None\n",
    "    address = None\n",
    "    try:\n",
    "        if proxy_address:\n",
    "            proxy_address, proxy_abi, proxy_contract = get_address_abi_contract(\n",
    "                proxy_address\n",
    "            )\n",
    "            token_address = proxy_contract.functions.getToken(token_address).call()\n",
    "        token_address, token_abi, token_contract = get_address_abi_contract(\n",
    "            token_address\n",
    "        )\n",
    "        # call name\n",
    "        name_raw = token_contract.functions.name().call()\n",
    "        symbol_raw = token_contract.functions.symbol().call()\n",
    "        address = token_contract.address\n",
    "        # Convert raw to str if needed\n",
    "        name = str(name_raw)\n",
    "        if isinstance(name_raw, (bytes, bytearray)):\n",
    "            name = name_raw.decode(\"utf-8\", errors=\"ignore\").rstrip(\"\\x00\")\n",
    "        symbol = str(symbol_raw)\n",
    "        if isinstance(symbol_raw, (bytes, bytearray)):\n",
    "            symbol = symbol_raw.decode(\"utf-8\", errors=\"ignore\").rstrip(\"\\x00\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching token name/symbol for {address}: {e}\")\n",
    "        if token_address:\n",
    "            cache[token_address] = {\n",
    "                \"name\": None,\n",
    "                \"symbol\": None,\n",
    "                \"address\": None,\n",
    "            }\n",
    "        try:\n",
    "            dirn = os.path.dirname(TOKEN_NAME_FILE) or \".\"\n",
    "            fd, tmp = tempfile.mkstemp(dir=dirn, text=True)\n",
    "            with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(cache, f, indent=2, ensure_ascii=False)\n",
    "            os.replace(tmp, TOKEN_NAME_FILE)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to save token cache: {e}\")\n",
    "        return {\n",
    "            \"name\": None,\n",
    "            \"symbol\": None,\n",
    "            \"address\": None,\n",
    "        }\n",
    "\n",
    "    # Update cache\n",
    "    cache[address] = {\n",
    "        \"name\": name,\n",
    "        \"symbol\": symbol,\n",
    "        \"address\": address,\n",
    "    }\n",
    "\n",
    "    # Write back atomically (overwrite)\n",
    "    try:\n",
    "        dirn = os.path.dirname(TOKEN_NAME_FILE) or \".\"\n",
    "        fd, tmp = tempfile.mkstemp(dir=dirn, text=True)\n",
    "        with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cache, f, indent=2, ensure_ascii=False)\n",
    "        os.replace(tmp, TOKEN_NAME_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: failed to save token cache: {e}\")\n",
    "\n",
    "    return cache[address]\n",
    "\n",
    "\n",
    "def decode_topics(log):\n",
    "    _, abi, contract = get_address_abi_contract(log[\"address\"])\n",
    "    # Try matching this log against the ABI events\n",
    "    for item in abi:\n",
    "        if item.get(\"type\") == \"event\":\n",
    "            event_signature = (\n",
    "                f'{item[\"name\"]}({\",\".join(i[\"type\"] for i in item[\"inputs\"])})'\n",
    "            )\n",
    "            event_hash = w3.keccak(text=event_signature).hex()\n",
    "\n",
    "            if log[\"topics\"][0].hex() == event_hash:\n",
    "                # Found matching event\n",
    "                decoded = contract.events[item[\"name\"]]().process_log(log)\n",
    "                return {\n",
    "                    \"event\": item[\"name\"],\n",
    "                    \"args\": dict(decoded[\"args\"]),\n",
    "                }\n",
    "\n",
    "    return {}  # no matching event in ABI\n",
    "\n",
    "\n",
    "def release_list(a):\n",
    "    del a[:]\n",
    "    del a\n",
    "\n",
    "\n",
    "def normalize_token_value(raw_value, decimals):\n",
    "    if decimals == 18:\n",
    "        return float(Web3.from_wei(raw_value, \"ether\"))\n",
    "    else:\n",
    "        return float(Decimal(raw_value) / Decimal(10**decimals))\n",
    "\n",
    "\n",
    "def inspect_contract_abi(contract_address, provider=None):\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    contract = get_contract(contract_address, provider)\n",
    "    abi = contract.abi\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CONTRACT: {contract_address}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Functions\n",
    "    functions = [item for item in abi if item.get(\"type\") == \"function\"]\n",
    "    if functions:\n",
    "        print(f\"\\nüìã FUNCTIONS ({len(functions)}):\")\n",
    "        for func in functions:\n",
    "            name = func.get(\"name\", \"unnamed\")\n",
    "            inputs = \", \".join(\n",
    "                [f\"{i['type']} {i.get('name', '')}\" for i in func.get(\"inputs\", [])]\n",
    "            )\n",
    "            outputs = \", \".join([o[\"type\"] for o in func.get(\"outputs\", [])])\n",
    "            state = func.get(\"stateMutability\", \"nonpayable\")\n",
    "            print(f\"  ‚Ä¢ {name}({inputs}) ‚Üí {outputs} [{state}]\")\n",
    "\n",
    "    # Events\n",
    "    events = [item for item in abi if item.get(\"type\") == \"event\"]\n",
    "    if events:\n",
    "        print(f\"\\nüì¢ EVENTS ({len(events)}):\")\n",
    "        for event in events:\n",
    "            name = event.get(\"name\", \"unnamed\")\n",
    "            inputs = \", \".join(\n",
    "                [f\"{i['type']} {i.get('name', '')}\" for i in event.get(\"inputs\", [])]\n",
    "            )\n",
    "            print(f\"  ‚Ä¢ {name}({inputs})\")\n",
    "\n",
    "    # Constructor\n",
    "    constructor = [item for item in abi if item.get(\"type\") == \"constructor\"]\n",
    "    if constructor:\n",
    "        print(f\"\\nüèóÔ∏è  CONSTRUCTOR:\")\n",
    "        for c in constructor:\n",
    "            inputs = \", \".join(\n",
    "                [f\"{i['type']} {i.get('name', '')}\" for i in c.get(\"inputs\", [])]\n",
    "            )\n",
    "            print(f\"  ‚Ä¢ constructor({inputs})\")\n",
    "\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "    return abi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e40f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNISWAP_V3_CONTRACT = \"0x1F98431c8aD98523631AE4a59f267346ea31F984\"\n",
    "# Uniswap V3 Factory\n",
    "factory_contract = get_contract(UNISWAP_V3_CONTRACT)\n",
    "pair_count = factory_contract.functions.allPairsLength().call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_thread_local = threading.local()\n",
    "\n",
    "\n",
    "def get_thread_connection(db_path):\n",
    "    if not hasattr(_thread_local, \"conn\"):\n",
    "        _thread_local.conn = duckdb.connect(db_path)\n",
    "    return _thread_local.conn\n",
    "\n",
    "\n",
    "def setup_database(db_path=DB_PATH):\n",
    "    conn = duckdb.connect(db_path)\n",
    "    with open(\"./out/V3/database/schema.sql\", \"r\") as f:\n",
    "        schema_sql = f.read()\n",
    "    conn.execute(schema_sql)\n",
    "    conn.close()\n",
    "    logging.info(\"‚úì Database schema created successfully\")\n",
    "\n",
    "\n",
    "def batch_insert_events(events, db_path, worker_id=\"main\"):\n",
    "    if not events:\n",
    "        return 0\n",
    "\n",
    "    transfers = []\n",
    "    swaps = []\n",
    "    mints = []\n",
    "    burns = []\n",
    "    syncs = []\n",
    "    approvals = []\n",
    "\n",
    "    for e in events:\n",
    "        event_type = e.get(\"event\")\n",
    "        args = e.get(\"args\", {})\n",
    "\n",
    "        if event_type == \"Transfer\":\n",
    "            transfers.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"from\", \"\"),\n",
    "                    args.get(\"to\", \"\"),\n",
    "                    int(args.get(\"value\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Swap\":\n",
    "            swaps.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"sender\", \"\"),\n",
    "                    args.get(\"to\", \"\"),\n",
    "                    int(args.get(\"amount0In\", 0)),\n",
    "                    int(args.get(\"amount1In\", 0)),\n",
    "                    int(args.get(\"amount0Out\", 0)),\n",
    "                    int(args.get(\"amount1Out\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Mint\":\n",
    "            mints.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"sender\", \"\"),\n",
    "                    int(args.get(\"amount0\", 0)),\n",
    "                    int(args.get(\"amount1\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Burn\":\n",
    "            burns.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"sender\", \"\"),\n",
    "                    args.get(\"to\", \"\"),\n",
    "                    int(args.get(\"amount0\", 0)),\n",
    "                    int(args.get(\"amount1\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Sync\":\n",
    "            syncs.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    int(args.get(\"reserve0\", 0)),\n",
    "                    int(args.get(\"reserve1\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif event_type == \"Approval\":\n",
    "            approvals.append(\n",
    "                (\n",
    "                    e[\"transactionHash\"],\n",
    "                    e[\"blockNumber\"],\n",
    "                    e.get(\"logIndex\", 0),\n",
    "                    e[\"address\"],\n",
    "                    args.get(\"owner\", \"\"),\n",
    "                    args.get(\"spender\", \"\"),\n",
    "                    int(args.get(\"value\", 0)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    conn.execute(\"BEGIN TRANSACTION\")\n",
    "    try:\n",
    "        if transfers:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO transfer (transaction_hash, block_number, log_index, pair_address, \n",
    "                                     from_address, to_address, value)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                transfers,\n",
    "            )\n",
    "\n",
    "        if swaps:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO swap (transaction_hash, block_number, log_index, pair_address,\n",
    "                                 sender, to_address, amount0_in, amount1_in, amount0_out, amount1_out)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                swaps,\n",
    "            )\n",
    "\n",
    "        if mints:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO mint (transaction_hash, block_number, log_index, pair_address,\n",
    "                                 sender, amount0, amount1)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                mints,\n",
    "            )\n",
    "\n",
    "        if burns:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO burn (transaction_hash, block_number, log_index, pair_address,\n",
    "                                 sender, to_address, amount0, amount1)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                burns,\n",
    "            )\n",
    "\n",
    "        if syncs:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO sync (transaction_hash, block_number, log_index, pair_address,\n",
    "                                 reserve0, reserve1)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                syncs,\n",
    "            )\n",
    "\n",
    "        if approvals:\n",
    "            conn.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO approval (transaction_hash, block_number, log_index, pair_address,\n",
    "                                     owner, spender, value)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (transaction_hash, log_index) DO NOTHING\n",
    "            \"\"\",\n",
    "                approvals,\n",
    "            )\n",
    "\n",
    "        conn.execute(\"COMMIT\")\n",
    "\n",
    "        total = (\n",
    "            len(transfers)\n",
    "            + len(swaps)\n",
    "            + len(mints)\n",
    "            + len(burns)\n",
    "            + len(syncs)\n",
    "            + len(approvals)\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"[{worker_id}] Inserted {total} events (T:{len(transfers)} S:{len(swaps)} M:{len(mints)} B:{len(burns)} Sy:{len(syncs)} A:{len(approvals)})\"\n",
    "        )\n",
    "        return total\n",
    "\n",
    "    except Exception as e:\n",
    "        conn.execute(\"ROLLBACK\")\n",
    "        logging.error(f\"[{worker_id}] batch_insert_events failed: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def mark_range_completed(start_block, end_block, db_path, worker_id=\"main\"):\n",
    "    conn = get_thread_connection(db_path)\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO processing_state (start_block, end_block, status, worker_id, updated_at)\n",
    "        VALUES (?, ?, 'completed', ?, NOW())\n",
    "        ON CONFLICT (start_block, end_block) \n",
    "        DO UPDATE SET \n",
    "            status = 'completed', \n",
    "            worker_id = ?,\n",
    "            updated_at = NOW()\n",
    "    \"\"\",\n",
    "        (start_block, end_block, worker_id, worker_id),\n",
    "    )\n",
    "\n",
    "\n",
    "def mark_range_processing(start_block, end_block, db_path, worker_id=\"main\"):\n",
    "    conn = get_thread_connection(db_path)\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO processing_state (start_block, end_block, status, worker_id, updated_at)\n",
    "        VALUES (?, ?, 'processing', ?, NOW())\n",
    "        ON CONFLICT (start_block, end_block) \n",
    "        DO UPDATE SET \n",
    "            status = 'processing',\n",
    "            worker_id = ?,\n",
    "            updated_at = NOW()\n",
    "    \"\"\",\n",
    "        (start_block, end_block, worker_id, worker_id),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_completed_ranges(db_path):\n",
    "    conn = get_thread_connection(db_path)\n",
    "    result = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT start_block, end_block \n",
    "        FROM processing_state \n",
    "        WHERE status = 'completed'\n",
    "    \"\"\"\n",
    "    ).fetchall()\n",
    "    return set((r[0], r[1]) for r in result)\n",
    "\n",
    "\n",
    "def get_database_stats(db_path):\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    result = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            (SELECT COUNT(*) FROM transfer) as total_transfers,\n",
    "            (SELECT COUNT(*) FROM swap) as total_swaps,\n",
    "            (SELECT COUNT(*) FROM mint) as total_mints,\n",
    "            (SELECT COUNT(*) FROM burn) as total_burns,\n",
    "            (SELECT COUNT(*) FROM sync) as total_syncs,\n",
    "            (SELECT COUNT(*) FROM approval) as total_approvals,\n",
    "            (SELECT COUNT(*) FROM processing_state WHERE status = 'completed') as completed_ranges,\n",
    "            (SELECT COUNT(*) FROM pair_metadata) as total_pairs,\n",
    "            (SELECT COUNT(*) FROM block_metadata) as total_blocks\n",
    "    \"\"\"\n",
    "    ).fetchone()\n",
    "\n",
    "    return {\n",
    "        \"total_transfers\": result[0],\n",
    "        \"total_swaps\": result[1],\n",
    "        \"total_mints\": result[2],\n",
    "        \"total_burns\": result[3],\n",
    "        \"total_syncs\": result[4],\n",
    "        \"total_approvals\": result[5],\n",
    "        \"completed_ranges\": result[6],\n",
    "        \"total_pairs\": result[7],\n",
    "        \"total_blocks\": result[8],\n",
    "    }\n",
    "\n",
    "\n",
    "def insert_pair_metadata(\n",
    "    pair_address,\n",
    "    token0_address,\n",
    "    token1_address,\n",
    "    db_path,\n",
    "    token0_symbol=None,\n",
    "    token1_symbol=None,\n",
    "    token0_decimals=None,\n",
    "    token1_decimals=None,\n",
    "    created_block=None,\n",
    "):\n",
    "    conn = get_thread_connection(db_path)\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO pair_metadata (pair_address, token0_address, token1_address, token0_symbol, \n",
    "                                  token1_symbol, token0_decimals, token1_decimals, created_block)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT (pair_address) \n",
    "        DO UPDATE SET \n",
    "            token0_symbol = COALESCE(EXCLUDED.token0_symbol, pair_metadata.token0_symbol),\n",
    "            token1_symbol = COALESCE(EXCLUDED.token1_symbol, pair_metadata.token1_symbol),\n",
    "            token0_decimals = COALESCE(EXCLUDED.token0_decimals, pair_metadata.token0_decimals),\n",
    "            token1_decimals = COALESCE(EXCLUDED.token1_decimals, pair_metadata.token1_decimals),\n",
    "            last_updated = NOW()\n",
    "    \"\"\",\n",
    "        (\n",
    "            pair_address,\n",
    "            token0_address,\n",
    "            token1_address,\n",
    "            token0_symbol,\n",
    "            token1_symbol,\n",
    "            token0_decimals,\n",
    "            token1_decimals,\n",
    "            created_block,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_pair_metadata(pair_address, db_path):\n",
    "    conn = get_thread_connection(db_path)\n",
    "    result = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT token0_address, token1_address, token0_symbol, token1_symbol, \n",
    "               token0_decimals, token1_decimals, created_block\n",
    "        FROM pair_metadata\n",
    "        WHERE pair_address = ?\n",
    "    \"\"\",\n",
    "        (pair_address,),\n",
    "    ).fetchone()\n",
    "\n",
    "    if result:\n",
    "        return {\n",
    "            \"token0_address\": result[0],\n",
    "            \"token1_address\": result[1],\n",
    "            \"token0_symbol\": result[2],\n",
    "            \"token1_symbol\": result[3],\n",
    "            \"token0_decimals\": result[4],\n",
    "            \"token1_decimals\": result[5],\n",
    "            \"created_block\": result[6],\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def batch_insert_block_metadata(blocks_data, db_path):\n",
    "    if not blocks_data:\n",
    "        return 0\n",
    "\n",
    "    conn = get_thread_connection(db_path)\n",
    "    conn.executemany(\n",
    "        \"\"\"\n",
    "        INSERT INTO block_metadata (block_number, block_timestamp, block_hash)\n",
    "        VALUES (?, ?, ?)\n",
    "        ON CONFLICT (block_number) DO NOTHING\n",
    "    \"\"\",\n",
    "        blocks_data,\n",
    "    )\n",
    "    return len(blocks_data)\n",
    "\n",
    "\n",
    "def normalize_values_for_pair(pair_address, db_path):\n",
    "    metadata = get_pair_metadata(pair_address, db_path)\n",
    "    if (\n",
    "        not metadata\n",
    "        or metadata[\"token0_decimals\"] is None\n",
    "        or metadata[\"token1_decimals\"] is None\n",
    "    ):\n",
    "        logging.warning(f\"Cannot normalize values for {pair_address}: missing decimals\")\n",
    "        return\n",
    "\n",
    "    lp_decimals = 18\n",
    "    token0_decimals = metadata[\"token0_decimals\"]\n",
    "    token1_decimals = metadata[\"token1_decimals\"]\n",
    "\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        UPDATE transfer \n",
    "        SET value_normalized = value::DECIMAL / POWER(10, ?)\n",
    "        WHERE pair_address = ? AND value_normalized IS NULL\n",
    "    \"\"\",\n",
    "        (lp_decimals, pair_address),\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        UPDATE mint \n",
    "        SET amount0_normalized = amount0::DECIMAL / POWER(10, ?),\n",
    "            amount1_normalized = amount1::DECIMAL / POWER(10, ?)\n",
    "        WHERE pair_address = ? AND amount0_normalized IS NULL\n",
    "    \"\"\",\n",
    "        (token0_decimals, token1_decimals, pair_address),\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        UPDATE burn \n",
    "        SET amount0_normalized = amount0::DECIMAL / POWER(10, ?),\n",
    "            amount1_normalized = amount1::DECIMAL / POWER(10, ?)\n",
    "        WHERE pair_address = ? AND amount0_normalized IS NULL\n",
    "    \"\"\",\n",
    "        (token0_decimals, token1_decimals, pair_address),\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        UPDATE swap \n",
    "        SET amount0_in_normalized = amount0_in::DECIMAL / POWER(10, ?),\n",
    "            amount1_in_normalized = amount1_in::DECIMAL / POWER(10, ?),\n",
    "            amount0_out_normalized = amount0_out::DECIMAL / POWER(10, ?),\n",
    "            amount1_out_normalized = amount1_out::DECIMAL / POWER(10, ?)\n",
    "        WHERE pair_address = ? AND amount0_in_normalized IS NULL\n",
    "    \"\"\",\n",
    "        (\n",
    "            token0_decimals,\n",
    "            token1_decimals,\n",
    "            token0_decimals,\n",
    "            token1_decimals,\n",
    "            pair_address,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        UPDATE sync \n",
    "        SET reserve0_normalized = reserve0::DECIMAL / POWER(10, ?),\n",
    "            reserve1_normalized = reserve1::DECIMAL / POWER(10, ?)\n",
    "        WHERE pair_address = ? AND reserve0_normalized IS NULL\n",
    "    \"\"\",\n",
    "        (token0_decimals, token1_decimals, pair_address),\n",
    "    )\n",
    "\n",
    "    logging.info(f\"‚úì Normalized values for pair {pair_address}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_uniswap_pair_metadata(\n",
    "    pair_address, provider=None, retry_count=0, max_retries=3\n",
    "):\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    try:\n",
    "        pair_contract = get_contract(pair_address, provider)\n",
    "        token0_address = pair_contract.functions.token0().call()\n",
    "        token1_address = pair_contract.functions.token1().call()\n",
    "\n",
    "        token0_contract = get_contract(token0_address, provider)\n",
    "        token1_contract = get_contract(token1_address, provider)\n",
    "\n",
    "        return {\n",
    "            \"pair_address\": pair_address,\n",
    "            \"token0_address\": token0_address,\n",
    "            \"token1_address\": token1_address,\n",
    "            \"token0_symbol\": token0_contract.functions.symbol().call(),\n",
    "            \"token0_decimals\": token0_contract.functions.decimals().call(),\n",
    "            \"token1_symbol\": token1_contract.functions.symbol().call(),\n",
    "            \"token1_decimals\": token1_contract.functions.decimals().call(),\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 429:\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = 2**retry_count\n",
    "                logging.warning(\n",
    "                    f\"Rate limit (429) for {pair_address[:10]}, waiting {wait_time}s...\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                provider, _ = PROVIDER_POOL.get_provider()\n",
    "                return fetch_uniswap_pair_metadata(\n",
    "                    pair_address, provider, retry_count + 1, max_retries\n",
    "                )\n",
    "            else:\n",
    "                raise Exception(f\"Max retries exceeded for {pair_address}\")\n",
    "\n",
    "        elif e.response.status_code == 402:\n",
    "            raise Exception(\n",
    "                f\"Payment required (402) - Infura credits exhausted for {pair_address}\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"HTTP {e.response.status_code} for {pair_address}: {e}\")\n",
    "            raise\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        if retry_count < max_retries:\n",
    "            logging.warning(\n",
    "                f\"Timeout for {pair_address[:10]}, retrying with new provider...\"\n",
    "            )\n",
    "            provider, _ = PROVIDER_POOL.get_provider()\n",
    "            return fetch_uniswap_pair_metadata(\n",
    "                pair_address, provider, retry_count + 1, max_retries\n",
    "            )\n",
    "        else:\n",
    "            logging.error(f\"Timeout after {max_retries} retries for {pair_address}\")\n",
    "            raise\n",
    "\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        if retry_count < max_retries:\n",
    "            logging.warning(f\"Connection error for {pair_address[:10]}, retrying...\")\n",
    "            provider, _ = PROVIDER_POOL.get_provider()\n",
    "            return fetch_uniswap_pair_metadata(\n",
    "                pair_address, provider, retry_count + 1, max_retries\n",
    "            )\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"Connection failed after {max_retries} retries for {pair_address}\"\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    except (ValueError, KeyError) as e:\n",
    "        logging.error(f\"Contract call failed for {pair_address}: {e}\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error fetching metadata for {pair_address}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_block_metadata(block_number, provider=None, retry_count=0, max_retries=3):\n",
    "    if provider is None:\n",
    "        provider, _ = PROVIDER_POOL.get_provider()\n",
    "\n",
    "    try:\n",
    "        block = provider.eth.get_block(block_number)\n",
    "        return (block_number, block[\"timestamp\"], block[\"hash\"].hex())\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 429:\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = 2**retry_count\n",
    "                logging.warning(\n",
    "                    f\"Rate limit (429) for block {block_number}, waiting {wait_time}s...\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                provider, _ = PROVIDER_POOL.get_provider()\n",
    "                return fetch_block_metadata(\n",
    "                    block_number, provider, retry_count + 1, max_retries\n",
    "                )\n",
    "            else:\n",
    "                raise Exception(f\"Max retries exceeded for block {block_number}\")\n",
    "\n",
    "        elif e.response.status_code == 402:\n",
    "            raise Exception(f\"Payment required (402) - Infura credits exhausted\")\n",
    "\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"HTTP {e.response.status_code} for block {block_number}: {e}\"\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        if retry_count < max_retries:\n",
    "            logging.warning(f\"Timeout for block {block_number}, retrying...\")\n",
    "            provider, _ = PROVIDER_POOL.get_provider()\n",
    "            return fetch_block_metadata(\n",
    "                block_number, provider, retry_count + 1, max_retries\n",
    "            )\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"Timeout after {max_retries} retries for block {block_number}\"\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        if retry_count < max_retries:\n",
    "            logging.warning(f\"Connection error for block {block_number}, retrying...\")\n",
    "            provider, _ = PROVIDER_POOL.get_provider()\n",
    "            return fetch_block_metadata(\n",
    "                block_number, provider, retry_count + 1, max_retries\n",
    "            )\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"Connection failed after {max_retries} retries for block {block_number}\"\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error fetching block {block_number}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_and_store_uniswap_pair_metadata(\n",
    "    pair_address, db_path, created_block=None, provider=None\n",
    "):\n",
    "    metadata = fetch_uniswap_pair_metadata(pair_address, provider)\n",
    "    if metadata:\n",
    "        insert_pair_metadata(\n",
    "            pair_address=metadata[\"pair_address\"],\n",
    "            token0_address=metadata[\"token0_address\"],\n",
    "            token1_address=metadata[\"token1_address\"],\n",
    "            db_path=db_path,\n",
    "            token0_symbol=metadata[\"token0_symbol\"],\n",
    "            token1_symbol=metadata[\"token1_symbol\"],\n",
    "            token0_decimals=metadata[\"token0_decimals\"],\n",
    "            token1_decimals=metadata[\"token1_decimals\"],\n",
    "            created_block=created_block,\n",
    "        )\n",
    "        return metadata\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_and_store_block_metadata(\n",
    "    block_numbers, db_path, provider=None, max_workers=8\n",
    "):\n",
    "    if provider is None:\n",
    "        provider, provider_name = PROVIDER_POOL.get_provider()\n",
    "    else:\n",
    "        provider_name = \"provided\"\n",
    "\n",
    "    blocks_data = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_block = {\n",
    "            executor.submit(fetch_block_metadata, block_num, provider): block_num\n",
    "            for block_num in block_numbers\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_block):\n",
    "            block_num = future_to_block[future]\n",
    "            try:\n",
    "                block_data = future.result()\n",
    "                if block_data:\n",
    "                    blocks_data.append(block_data)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to fetch block {block_num}: {e}\")\n",
    "\n",
    "    if blocks_data:\n",
    "        batch_insert_block_metadata(blocks_data, db_path)\n",
    "        logging.info(\n",
    "            f\"‚úì Stored metadata for {len(blocks_data)} blocks using {provider_name}\"\n",
    "        )\n",
    "\n",
    "    return len(blocks_data)\n",
    "\n",
    "\n",
    "def collect_missing_pair_metadata(db_path, batch_size=50, provider=None, max_workers=4):\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    all_pairs = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT DISTINCT pair_address FROM transfer\n",
    "    \"\"\"\n",
    "    ).fetchall()\n",
    "    all_pairs = [r[0] for r in all_pairs]\n",
    "\n",
    "    existing_pairs = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT pair_address FROM pair_metadata\n",
    "        WHERE token0_decimals IS NOT NULL AND token1_decimals IS NOT NULL\n",
    "    \"\"\"\n",
    "    ).fetchall()\n",
    "    existing_pairs = set(r[0] for r in existing_pairs)\n",
    "\n",
    "    missing_pairs = [p for p in all_pairs if p not in existing_pairs]\n",
    "\n",
    "    if not missing_pairs:\n",
    "        logging.info(\"‚úì All pairs already have metadata\")\n",
    "        return\n",
    "\n",
    "    logging.info(\n",
    "        f\"Found {len(missing_pairs)} pairs missing metadata out of {len(all_pairs)} total\"\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(missing_pairs), batch_size):\n",
    "        batch = missing_pairs[i : i + batch_size]\n",
    "        logging.info(\n",
    "            f\"Processing batch {i // batch_size + 1}/{(len(missing_pairs) + batch_size - 1) // batch_size}\"\n",
    "        )\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_pair = {\n",
    "                executor.submit(\n",
    "                    fetch_and_store_uniswap_pair_metadata, pair, db_path, None, provider\n",
    "                ): pair\n",
    "                for pair in batch\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_pair):\n",
    "                pair = future_to_pair[future]\n",
    "                try:\n",
    "                    metadata = future.result()\n",
    "                    if metadata:\n",
    "                        logging.info(\n",
    "                            f\"‚úì {metadata['token0_symbol']}/{metadata['token1_symbol']} - {pair[:10]}...\"\n",
    "                        )\n",
    "                    else:\n",
    "                        logging.warning(f\"‚úó Failed: {pair[:10]}...\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"‚úó Error for {pair[:10]}: {e}\")\n",
    "\n",
    "    logging.info(\"‚úì Metadata collection complete\")\n",
    "\n",
    "\n",
    "def normalize_missing_pairs(db_path, max_workers=4):\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    pairs_with_metadata = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT pair_address, token0_decimals, token1_decimals\n",
    "        FROM pair_metadata\n",
    "        WHERE token0_decimals IS NOT NULL AND token1_decimals IS NOT NULL\n",
    "    \"\"\"\n",
    "    ).fetchall()\n",
    "\n",
    "    if not pairs_with_metadata:\n",
    "        logging.warning(\n",
    "            \"No pairs with metadata found - run collect_missing_pair_metadata() first\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    pairs_to_normalize = []\n",
    "    for pair_address, token0_dec, token1_dec in pairs_with_metadata:\n",
    "        needs_norm = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT COUNT(*) FROM transfer\n",
    "            WHERE pair_address = ? AND value_normalized IS NULL\n",
    "            LIMIT 1\n",
    "        \"\"\",\n",
    "            (pair_address,),\n",
    "        ).fetchone()[0]\n",
    "\n",
    "        if needs_norm > 0:\n",
    "            pairs_to_normalize.append(pair_address)\n",
    "\n",
    "    if not pairs_to_normalize:\n",
    "        logging.info(\"‚úì All pairs already normalized\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Normalizing {len(pairs_to_normalize)} pairs...\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_pair = {\n",
    "            executor.submit(normalize_values_for_pair, pair, db_path): pair\n",
    "            for pair in pairs_to_normalize\n",
    "        }\n",
    "\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_pair):\n",
    "            pair = future_to_pair[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                completed += 1\n",
    "                if completed % 10 == 0 or completed == len(pairs_to_normalize):\n",
    "                    logging.info(\n",
    "                        f\"Progress: {completed}/{len(pairs_to_normalize)} pairs normalized\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to normalize {pair[:10]}: {e}\")\n",
    "\n",
    "    logging.info(\"‚úì Normalization complete\")\n",
    "\n",
    "\n",
    "def populate_block_metadata_for_range(\n",
    "    start_block, end_block, db_path, batch_size=1000, provider=None, max_workers=8\n",
    "):\n",
    "    conn = get_thread_connection(db_path)\n",
    "\n",
    "    existing_blocks = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT block_number FROM block_metadata\n",
    "        WHERE block_number BETWEEN ? AND ?\n",
    "    \"\"\",\n",
    "        (start_block, end_block),\n",
    "    ).fetchall()\n",
    "    existing_blocks = {b[0] for b in existing_blocks}\n",
    "\n",
    "    all_blocks = set(range(start_block, end_block + 1))\n",
    "    missing_blocks = sorted(all_blocks - existing_blocks)\n",
    "\n",
    "    if not missing_blocks:\n",
    "        logging.info(\"‚úì All blocks already have metadata\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Need to fetch {len(missing_blocks)} block timestamps\")\n",
    "\n",
    "    for i in range(0, len(missing_blocks), batch_size):\n",
    "        batch = missing_blocks[i : i + batch_size]\n",
    "        fetch_and_store_block_metadata(batch, db_path, provider, max_workers)\n",
    "        logging.info(\n",
    "            f\"Progress: {min(i + batch_size, len(missing_blocks))}/{len(missing_blocks)} blocks processed\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b744a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Enhanced Blockchain Scanning Functions with Metadata Collection\n",
    "\n",
    "\n",
    "def fetch_logs_for_range(\n",
    "    start_block, end_block, addresses, worker_id=\"main\", retry_count=0, max_retries=5\n",
    "):\n",
    "    provider, provider_name = get_provider()\n",
    "\n",
    "    try:\n",
    "        params = {\n",
    "            \"fromBlock\": start_block,\n",
    "            \"toBlock\": end_block,\n",
    "            \"address\": addresses,\n",
    "        }\n",
    "\n",
    "        logs = provider.eth.get_logs(params)\n",
    "\n",
    "        transactions = []\n",
    "        for log in logs:\n",
    "            transaction = {\n",
    "                \"transactionHash\": provider.to_hex(log[\"transactionHash\"]),\n",
    "                \"blockNumber\": log[\"blockNumber\"],\n",
    "                \"logIndex\": log.get(\"logIndex\", 0),\n",
    "                \"address\": log[\"address\"],\n",
    "                \"data\": provider.to_hex(log[\"data\"]),\n",
    "            }\n",
    "\n",
    "            topics = decode_topics(log)\n",
    "            transaction.update(topics)\n",
    "\n",
    "            if log.get(\"topics\") and len(log[\"topics\"]) > 0:\n",
    "                transaction[\"eventSignature\"] = provider.to_hex(log[\"topics\"][0])\n",
    "            else:\n",
    "                transaction[\"eventSignature\"] = \"\"\n",
    "\n",
    "            transactions.append(transaction)\n",
    "\n",
    "        logging.info(\n",
    "            f\"[{worker_id}] [{provider_name}] Fetched {len(transactions)} events from blocks [{start_block:,}, {end_block:,}]\"\n",
    "        )\n",
    "        return transactions\n",
    "\n",
    "    except HTTPError as e:\n",
    "        if e.response.status_code == 413:\n",
    "            # Response too large - need to split the range\n",
    "            logging.warning(\n",
    "                f\"[{worker_id}] [{provider_name}] Response too large (413) for range [{start_block:,}, {end_block:,}] - will split\"\n",
    "            )\n",
    "            raise Web3RPCError(\"Response payload too large - splitting range\")\n",
    "        \n",
    "        elif e.response.status_code == 429:\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = 2**retry_count\n",
    "                logging.warning(\n",
    "                    f\"[{worker_id}] [{provider_name}] Rate limit hit, waiting {wait_time}s...\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                return fetch_logs_for_range(\n",
    "                    start_block,\n",
    "                    end_block,\n",
    "                    addresses,\n",
    "                    worker_id,\n",
    "                    retry_count + 1,\n",
    "                    max_retries,\n",
    "                )\n",
    "            else:\n",
    "                logging.error(f\"[{worker_id}] Max retries reached\")\n",
    "                raise\n",
    "        \n",
    "        elif e.response.status_code == 402:\n",
    "            logging.critical(f\"[{worker_id}] Payment required (402)\")\n",
    "            raise\n",
    "        \n",
    "        else:\n",
    "            logging.error(f\"[{worker_id}] HTTP error {e.response.status_code}: {e}\")\n",
    "            raise\n",
    "\n",
    "    except Web3RPCError as e:\n",
    "        if \"more than 10000 results\" in str(e) or \"-32005\" in str(e) or \"Response payload too large\" in str(e):\n",
    "            raise\n",
    "        else:\n",
    "            logging.error(f\"[{worker_id}] Web3 RPC error: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def collect_block_metadata_for_range(start_block, end_block, worker_id=\"main\"):\n",
    "    with _connection_lock:\n",
    "        conn = duckdb.connect(_db_path)\n",
    "        try:\n",
    "            existing_blocks = set(\n",
    "                conn.execute(\n",
    "                    \"\"\"\n",
    "                SELECT block_number FROM block_metadata\n",
    "                WHERE block_number BETWEEN ? AND ?\n",
    "            \"\"\",\n",
    "                    (start_block, end_block),\n",
    "                ).fetchall()\n",
    "            )\n",
    "            existing_blocks = {b[0] for b in existing_blocks}\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    missing_blocks = [\n",
    "        b for b in range(start_block, end_block + 1) if b not in existing_blocks\n",
    "    ]\n",
    "\n",
    "    if not missing_blocks:\n",
    "        return 0\n",
    "\n",
    "    provider, provider_name = get_provider()\n",
    "    blocks_data = []\n",
    "\n",
    "    for block_num in missing_blocks:\n",
    "        try:\n",
    "            block = provider.eth.get_block(block_num)\n",
    "            blocks_data.append((block_num, block[\"timestamp\"], block[\"hash\"].hex()))\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[{worker_id}] Failed to fetch block {block_num}: {e}\")\n",
    "\n",
    "    if blocks_data:\n",
    "        batch_insert_block_metadata(blocks_data)\n",
    "        logging.debug(f\"[{worker_id}] Stored metadata for {len(blocks_data)} blocks\")\n",
    "\n",
    "    return len(blocks_data)\n",
    "\n",
    "\n",
    "def collect_pair_metadata_from_events(events, worker_id=\"main\"):\n",
    "    unique_pairs = set(e[\"address\"] for e in events if \"address\" in e)\n",
    "\n",
    "    for pair_address in unique_pairs:\n",
    "        existing = get_pair_metadata(pair_address)\n",
    "        if existing is None:\n",
    "            try:\n",
    "                metadata = fetch_and_store_pair_metadata(pair_address)\n",
    "                if metadata:\n",
    "                    logging.debug(\n",
    "                        f\"[{worker_id}] Stored metadata for {metadata['token0_symbol']}/{metadata['token1_symbol']}\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                logging.warning(\n",
    "                    f\"[{worker_id}] Failed to fetch metadata for {pair_address}: {e}\"\n",
    "                )\n",
    "\n",
    "\n",
    "def process_block_range(\n",
    "    start_block, end_block, addresses, worker_id=\"main\", collect_metadata=False\n",
    "):\n",
    "    if is_shutdown_requested():\n",
    "        logging.info(\n",
    "            f\"[{worker_id}] Shutdown requested - skipping range [{start_block:,}, {end_block:,}]\"\n",
    "        )\n",
    "        return 0\n",
    "\n",
    "    if (start_block, end_block) in get_completed_ranges():\n",
    "        logging.debug(\n",
    "            f\"[{worker_id}] Skipping already processed range [{start_block:,}, {end_block:,}]\"\n",
    "        )\n",
    "        return 0\n",
    "\n",
    "    mark_range_processing(start_block, end_block, worker_id)\n",
    "\n",
    "    try:\n",
    "        events = fetch_logs_for_range(start_block, end_block, addresses, worker_id)\n",
    "\n",
    "        if is_shutdown_requested():\n",
    "            logging.warning(\n",
    "                f\"[{worker_id}] Shutdown requested after fetch - saving {len(events)} events before stopping\"\n",
    "            )\n",
    "\n",
    "        batch_insert_events(events, worker_id)\n",
    "\n",
    "        if collect_metadata and events:\n",
    "            try:\n",
    "                collect_block_metadata_for_range(start_block, end_block, worker_id)\n",
    "                collect_pair_metadata_from_events(events, worker_id)\n",
    "            except Exception as e:\n",
    "                logging.warning(\n",
    "                    f\"[{worker_id}] Metadata collection failed (non-fatal): {e}\"\n",
    "                )\n",
    "\n",
    "        mark_range_completed(start_block, end_block, worker_id)\n",
    "\n",
    "        logging.debug(\n",
    "            f\"[{worker_id}] ‚úì Processed [{start_block:,}, {end_block:,}] - {len(events)} events\"\n",
    "        )\n",
    "        return len(events)\n",
    "\n",
    "    except (Web3RPCError, HTTPError) as e:\n",
    "        # Check if we need to split (too many results OR response too large)\n",
    "        if (\n",
    "            \"more than 10000 results\" in str(e)\n",
    "            or \"-32005\" in str(e)\n",
    "            or \"Response payload too large\" in str(e)\n",
    "            or (hasattr(e, \"response\") and e.response.status_code == 413)\n",
    "        ):\n",
    "\n",
    "            mid = (start_block + end_block) // 2\n",
    "\n",
    "            if mid == start_block:\n",
    "                logging.error(\n",
    "                    f\"[{worker_id}] Cannot split range [{start_block:,}, {end_block:,}] further - skipping\"\n",
    "                )\n",
    "                mark_range_completed(start_block, end_block, worker_id)\n",
    "                return 0\n",
    "\n",
    "            logging.info(\n",
    "                f\"[{worker_id}] Splitting [{start_block:,}, {end_block:,}] at {mid:,} (reason: {type(e).__name__})\"\n",
    "            )\n",
    "\n",
    "            count1 = process_block_range(\n",
    "                start_block, mid, addresses, worker_id, collect_metadata\n",
    "            )\n",
    "\n",
    "            if is_shutdown_requested():\n",
    "                logging.warning(\n",
    "                    f\"[{worker_id}] Shutdown requested - skipping second half of split\"\n",
    "                )\n",
    "                return count1\n",
    "\n",
    "            count2 = process_block_range(\n",
    "                mid + 1, end_block, addresses, worker_id, collect_metadata\n",
    "            )\n",
    "\n",
    "            return count1 + count2\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"[{worker_id}] Failed to process [{start_block:,}, {end_block:,}]: {e}\"\n",
    "            )\n",
    "            return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{worker_id}] Unexpected error: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return 0\n",
    "\n",
    "\n",
    "def generate_block_ranges(start_block, end_block, chunk_size):\n",
    "    completed = get_completed_ranges()\n",
    "\n",
    "    ranges = []\n",
    "    current = start_block\n",
    "\n",
    "    while current <= end_block:\n",
    "        end = min(current + chunk_size - 1, end_block)\n",
    "\n",
    "        if (current, end) not in completed:\n",
    "            ranges.append((current, end))\n",
    "\n",
    "        current = end + 1\n",
    "\n",
    "    return ranges\n",
    "\n",
    "\n",
    "def scan_blockchain(\n",
    "    addresses,\n",
    "    start_block,\n",
    "    end_block,\n",
    "    chunk_size=10000,\n",
    "    max_workers=3,\n",
    "    collect_metadata=False,\n",
    "):\n",
    "    ranges = generate_block_ranges(start_block, end_block, chunk_size)\n",
    "\n",
    "    if not ranges:\n",
    "        logging.info(\"No ranges to process - all already completed!\")\n",
    "        return\n",
    "\n",
    "    total_ranges = len(ranges)\n",
    "    logging.info(f\"Processing {total_ranges} block ranges with {max_workers} workers\")\n",
    "\n",
    "    total_events = 0\n",
    "    completed_ranges = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_range = {\n",
    "            executor.submit(\n",
    "                process_block_range,\n",
    "                start,\n",
    "                end,\n",
    "                addresses,\n",
    "                f\"worker-{i % max_workers}\",\n",
    "                collect_metadata,\n",
    "            ): (start, end, i)\n",
    "            for i, (start, end) in enumerate(ranges)\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            for future in as_completed(future_to_range):\n",
    "                if is_shutdown_requested():\n",
    "                    logging.warning(\n",
    "                        \"Shutdown requested - waiting for active tasks to complete...\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                start, end, idx = future_to_range[future]\n",
    "\n",
    "                try:\n",
    "                    event_count = future.result()\n",
    "                    total_events += event_count\n",
    "                    completed_ranges += 1\n",
    "\n",
    "                    progress = (completed_ranges / total_ranges) * 100\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = completed_ranges / elapsed if elapsed > 0 else 0\n",
    "                    eta_seconds = (\n",
    "                        (total_ranges - completed_ranges) / rate if rate > 0 else 0\n",
    "                    )\n",
    "                    eta_str = f\"{int(eta_seconds // 60)}m {int(eta_seconds % 60)}s\"\n",
    "\n",
    "                    logging.info(\n",
    "                        f\"Progress: {completed_ranges}/{total_ranges} ({progress:.1f}%) | \"\n",
    "                        f\"Events: {total_events:,} | \"\n",
    "                        f\"Rate: {rate:.1f} ranges/s | \"\n",
    "                        f\"ETA: {eta_str}\"\n",
    "                    )\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Range [{start:,}, {end:,}] failed: {e}\")\n",
    "\n",
    "            if is_shutdown_requested():\n",
    "                logging.warning(\n",
    "                    \"Waiting for active workers to finish their current ranges...\"\n",
    "                )\n",
    "                executor.shutdown(wait=True, cancel_futures=True)\n",
    "                logging.info(\n",
    "                    f\"‚úì Graceful shutdown complete. Processed {completed_ranges}/{total_ranges} ranges.\"\n",
    "                )\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logging.warning(\n",
    "                \"\\n‚ö†Ô∏è  Additional Ctrl+C detected - forcing immediate shutdown...\"\n",
    "            )\n",
    "            executor.shutdown(wait=False, cancel_futures=True)\n",
    "            raise\n",
    "\n",
    "    elapsed_total = time.time() - start_time\n",
    "    logging.info(f\"\\n{'='*60}\")\n",
    "    logging.info(f\"Scan completed!\")\n",
    "    logging.info(f\"Total events fetched: {total_events:,}\")\n",
    "    logging.info(f\"Ranges processed: {completed_ranges}/{total_ranges}\")\n",
    "    logging.info(f\"Total time: {int(elapsed_total // 60)}m {int(elapsed_total % 60)}s\")\n",
    "    logging.info(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "def post_process_normalization():\n",
    "    logging.info(\"Starting post-processing normalization...\")\n",
    "\n",
    "    pairs = get_all_unique_pairs_from_db()\n",
    "    logging.info(f\"Found {len(pairs)} unique pairs\")\n",
    "\n",
    "    for idx, pair_address in enumerate(pairs):\n",
    "        logging.info(f\"[{idx+1}/{len(pairs)}] Processing {pair_address}\")\n",
    "\n",
    "        metadata = get_pair_metadata(pair_address)\n",
    "        if metadata is None:\n",
    "            logging.info(f\"  Fetching metadata...\")\n",
    "            metadata = fetch_and_store_pair_metadata(pair_address)\n",
    "\n",
    "        if metadata and metadata[\"token0_decimals\"] is not None:\n",
    "            logging.info(\n",
    "                f\"  Normalizing values for {metadata.get('token0_symbol', '?')}/{metadata.get('token1_symbol', '?')}...\"\n",
    "            )\n",
    "            normalize_values_for_pair(pair_address)\n",
    "        else:\n",
    "            logging.warning(f\"  Cannot normalize - missing decimals\")\n",
    "\n",
    "    logging.info(\"‚úì Post-processing complete\")\n",
    "\n",
    "\n",
    "print(\"‚úì Enhanced scanning functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865dc400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Scanning Function with Batch Support\n",
    "\n",
    "def scan_blockchain_to_duckdb(\n",
    "    event_file=V2_EVENT_BY_CONTRACTS,\n",
    "    db_path=DB_PATH,\n",
    "    start_block=10000001,\n",
    "    end_block=20000000,\n",
    "    chunk_size=10000,\n",
    "    max_workers=3,\n",
    "    token_filter=None,\n",
    "):\n",
    "\n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(\"BLOCKCHAIN SCANNER STARTING\")\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    logging.info(f\"Loading addresses from {event_file}...\")\n",
    "    with open(event_file, \"r\") as f:\n",
    "        all_pairs = json.load(f)\n",
    "\n",
    "    all_addresses = [Web3.to_checksum_address(addr) for addr in all_pairs.keys()]\n",
    "\n",
    "    if token_filter:\n",
    "        filter_checksummed = [Web3.to_checksum_address(addr) for addr in token_filter]\n",
    "        addresses = [addr for addr in all_addresses if addr in filter_checksummed]\n",
    "        logging.info(\n",
    "            f\"Filtered to {len(addresses)} addresses from {len(all_addresses)} total\"\n",
    "        )\n",
    "    else:\n",
    "        addresses = all_addresses\n",
    "        logging.info(f\"Using all {len(addresses)} addresses\")\n",
    "\n",
    "    logging.info(\"Setting up Web3 connection pool...\")\n",
    "    setup_web3_pool(ETHERSCAN_API_KEY_DICT)\n",
    "\n",
    "    logging.info(f\"Setting up database at {db_path}...\")\n",
    "    setup_database(db_path)\n",
    "\n",
    "    stats = get_database_stats()\n",
    "    logging.info(\"\\nCurrent database stats:\")\n",
    "    logging.info(f\"  Transfers: {stats['total_transfers']:,}\")\n",
    "    logging.info(f\"  Swaps: {stats['total_swaps']:,}\")\n",
    "    logging.info(f\"  Mints: {stats['total_mints']:,}\")\n",
    "    logging.info(f\"  Burns: {stats['total_burns']:,}\")\n",
    "    logging.info(f\"  Syncs: {stats['total_syncs']:,}\")\n",
    "    logging.info(f\"  Approvals: {stats['total_approvals']:,}\")\n",
    "    logging.info(f\"  Pairs with metadata: {stats['total_pairs']:,}\")\n",
    "    logging.info(f\"  Completed ranges: {stats['completed_ranges']}\")\n",
    "\n",
    "    logging.info(f\"\\nStarting scan:\")\n",
    "    logging.info(f\"  Block range: {start_block:,} to {end_block:,}\")\n",
    "    logging.info(f\"  Chunk size: {chunk_size:,}\")\n",
    "    logging.info(f\"  Workers: {max_workers}\")\n",
    "    logging.info(f\"  Addresses: {len(addresses)}\")\n",
    "    logging.info(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        scan_blockchain(\n",
    "            addresses, start_block, end_block, chunk_size, max_workers, False\n",
    "        )\n",
    "\n",
    "        final_stats = get_database_stats()\n",
    "        logging.info(\"\\n\" + \"=\" * 60)\n",
    "        logging.info(\"SCAN COMPLETED!\")\n",
    "        logging.info(\"=\" * 60)\n",
    "        logging.info(f\"  Transfers: {final_stats['total_transfers']:,}\")\n",
    "        logging.info(f\"  Swaps: {final_stats['total_swaps']:,}\")\n",
    "        logging.info(f\"  Mints: {final_stats['total_mints']:,}\")\n",
    "        logging.info(f\"  Burns: {final_stats['total_burns']:,}\")\n",
    "        logging.info(f\"  Syncs: {final_stats['total_syncs']:,}\")\n",
    "        logging.info(f\"  Approvals: {final_stats['total_approvals']:,}\")\n",
    "        logging.info(\"=\" * 60)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"\\n\\nInterrupted by user - progress saved to database\")\n",
    "        logging.info(\"You can restart to continue from where it left off\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def scan_all_pairs_in_batches(\n",
    "    event_file=V2_EVENT_BY_CONTRACTS,\n",
    "    db_path=DB_PATH,\n",
    "    start_block=10000001,\n",
    "    end_block=20000000,\n",
    "    chunk_size=10000,\n",
    "    max_workers=3,\n",
    "    batch_size=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Scan all pairs in the event file in batches.\n",
    "    Safer for large numbers of pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(\"BATCH SCANNER STARTING\")\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    with open(event_file, \"r\") as f:\n",
    "        all_pairs = json.load(f)\n",
    "\n",
    "    all_addresses = list(all_pairs.keys())\n",
    "    total_pairs = len(all_addresses)\n",
    "    total_batches = (total_pairs + batch_size - 1) // batch_size\n",
    "\n",
    "    logging.info(f\"Total pairs to scan: {total_pairs}\")\n",
    "    logging.info(f\"Batch size: {batch_size} pairs per batch\")\n",
    "    logging.info(f\"Total batches: {total_batches}\")\n",
    "    logging.info(f\"Block range: {start_block:,} to {end_block:,}\")\n",
    "    logging.info(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    for i in range(0, total_pairs, batch_size):\n",
    "        batch = all_addresses[i : i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "\n",
    "        logging.info(f\"\\n{'='*60}\")\n",
    "        logging.info(f\"BATCH {batch_num}/{total_batches}\")\n",
    "        logging.info(f\"Pairs in this batch: {len(batch)}\")\n",
    "        logging.info(f\"{'='*60}\\n\")\n",
    "\n",
    "        try:\n",
    "            scan_blockchain_to_duckdb(\n",
    "                event_file=event_file,\n",
    "                db_path=db_path,\n",
    "                start_block=start_block,\n",
    "                end_block=end_block,\n",
    "                chunk_size=chunk_size,\n",
    "                max_workers=max_workers,\n",
    "                token_filter=batch,\n",
    "            )\n",
    "        except KeyboardInterrupt:\n",
    "            logging.warning(f\"\\nInterrupted at batch {batch_num}/{total_batches}\")\n",
    "            logging.info(\n",
    "                \"Progress saved - you can resume by adjusting batch parameters\"\n",
    "            )\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Batch {batch_num} failed: {e}\")\n",
    "            logging.info(f\"Continuing with next batch...\")\n",
    "            continue\n",
    "\n",
    "    logging.info(\"\\n\" + \"=\" * 60)\n",
    "    logging.info(\"ALL BATCHES COMPLETE\")\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    final_stats = get_database_stats()\n",
    "    logging.info(f\"  Total Transfers: {final_stats['total_transfers']:,}\")\n",
    "    logging.info(f\"  Total Swaps: {final_stats['total_swaps']:,}\")\n",
    "    logging.info(f\"  Total Mints: {final_stats['total_mints']:,}\")\n",
    "    logging.info(f\"  Total Burns: {final_stats['total_burns']:,}\")\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    logging.info(\"\\nCollecting metadata for all pairs...\")\n",
    "    collect_missing_pair_metadata()\n",
    "\n",
    "    logging.info(\"\\nNormalizing values...\")\n",
    "    normalize_missing_pairs()\n",
    "\n",
    "    logging.info(\"\\n‚úì Complete pipeline finished\")\n",
    "\n",
    "\n",
    "def query_database(db_path=\"out/V2/uniswap_v2_events.duckdb\"):\n",
    "\n",
    "    conn = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"DATABASE QUERIES\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        print(\"\\n1. Event counts by type:\")\n",
    "        print(\n",
    "            f\"  Transfers: {conn.execute('SELECT COUNT(*) FROM transfer').fetchone()[0]:,}\"\n",
    "        )\n",
    "        print(f\"  Swaps: {conn.execute('SELECT COUNT(*) FROM swap').fetchone()[0]:,}\")\n",
    "        print(f\"  Mints: {conn.execute('SELECT COUNT(*) FROM mint').fetchone()[0]:,}\")\n",
    "        print(f\"  Burns: {conn.execute('SELECT COUNT(*) FROM burn').fetchone()[0]:,}\")\n",
    "        print(f\"  Syncs: {conn.execute('SELECT COUNT(*) FROM sync').fetchone()[0]:,}\")\n",
    "        print(\n",
    "            f\"  Approvals: {conn.execute('SELECT COUNT(*) FROM approval').fetchone()[0]:,}\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n2. Pair metadata coverage:\")\n",
    "        result = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                COUNT(DISTINCT t.pair_address) as total_pairs,\n",
    "                COUNT(DISTINCT pm.pair_address) as pairs_with_metadata,\n",
    "                COUNT(DISTINCT CASE WHEN pm.token0_decimals IS NOT NULL THEN pm.pair_address END) as pairs_with_decimals\n",
    "            FROM (SELECT DISTINCT pair_address FROM transfer) t\n",
    "            LEFT JOIN pair_metadata pm ON t.pair_address = pm.pair_address\n",
    "        \"\"\"\n",
    "        ).fetchone()\n",
    "        print(f\"  Total pairs: {result[0]:,}\")\n",
    "        print(f\"  With metadata: {result[1]:,}\")\n",
    "        print(f\"  With decimals: {result[2]:,}\")\n",
    "\n",
    "        print(\"\\n3. Most active pairs (by swaps):\")\n",
    "        result = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                s.pair_address,\n",
    "                COALESCE(pm.token0_symbol || '/' || pm.token1_symbol, 'Unknown') as pair_name,\n",
    "                COUNT(*) as swap_count\n",
    "            FROM swap s\n",
    "            LEFT JOIN pair_metadata pm ON s.pair_address = pm.pair_address\n",
    "            GROUP BY s.pair_address, pair_name\n",
    "            ORDER BY swap_count DESC\n",
    "            LIMIT 10\n",
    "        \"\"\"\n",
    "        ).fetchdf()\n",
    "        print(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def get_pair_info(pair_address, db_path=\"out/V2/uniswap_v2_events.duckdb\"):\n",
    "    pair_address = Web3.to_checksum_address(pair_address)\n",
    "\n",
    "    conn = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"PAIR INFORMATION: {pair_address}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        metadata = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT token0_address, token1_address, token0_symbol, token1_symbol,\n",
    "                   token0_decimals, token1_decimals, created_block\n",
    "            FROM pair_metadata\n",
    "            WHERE pair_address = ?\n",
    "        \"\"\",\n",
    "            (pair_address,),\n",
    "        ).fetchone()\n",
    "\n",
    "        if metadata:\n",
    "            print(\"\\nMetadata:\")\n",
    "            print(f\"  Pair: {metadata[2] or '?'}/{metadata[3] or '?'}\")\n",
    "            print(f\"  Token0: {metadata[0]} ({metadata[4] or '?'} decimals)\")\n",
    "            print(f\"  Token1: {metadata[1]} ({metadata[5] or '?'} decimals)\")\n",
    "            if metadata[6]:\n",
    "                print(f\"  Created at block: {metadata[6]:,}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  No metadata found for this pair\")\n",
    "\n",
    "        print(\"\\nEvent counts:\")\n",
    "        transfers = conn.execute(\n",
    "            \"SELECT COUNT(*) FROM transfer WHERE pair_address = ?\", (pair_address,)\n",
    "        ).fetchone()[0]\n",
    "        swaps = conn.execute(\n",
    "            \"SELECT COUNT(*) FROM swap WHERE pair_address = ?\", (pair_address,)\n",
    "        ).fetchone()[0]\n",
    "        mints = conn.execute(\n",
    "            \"SELECT COUNT(*) FROM mint WHERE pair_address = ?\", (pair_address,)\n",
    "        ).fetchone()[0]\n",
    "        burns = conn.execute(\n",
    "            \"SELECT COUNT(*) FROM burn WHERE pair_address = ?\", (pair_address,)\n",
    "        ).fetchone()[0]\n",
    "\n",
    "        print(f\"  Transfers: {transfers:,}\")\n",
    "        print(f\"  Swaps: {swaps:,}\")\n",
    "        print(f\"  Mints: {mints:,}\")\n",
    "        print(f\"  Burns: {burns:,}\")\n",
    "\n",
    "        latest_sync = conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT reserve0_normalized, reserve1_normalized, block_number\n",
    "            FROM sync\n",
    "            WHERE pair_address = ?\n",
    "            ORDER BY block_number DESC\n",
    "            LIMIT 1\n",
    "        \"\"\",\n",
    "            (pair_address,),\n",
    "        ).fetchone()\n",
    "\n",
    "        if latest_sync and latest_sync[0] is not None:\n",
    "            print(f\"\\nLatest reserves (block {latest_sync[2]:,}):\")\n",
    "            print(f\"  Reserve0: {latest_sync[0]:,.6f}\")\n",
    "            print(f\"  Reserve1: {latest_sync[1]:,.6f}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "print(\"‚úì Enhanced main functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01dc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_filter = [\n",
    "    \"0xB4e16d0168e52d35CaCD2c6185b44281Ec28C9Dc\",\n",
    "    \"0x3139Ffc91B99aa94DA8A2dc13f1fC36F9BDc98eE\",\n",
    "    \"0x12EDE161c702D1494612d19f05992f43aa6A26FB\",\n",
    "    \"0xA478c2975Ab1Ea89e8196811F51A7B7Ade33eB11\",\n",
    "    \"0x07F068ca326a469Fc1d87d85d448990C8cBa7dF9\",\n",
    "    \"0xAE461cA67B15dc8dc81CE7615e0320dA1A9aB8D5\",\n",
    "    \"0xCe407CD7b95B39d3B4d53065E711e713dd5C5999\",\n",
    "    \"0x33C2d48Bc95FB7D0199C5C693e7a9F527145a9Af\",\n",
    "]\n",
    "START_BLOCK = 10000000\n",
    "END_BLOCK = 10500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da79d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_filter = [\n",
    "    \"0xB4e16d0168e52d35CaCD2c6185b44281Ec28C9Dc\",\n",
    "    \"0x3139Ffc91B99aa94DA8A2dc13f1fC36F9BDc98eE\",\n",
    "    \"0x12EDE161c702D1494612d19f05992f43aa6A26FB\",\n",
    "    \"0xA478c2975Ab1Ea89e8196811F51A7B7Ade33eB11\",\n",
    "    \"0x07F068ca326a469Fc1d87d85d448990C8cBa7dF9\",\n",
    "    \"0xAE461cA67B15dc8dc81CE7615e0320dA1A9aB8D5\",\n",
    "    \"0xCe407CD7b95B39d3B4d53065E711e713dd5C5999\",\n",
    "    \"0x33C2d48Bc95FB7D0199C5C693e7a9F527145a9Af\",\n",
    "]\n",
    "\n",
    "\n",
    "START_BLOCK = 6000000\n",
    "END_BLOCK = 10500000\n",
    "#END_BLOCK = w3.eth.block_number\n",
    "# Usage Example 2: Scan ALL pairs in batches (RECOMMENDED)\n",
    "try:\n",
    "    scan_all_pairs_in_batches(\n",
    "        start_block=START_BLOCK,\n",
    "        end_block=END_BLOCK,\n",
    "        chunk_size=10000,\n",
    "        max_workers=18,\n",
    "        batch_size=50,  # 50 pairs at a time\n",
    "    )   \n",
    "except KeyboardInterrupt:\n",
    "    logging.warning(\"Shutdown requested by user\")\n",
    "    # Clean shutdown logic\n",
    "except Exception as e:\n",
    "    logging.error(f\"Fatal error: {e}\")\n",
    "    raise  \n",
    "\n",
    "# Usage Example 3: Continue metadata collection after scanning\n",
    "collect_missing_pair_metadata()\n",
    "normalize_missing_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logs_with_chunking(\n",
    "    get_logs_fn,\n",
    "    from_block,\n",
    "    to_block,\n",
    "    argument_filters=None,\n",
    "    initial_chunk_size=10000,\n",
    "    max_retries=5,\n",
    "    base_delay=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch logs with automatic chunking and rate limit handling.\n",
    "    \"\"\"\n",
    "\n",
    "    if argument_filters is None:\n",
    "        argument_filters = {}\n",
    "\n",
    "    # Convert 'latest' to actual block number\n",
    "    if to_block == \"latest\":\n",
    "        to_block = w3.eth.block_number\n",
    "\n",
    "    def fetch_with_retry(start, end, retries=0):\n",
    "        \"\"\"Fetch logs with exponential backoff on rate limit errors.\"\"\"\n",
    "        try:\n",
    "            time.sleep(base_delay)\n",
    "\n",
    "            logs = get_logs_fn(\n",
    "                from_block=start, to_block=end, argument_filters=argument_filters\n",
    "            )\n",
    "            return logs\n",
    "\n",
    "        except HTTPError as e:\n",
    "            if \"429\" in str(e) or \"Too Many Requests\" in str(e):\n",
    "                if retries < max_retries:\n",
    "                    wait_time = base_delay * (2**retries)\n",
    "                    print(\n",
    "                        f\"  ‚ö† Rate limit hit, waiting {wait_time:.1f}s... (retry {retries + 1}/{max_retries})\"\n",
    "                    )\n",
    "                    time.sleep(wait_time)\n",
    "                    return fetch_with_retry(start, end, retries + 1)\n",
    "                else:\n",
    "                    print(f\"  ‚úó Max retries reached\")\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def fetch_range(start, end, chunk_size):\n",
    "        \"\"\"Recursive function to fetch a block range with dynamic chunking.\"\"\"\n",
    "\n",
    "        if end - start <= chunk_size:\n",
    "            try:\n",
    "                print(f\"Fetching blocks {start} to {end} ({end - start + 1} blocks)...\")\n",
    "                logs = fetch_with_retry(start, end)\n",
    "                print(f\"  ‚úì Got {len(logs)} logs\")\n",
    "                return logs\n",
    "\n",
    "            except HTTPError as e:\n",
    "                raise\n",
    "\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "\n",
    "                if \"-32005\" in error_str or \"more than 10000 results\" in error_str:\n",
    "                    print(f\"  ‚ö† Too many results, splitting...\")\n",
    "\n",
    "                    mid = (start + end) // 2\n",
    "\n",
    "                    if mid == start:\n",
    "                        print(f\"  ‚ö† Cannot split further\")\n",
    "                        try:\n",
    "                            if hasattr(e, \"args\") and len(e.args) > 0:\n",
    "                                error_data = e.args[0]\n",
    "                                if (\n",
    "                                    isinstance(error_data, dict)\n",
    "                                    and \"data\" in error_data\n",
    "                                ):\n",
    "                                    suggested_to = int(\n",
    "                                        error_data[\"data\"].get(\"to\", hex(end)), 16\n",
    "                                    )\n",
    "                                    if suggested_to < end:\n",
    "                                        print(f\"  Using RPC hint: {suggested_to}\")\n",
    "                                        return fetch_range(\n",
    "                                            start, suggested_to, chunk_size // 2\n",
    "                                        )\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        raise Exception(\n",
    "                            f\"Cannot split block range {start}-{end} further\"\n",
    "                        )\n",
    "\n",
    "                    left_logs = fetch_range(start, mid, chunk_size // 2)\n",
    "                    right_logs = fetch_range(mid + 1, end, chunk_size // 2)\n",
    "\n",
    "                    return left_logs + right_logs\n",
    "                else:\n",
    "                    print(f\"  ‚úó Error: {e}\")\n",
    "                    raise\n",
    "\n",
    "        else:\n",
    "            print(f\"Splitting range {start}-{end} into chunks of {chunk_size}...\")\n",
    "            current = start\n",
    "            logs = []\n",
    "\n",
    "            while current <= end:\n",
    "                chunk_end = min(current + chunk_size - 1, end)\n",
    "                chunk_logs = fetch_range(current, chunk_end, chunk_size)\n",
    "                logs.extend(chunk_logs)\n",
    "                current = chunk_end + 1\n",
    "\n",
    "            return logs\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fetching logs from block {from_block} to {to_block}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    all_logs = fetch_range(from_block, to_block, initial_chunk_size)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì Complete! Total logs fetched: {len(all_logs)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return all_logs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN CODE\n",
    "# ============================================================\n",
    "\n",
    "# 1. Get the factory contract\n",
    "address, abi, contract = get_address_abi_contract(UNISWAP_V2_CONTRACT)\n",
    "\n",
    "start_block = 0\n",
    "end_block = \"latest\"\n",
    "\n",
    "# 2. Fetch all PairCreated events\n",
    "print(\"Fetching all PairCreated events from Uniswap V2 Factory...\")\n",
    "pair_created_logs = get_logs_with_chunking(\n",
    "    get_logs_fn=contract.events.PairCreated().get_logs,\n",
    "    from_block=start_block,\n",
    "    to_block=end_block,\n",
    "    argument_filters={},\n",
    "    initial_chunk_size=10000,\n",
    "    max_retries=5,\n",
    "    base_delay=0.5,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Found {len(pair_created_logs)} pairs\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 3. Get event names from one sample pair (all pairs have same interface)\n",
    "if len(pair_created_logs) > 0:\n",
    "    sample_pair_address = pair_created_logs[0].args.pair\n",
    "    print(f\"Getting event list from sample pair: {sample_pair_address}\")\n",
    "\n",
    "    pair_address, pair_abi, pair_contract = get_address_abi_contract(\n",
    "        sample_pair_address\n",
    "    )\n",
    "    event_names = [ev.event_name for ev in pair_contract.events]\n",
    "\n",
    "    print(f\"All pairs have these events: {event_names}\\n\")\n",
    "\n",
    "    # 4. Build the dictionary structure\n",
    "    FULL_EVENT_BY_CONTRACTS = {}\n",
    "\n",
    "    print(f\"Building dictionary structure for {len(pair_created_logs)} pairs...\")\n",
    "\n",
    "    for idx, log in enumerate(pair_created_logs):\n",
    "        pair_addr = log.args.pair\n",
    "\n",
    "        # Create structure with empty dicts for each event\n",
    "        FULL_EVENT_BY_CONTRACTS[pair_addr] = {event: {} for event in event_names}\n",
    "\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(pair_created_logs)} pairs...\")\n",
    "\n",
    "    print(f\"  ‚úì Completed all {len(pair_created_logs)} pairs\\n\")\n",
    "\n",
    "    # 5. Save to disk\n",
    "    output_file = \"uniswap_v2_pairs_events.json\"\n",
    "\n",
    "    print(f\"Saving to {output_file}...\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(FULL_EVENT_BY_CONTRACTS, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"‚úì Saved successfully!\")\n",
    "\n",
    "    # 6. Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total pairs: {len(FULL_EVENT_BY_CONTRACTS)}\")\n",
    "    print(f\"Events per pair: {len(event_names)}\")\n",
    "    print(f\"Event types: {', '.join(event_names)}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Print first 3 pairs as sample\n",
    "    print(\"Sample (first 3 pairs):\")\n",
    "    for idx, (pair_addr, events) in enumerate(\n",
    "        list(FULL_EVENT_BY_CONTRACTS.items())[:3]\n",
    "    ):\n",
    "        print(f\"\\n{pair_addr}:\")\n",
    "        for event_name in events.keys():\n",
    "            print(f\"  - {event_name}: {{}}\")\n",
    "\n",
    "else:\n",
    "    print(\"No pairs found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3db616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important code\n",
    "# We look for the Genesis Uniswap factory, and we get all its events (Only 1 for the V1_factory: 'NewExchange', Only 1 for the V2_factory: PairCreated)\n",
    "# Then we scan from 0 to latest block every NexEchange created from this Factory\n",
    "# (We have the filter of events in case we are filtering events from contract that have multiple events to remove when we don't care)\n",
    "address, abi, contract = get_address_abi_contract(\n",
    "    UNISWAP_V2_CONTRACT\n",
    ")  # Uniswap Genesis Factory\n",
    "start_block = 0\n",
    "end_block = 'latest'\n",
    "# list all event names\n",
    "event_names = [ev.event_name for ev in contract.events]\n",
    "print(event_names)\n",
    "# define which events you want and filters directly\n",
    "events_to_scan = [\n",
    "    contract.events.PairCreated().get_logs,\n",
    "    #contract.events.Transfer().get_logs,\n",
    "    #contract.events.Approval().get_logs,\n",
    "]\n",
    "L_LOGS = [] # IMPORTANT\n",
    "for get_logs_fn in events_to_scan:\n",
    "    logs = get_logs_fn(\n",
    "        from_block=start_block,\n",
    "        to_block=end_block,\n",
    "        argument_filters={},  # or {\"from\": some_address}, {\"to\": [addr1, addr2]}\n",
    "    )\n",
    "    for log in logs:\n",
    "        # print(log[\"transactionHash\"].hex(), log[\"blockNumber\"], log[\"event\"])\n",
    "        L_LOGS.append(log)\n",
    "\n",
    "# Important code we use in combination with the events filter\n",
    "# We created a list of Exchange created by the Uniswap V1 Factory Contract and we list all their Events\n",
    "# We create the Dictionnary\n",
    "# \"exchange_address_1\": {\"event_1\": {}, event_2: {}, event_3:{}}\n",
    "# This dict fed with the code allow us to retrieve every transactions with the events(logs) of this exchange\n",
    "# we can then sniff Liquidity out of it\n",
    "\n",
    "FULL_EVENT_BY_CONTRACTS = {}  # IMPORTANT\n",
    "for log in L_LOGS:\n",
    "    add, abi, contract = get_address_abi_contract(log.args.exchange)\n",
    "    event_names = [ev.event_name for ev in contract.events]\n",
    "    FULL_EVENT_BY_CONTRACTS[add] = {event: {} for event in event_names}\n",
    "    time.sleep(1)\n",
    "\n",
    "print(len(FULL_EVENT_BY_CONTRACTS)) \n",
    "\n",
    "if not os.path.exists(V2_EVENT_BY_CONTRACTS):\n",
    "    with open(V2_EVENT_BY_CONTRACTS, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(V2_EVENT_BY_CONTRACTS, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Query specific pair data for analysis\n",
    "\n",
    "\n",
    "def load_pair_data_for_analysis(pair_address, block_start=None, block_end=None):\n",
    "    \"\"\"\n",
    "    Load normalized transfer data for a specific pair.\n",
    "    This is what you need for liquidity analysis.\n",
    "    \"\"\"\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "\n",
    "    block_filter = \"\"\n",
    "    if block_start and block_end:\n",
    "        block_filter = f\"AND block_number BETWEEN {block_start} AND {block_end}\"\n",
    "    elif block_start:\n",
    "        block_filter = f\"AND block_number >= {block_start}\"\n",
    "    elif block_end:\n",
    "        block_filter = f\"AND block_number <= {block_end}\"\n",
    "\n",
    "    conn = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "    # Get pair metadata\n",
    "    metadata = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT token0_symbol, token1_symbol, token0_decimals, token1_decimals\n",
    "        FROM pair_metadata\n",
    "        WHERE pair_address = ?\n",
    "    \"\"\",\n",
    "        (pair_address,),\n",
    "    ).fetchone()\n",
    "\n",
    "    if metadata:\n",
    "        pair_name = f\"{metadata[0]}/{metadata[1]}\"\n",
    "        print(f\"Loading data for {pair_name} pair ({pair_address[:10]}...)\")\n",
    "    else:\n",
    "        pair_name = \"Unknown\"\n",
    "        print(f\"‚ö†Ô∏è  No metadata found for {pair_address}\")\n",
    "\n",
    "    # Load transfer events (this is what you need for liquidity tracking)\n",
    "    df = conn.execute(\n",
    "        f\"\"\"\n",
    "        SELECT \n",
    "            block_number as block,\n",
    "            pair_address as address,\n",
    "            from_address,\n",
    "            to_address,\n",
    "            COALESCE(value_normalized, CAST(value AS DOUBLE) / 1e18) as value\n",
    "        FROM transfer\n",
    "        WHERE pair_address = ?\n",
    "        {block_filter}\n",
    "        ORDER BY block_number, log_index\n",
    "    \"\"\",\n",
    "        (pair_address,),\n",
    "    ).fetchdf()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è  No transfer events found\")\n",
    "        return df\n",
    "\n",
    "    # Add metadata\n",
    "    df[\"pair_name\"] = pair_name\n",
    "    df[\"address\"] = df[\"address\"].apply(w3.to_checksum_address)\n",
    "    df[\"from_address\"] = df[\"from_address\"].apply(w3.to_checksum_address)\n",
    "    df[\"to_address\"] = df[\"to_address\"].apply(w3.to_checksum_address)\n",
    "\n",
    "    print(f\"Loaded {len(df):,} transfer events\")\n",
    "    print(f\"Block range: {df['block'].min():,} to {df['block'].max():,}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_pair_summary_stats(pair_address):\n",
    "    \"\"\"\n",
    "    Get summary statistics for a pair.\n",
    "    \"\"\"\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "\n",
    "    conn = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "    stats = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT t.transaction_hash) as total_transactions,\n",
    "            COUNT(*) FILTER (WHERE t.from_address = '0x0000000000000000000000000000000000000000') as mints,\n",
    "            COUNT(*) FILTER (WHERE t.to_address = '0x0000000000000000000000000000000000000000') as burns,\n",
    "            COUNT(*) FILTER (WHERE t.from_address != '0x0000000000000000000000000000000000000000' \n",
    "                             AND t.to_address != '0x0000000000000000000000000000000000000000') as transfers,\n",
    "            (SELECT COUNT(*) FROM swap WHERE pair_address = ?) as swaps,\n",
    "            MIN(t.block_number) as first_block,\n",
    "            MAX(t.block_number) as last_block\n",
    "        FROM transfer t\n",
    "        WHERE t.pair_address = ?\n",
    "    \"\"\",\n",
    "        (pair_address, pair_address),\n",
    "    ).fetchone()\n",
    "\n",
    "    metadata = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT token0_symbol, token1_symbol\n",
    "        FROM pair_metadata\n",
    "        WHERE pair_address = ?\n",
    "    \"\"\",\n",
    "        (pair_address,),\n",
    "    ).fetchone()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    if metadata:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PAIR SUMMARY: {metadata[0]}/{metadata[1]}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    print(f\"Total transactions: {stats[0]:,}\")\n",
    "    print(f\"Mints (add liquidity): {stats[1]:,}\")\n",
    "    print(f\"Burns (remove liquidity): {stats[2]:,}\")\n",
    "    print(f\"Transfers (LP token): {stats[3]:,}\")\n",
    "    print(f\"Swaps: {stats[4]:,}\")\n",
    "    print(f\"Block range: {stats[5]:,} to {stats[6]:,}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "df = load_pair_data_for_analysis(token_filter[0], START_BLOCK, END_BLOCK)\n",
    "get_pair_summary_stats(token_filter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ac509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ST GRAPH, evolution of the UNISWAP v1 (UNI-V1) amount of token issued/burned (GLOBAL TOTAL over block)\n",
    "# Important to compare the size of every pool but we need to link \"value\" to either $ or something relevant for comparison\n",
    "# NEED: df\n",
    "totals = (\n",
    "    df.groupby([\"block\", \"address\"], as_index=False)[\"value\"]\n",
    "    .sum()\n",
    "    .sort_values([\"address\", \"block\"])\n",
    ")\n",
    "totals[\"cum_value\"] = totals.groupby(\"address\")[\"value\"].cumsum()\n",
    "\n",
    "# # 2) fill missing blocks only inside each address' span (min..max), then cumulate\n",
    "# totals = totals.groupby(\"address\", group_keys=False).apply(\n",
    "#     lambda g: (\n",
    "#         g.set_index(\"block\")\n",
    "#         .reindex(range(g[\"block\"].min(), g[\"block\"].max() + 1), fill_value=0)\n",
    "#         .rename_axis(\"block\")\n",
    "#         .reset_index()\n",
    "#         .assign(address=g.name)\n",
    "#     )\n",
    "# )\n",
    "# totals = (\n",
    "#     totals[[\"block\", \"address\", \"value\"]]\n",
    "#     .sort_values([\"address\", \"block\"])\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "pools_of_interest = [\n",
    "    w3.to_checksum_address(token_filter[0]),\n",
    "    w3.to_checksum_address(token_filter[1]),\n",
    "    w3.to_checksum_address(token_filter[2]),\n",
    "]\n",
    "\n",
    "# pools_of_interest = [\"add_1\",\"add_2\",\"add_3\"]\n",
    "cum_long_sub = totals[totals[\"address\"].isin(pools_of_interest)]\n",
    "\n",
    "fig = px.area(\n",
    "    cum_long_sub,\n",
    "    x=\"block\",\n",
    "    y=\"cum_value\",\n",
    "    color=\"address\",\n",
    "    line_group=\"address\",\n",
    "    title=\"Cumulative liquidity evolution per pool\",\n",
    "    labels={\"cum_value\": \"Cumulative liquidity\", \"address\": \"Pool address\"},\n",
    ")\n",
    "\n",
    "# Optionally, you can also do px.line instead of px.area if you prefer lines without fill\n",
    "fig = px.line(cum_long_sub, x=\"block\", y=\"cum_value\", color=\"address\",\n",
    "              title=\"Cumulative liquidity per pool\")\n",
    "# You can also make it not stacked (i.e. overlayed) by doing:\n",
    "# fig = px.area(\n",
    "#     cum_long_sub,\n",
    "#     x=\"block\",\n",
    "#     y=\"cum_value\",\n",
    "#     color=\"address\",\n",
    "#     line_group=\"address\",\n",
    "#     facet_col=None,\n",
    "#     # maybe set `groupnorm=None` or other arguments\n",
    "# )\n",
    "\n",
    "fig.update_layout(legend_title=\"Pool address\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block_filter(block_start=None, block_end=None):\n",
    "    if block_start is not None and block_end is not None:\n",
    "        return f\"AND block_number BETWEEN {block_start} AND {block_end}\"\n",
    "    if block_start is not None:\n",
    "        return f\"AND block_number >= {block_start}\"\n",
    "    if block_end is not None:\n",
    "        return f\"AND block_number <= {block_end}\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def calculate_pool_liquidity_python_optimized(\n",
    "    db_path, pair_address, block_start=None, block_end=None\n",
    "):\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "    block_filter = build_block_filter(block_start, block_end)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        block_number AS block,\n",
    "        from_address AS provider,\n",
    "        -CAST(value AS DOUBLE) AS delta\n",
    "    FROM transfer\n",
    "    WHERE pair_address = '{pair_address}'\n",
    "        AND from_address != '0x0000000000000000000000000000000000000000'\n",
    "        {block_filter}\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        block_number AS block,\n",
    "        to_address AS provider,\n",
    "        CAST(value AS DOUBLE) AS delta\n",
    "    FROM transfer\n",
    "    WHERE pair_address = '{pair_address}'\n",
    "        AND to_address != '0x0000000000000000000000000000000000000000'\n",
    "        {block_filter}\n",
    "    ORDER BY block, provider\n",
    "    \"\"\"\n",
    "\n",
    "    with duckdb.connect(db_path, read_only=True) as conn:\n",
    "        df = conn.execute(query).fetch_df()\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_grouped = df.groupby([\"block\", \"provider\"], as_index=False)[\"delta\"].sum()\n",
    "    df_grouped = df_grouped.sort_values([\"provider\", \"block\"])\n",
    "    df_grouped[\"cum_provider\"] = df_grouped.groupby(\"provider\")[\"delta\"].cumsum()\n",
    "\n",
    "    all_blocks = np.sort(df_grouped[\"block\"].unique())\n",
    "    all_providers = df_grouped[\"provider\"].unique()\n",
    "\n",
    "    provider_histories = []\n",
    "\n",
    "    for provider in all_providers:\n",
    "        provider_data = df_grouped[df_grouped[\"provider\"] == provider].copy()\n",
    "        first_block = provider_data[\"block\"].min()\n",
    "\n",
    "        provider_blocks = provider_data[\"block\"].values\n",
    "        provider_balances = provider_data[\"cum_provider\"].values\n",
    "\n",
    "        for block in all_blocks:\n",
    "            if block >= first_block:\n",
    "                idx = np.searchsorted(provider_blocks, block, side=\"right\") - 1\n",
    "                if idx >= 0:\n",
    "                    balance = provider_balances[idx]\n",
    "                    if balance > 1e-8:\n",
    "                        provider_histories.append(\n",
    "                            {\n",
    "                                \"block\": block,\n",
    "                                \"provider\": provider,\n",
    "                                \"cum_provider\": balance,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "    df_full = pd.DataFrame(provider_histories)\n",
    "\n",
    "    if df_full.empty:\n",
    "        return df_full\n",
    "\n",
    "    pool_per_block = df_full.groupby(\"block\", as_index=False)[\"cum_provider\"].sum()\n",
    "    pool_per_block.rename(columns={\"cum_provider\": \"cum_pool\"}, inplace=True)\n",
    "\n",
    "    df_full = df_full.merge(pool_per_block, on=\"block\", how=\"left\")\n",
    "\n",
    "    df_full[\"share_pct\"] = np.where(\n",
    "        df_full[\"cum_pool\"].abs() < 1e-10,\n",
    "        0.0,\n",
    "        (df_full[\"cum_provider\"] / df_full[\"cum_pool\"] * 100),\n",
    "    )\n",
    "    df_full[\"share_pct\"] = df_full[\"share_pct\"].clip(0, 100)\n",
    "\n",
    "    df_full = df_full[df_full[\"share_pct\"] >= 0.1].copy()\n",
    "\n",
    "    df_full[\"provider_label\"] = df_full[\"provider\"].apply(create_provider_label)\n",
    "\n",
    "    return df_full.sort_values([\"block\", \"provider\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def validate_liquidity_data(db_path, pair_address, block_start=None, block_end=None):\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "    block_filter = build_block_filter(block_start, block_end)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH mints AS (\n",
    "        SELECT \n",
    "            block_number,\n",
    "            SUM(CAST(value AS DOUBLE)) AS minted\n",
    "        FROM transfer\n",
    "        WHERE pair_address = '{pair_address}'\n",
    "            AND from_address = '0x0000000000000000000000000000000000000000'\n",
    "            {block_filter}\n",
    "        GROUP BY block_number\n",
    "    ),\n",
    "    burns AS (\n",
    "        SELECT \n",
    "            block_number,\n",
    "            SUM(CAST(value AS DOUBLE)) AS burned\n",
    "        FROM transfer\n",
    "        WHERE pair_address = '{pair_address}'\n",
    "            AND to_address = '0x0000000000000000000000000000000000000000'\n",
    "            {block_filter}\n",
    "        GROUP BY block_number\n",
    "    ),\n",
    "    all_blocks AS (\n",
    "        SELECT DISTINCT block_number FROM mints\n",
    "        UNION\n",
    "        SELECT DISTINCT block_number FROM burns\n",
    "    ),\n",
    "    supply_tracking AS (\n",
    "        SELECT \n",
    "            ab.block_number,\n",
    "            COALESCE(m.minted, 0) AS minted,\n",
    "            COALESCE(b.burned, 0) AS burned,\n",
    "            SUM(COALESCE(m.minted, 0) - COALESCE(b.burned, 0)) \n",
    "                OVER (ORDER BY ab.block_number) AS cumulative_supply\n",
    "        FROM all_blocks ab\n",
    "        LEFT JOIN mints m ON ab.block_number = m.block_number\n",
    "        LEFT JOIN burns b ON ab.block_number = b.block_number\n",
    "    )\n",
    "    SELECT * FROM supply_tracking ORDER BY block_number\n",
    "    \"\"\"\n",
    "\n",
    "    with duckdb.connect(db_path, read_only=True) as conn:\n",
    "        validation_df = conn.execute(query).fetch_df()\n",
    "\n",
    "    return validation_df\n",
    "\n",
    "\n",
    "def print_liquidity_validation(db_path, pair_address, block_start=None, block_end=None):\n",
    "    validation_df = validate_liquidity_data(\n",
    "        db_path, pair_address, block_start, block_end\n",
    "    )\n",
    "\n",
    "    if validation_df.empty:\n",
    "        print(\"‚ö†Ô∏è  No liquidity events found\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LIQUIDITY VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    total_minted = validation_df[\"minted\"].sum()\n",
    "    total_burned = validation_df[\"burned\"].sum()\n",
    "    final_supply = validation_df[\"cumulative_supply\"].iloc[-1]\n",
    "\n",
    "    print(f\"Total LP Tokens Minted: {total_minted:,.6f}\")\n",
    "    print(f\"Total LP Tokens Burned: {total_burned:,.6f}\")\n",
    "    print(f\"Net Supply (Current): {final_supply:,.6f}\")\n",
    "    print(f\"First Event Block: {validation_df['block_number'].min()}\")\n",
    "    print(f\"Last Event Block: {validation_df['block_number'].max()}\")\n",
    "    print(f\"Total Events: {len(validation_df)}\")\n",
    "\n",
    "    if final_supply < 1000:\n",
    "        print(\n",
    "            \"‚ö†Ô∏è  Warning: Supply seems low. First 1000 wei should be permanently locked.\"\n",
    "        )\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def get_mint_burn_summary(db_path, pair_address, block_start=None, block_end=None):\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "    block_filter = build_block_filter(block_start, block_end)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH mint_events AS (\n",
    "        SELECT \n",
    "            t.block_number,\n",
    "            t.to_address AS provider,\n",
    "            CAST(t.value AS DOUBLE) AS lp_tokens,\n",
    "            m.amount0,\n",
    "            m.amount1\n",
    "        FROM transfer t\n",
    "        LEFT JOIN mint m ON t.transaction_hash = m.transaction_hash \n",
    "            AND t.pair_address = m.pair_address\n",
    "        WHERE t.pair_address = '{pair_address}'\n",
    "            AND t.from_address = '0x0000000000000000000000000000000000000000'\n",
    "            AND t.to_address != '0x0000000000000000000000000000000000000000'\n",
    "            {block_filter.replace('block_number', 't.block_number') if block_filter else ''}\n",
    "    ),\n",
    "    burn_events AS (\n",
    "        SELECT \n",
    "            t.block_number,\n",
    "            t.from_address AS provider,\n",
    "            CAST(t.value AS DOUBLE) AS lp_tokens,\n",
    "            b.amount0,\n",
    "            b.amount1\n",
    "        FROM transfer t\n",
    "        LEFT JOIN burn b ON t.transaction_hash = b.transaction_hash \n",
    "            AND t.pair_address = b.pair_address\n",
    "        WHERE t.pair_address = '{pair_address}'\n",
    "            AND t.to_address = '0x0000000000000000000000000000000000000000'\n",
    "            AND t.from_address != '0x0000000000000000000000000000000000000000'\n",
    "            {block_filter.replace('block_number', 't.block_number') if block_filter else ''}\n",
    "    )\n",
    "    SELECT \n",
    "        'MINT' AS event_type,\n",
    "        COUNT(*) AS event_count,\n",
    "        SUM(lp_tokens) AS total_lp_tokens,\n",
    "        SUM(CAST(amount0 AS DOUBLE)) AS total_amount0,\n",
    "        SUM(CAST(amount1 AS DOUBLE)) AS total_amount1\n",
    "    FROM mint_events\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'BURN' AS event_type,\n",
    "        COUNT(*) AS event_count,\n",
    "        SUM(lp_tokens) AS total_lp_tokens,\n",
    "        SUM(CAST(amount0 AS DOUBLE)) AS total_amount0,\n",
    "        SUM(CAST(amount1 AS DOUBLE)) AS total_amount1\n",
    "    FROM burn_events\n",
    "    \"\"\"\n",
    "\n",
    "    with duckdb.connect(db_path, read_only=True) as conn:\n",
    "        summary_df = conn.execute(query).fetch_df()\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def print_mint_burn_summary(db_path, pair_address, block_start=None, block_end=None):\n",
    "    summary_df = get_mint_burn_summary(db_path, pair_address, block_start, block_end)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MINT/BURN SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for _, row in summary_df.iterrows():\n",
    "        print(f\"\\n{row['event_type']} Events:\")\n",
    "        print(f\"  Count: {int(row['event_count'])}\")\n",
    "        print(f\"  Total LP Tokens: {row['total_lp_tokens']:,.6f}\")\n",
    "        if row[\"total_amount0\"] is not None:\n",
    "            print(f\"  Total Token0: {row['total_amount0']:,.6f}\")\n",
    "        if row[\"total_amount1\"] is not None:\n",
    "            print(f\"  Total Token1: {row['total_amount1']:,.6f}\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def analyze_pool_liquidity(\n",
    "    db_path,\n",
    "    pair_address,\n",
    "    block_start=None,\n",
    "    block_end=None,\n",
    "    show_plots=True,\n",
    "    show_validation=True,\n",
    "):\n",
    "    pair_address = w3.to_checksum_address(pair_address)\n",
    "\n",
    "    if show_validation:\n",
    "        print_liquidity_validation(db_path, pair_address, block_start, block_end)\n",
    "        print_mint_burn_summary(db_path, pair_address, block_start, block_end)\n",
    "\n",
    "    print(\"\\nCalculating pool liquidity distribution...\")\n",
    "    liquidity_df = calculate_pool_liquidity_python_optimized(\n",
    "        db_path=db_path,\n",
    "        pair_address=pair_address,\n",
    "        block_start=block_start,\n",
    "        block_end=block_end,\n",
    "    )\n",
    "\n",
    "    if liquidity_df.empty:\n",
    "        print(\"‚ö†Ô∏è  No liquidity data found for this pair\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Total rows in liquidity data: {len(liquidity_df)}\")\n",
    "    print(\n",
    "        f\"Block range: {liquidity_df['block'].min()} to {liquidity_df['block'].max()}\"\n",
    "    )\n",
    "    print(f\"Number of unique providers: {liquidity_df['provider'].nunique()}\")\n",
    "\n",
    "    if show_plots:\n",
    "        print(\"\\nGenerating percentage ownership chart...\")\n",
    "        fig_pct = plot_staircase_ownership(liquidity_df)\n",
    "        fig_pct.show()\n",
    "\n",
    "        print(\"Generating absolute liquidity chart...\")\n",
    "        fig_abs = plot_absolute_liquidity_staircase(liquidity_df)\n",
    "        fig_abs.show()\n",
    "\n",
    "        print(\"Generating concentration analysis...\")\n",
    "        fig_conc, concentration_metrics = plot_ownership_concentration(liquidity_df)\n",
    "        fig_conc.show()\n",
    "\n",
    "        print(\"\\nGenerating current ownership snapshot (bubble chart)...\")\n",
    "        fig_bubble = plot_bubble_ownership_snapshot(liquidity_df)\n",
    "        fig_bubble.show()\n",
    "    else:\n",
    "        _, concentration_metrics = plot_ownership_concentration(liquidity_df)\n",
    "\n",
    "    print_liquidity_summary(liquidity_df)\n",
    "    print_concentration_summary(concentration_metrics)\n",
    "\n",
    "    return liquidity_df, concentration_metrics\n",
    "\n",
    "\n",
    "def create_provider_label(address):\n",
    "    checksum_addr = w3.to_checksum_address(address)\n",
    "    short_addr = f\"{checksum_addr[:6]}...{checksum_addr[-4:]}\"\n",
    "    return short_addr\n",
    "\n",
    "\n",
    "def add_million_block_markers(fig, min_block, max_block):\n",
    "    start = (min_block // 1_000_000) * 1_000_000\n",
    "    end = (max_block // 1_000_000 + 1) * 1_000_000 + 1\n",
    "\n",
    "    for million_block in range(start, end, 1_000_000):\n",
    "        if min_block <= million_block <= max_block:\n",
    "            fig.add_vline(\n",
    "                x=million_block,\n",
    "                line_width=2,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"black\",\n",
    "                opacity=0.4,\n",
    "                annotation_text=f\"{million_block / 1_000_000:.0f}M\",\n",
    "                annotation_position=\"top\",\n",
    "                annotation_font_size=12,\n",
    "            )\n",
    "\n",
    "\n",
    "def plot_staircase_ownership(df):\n",
    "    fig = go.Figure()\n",
    "    providers = sorted(df[\"provider_label\"].unique())\n",
    "\n",
    "    for provider in providers:\n",
    "        provider_data = df[df[\"provider_label\"] == provider].sort_values(\"block\")\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=provider_data[\"block\"],\n",
    "                y=provider_data[\"share_pct\"],\n",
    "                name=provider,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5, shape=\"hv\"),\n",
    "                stackgroup=\"one\",\n",
    "                groupnorm=\"\",\n",
    "                hovertemplate=\"<b>%{fullData.name}</b><br>Block: %{x}<br>Share: %{y:.4f}%<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    add_million_block_markers(fig, df[\"block\"].min(), df[\"block\"].max())\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Pool Ownership Distribution (Staircase View)\",\n",
    "        hovermode=\"x\",\n",
    "        yaxis_title=\"Ownership Share (%)\",\n",
    "        xaxis_title=\"Block Number\",\n",
    "        legend=dict(\n",
    "            title=\"Provider\",\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "        ),\n",
    "        yaxis=dict(range=[0, 100]),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_absolute_liquidity_staircase(df):\n",
    "    fig = go.Figure()\n",
    "    providers = sorted(df[\"provider_label\"].unique())\n",
    "\n",
    "    for provider in providers:\n",
    "        provider_data = df[df[\"provider_label\"] == provider].sort_values(\"block\")\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=provider_data[\"block\"],\n",
    "                y=provider_data[\"cum_provider\"],\n",
    "                name=provider,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5, shape=\"hv\"),\n",
    "                stackgroup=\"one\",\n",
    "                hovertemplate=\"<b>%{fullData.name}</b><br>Block: %{x}<br>Amount: %{y:.6f}<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    add_million_block_markers(fig, df[\"block\"].min(), df[\"block\"].max())\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Pool Liquidity by Provider (Absolute Values)\",\n",
    "        hovermode=\"x\",\n",
    "        yaxis_title=\"Liquidity Amount (Token Units)\",\n",
    "        xaxis_title=\"Block Number\",\n",
    "        legend=dict(\n",
    "            title=\"Provider\",\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def calculate_hhi_metrics(df):\n",
    "    share_clean = np.where(\n",
    "        np.isinf(df[\"share_pct\"]) | np.isnan(df[\"share_pct\"]), 0, df[\"share_pct\"]\n",
    "    )\n",
    "    df = df.assign(share_pct_clean=share_clean)\n",
    "\n",
    "    hhi_agg = (\n",
    "        df.groupby(\"block\")\n",
    "        .agg(\n",
    "            hhi=(\"share_pct_clean\", lambda x: (x**2).sum()),\n",
    "            active_providers=(\"share_pct_clean\", lambda x: (x > 0.01).sum()),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return hhi_agg\n",
    "\n",
    "\n",
    "def add_hhi_zones(fig):\n",
    "    zones = [\n",
    "        (0, 1500, \"green\", \"Competitive\"),\n",
    "        (1500, 2500, \"yellow\", \"Moderate\"),\n",
    "        (2500, 10000, \"red\", \"Concentrated\"),\n",
    "    ]\n",
    "\n",
    "    for y0, y1, color, label in zones:\n",
    "        fig.add_hrect(\n",
    "            y0=y0,\n",
    "            y1=y1,\n",
    "            fillcolor=color,\n",
    "            opacity=0.1,\n",
    "            annotation_text=label,\n",
    "            secondary_y=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_ownership_concentration(df):\n",
    "    hhi_df = calculate_hhi_metrics(df)\n",
    "\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hhi_df[\"block\"],\n",
    "            y=hhi_df[\"hhi\"],\n",
    "            name=\"HHI (Concentration)\",\n",
    "            line=dict(color=\"#F46821\", width=2),\n",
    "        ),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hhi_df[\"block\"],\n",
    "            y=hhi_df[\"active_providers\"],\n",
    "            name=\"Active Providers\",\n",
    "            line=dict(color=\"#29BEFD\", width=2),\n",
    "        ),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title=\"Pool Concentration Analysis\", hovermode=\"x unified\")\n",
    "    fig.update_xaxes(title_text=\"Block Number\")\n",
    "    fig.update_yaxes(title_text=\"HHI Score\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Number of Providers\", secondary_y=True)\n",
    "\n",
    "    add_hhi_zones(fig)\n",
    "\n",
    "    return fig, hhi_df\n",
    "\n",
    "\n",
    "def get_concentration_status(hhi):\n",
    "    if hhi < 1500:\n",
    "        return \"‚úÖ COMPETITIVE (Decentralized)\"\n",
    "    elif hhi < 2500:\n",
    "        return \"‚ö†Ô∏è  MODERATE CONCENTRATION\"\n",
    "    else:\n",
    "        return \"üî¥ HIGHLY CONCENTRATED\"\n",
    "\n",
    "\n",
    "def print_liquidity_summary(df):\n",
    "    max_block = df[\"block\"].max()\n",
    "\n",
    "    summary = (\n",
    "        df.groupby([\"provider\", \"provider_label\"])[\"cum_provider\"]\n",
    "        .last()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LIQUIDITY SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for (provider, label), amount in summary.items():\n",
    "        provider_checksum = w3.to_checksum_address(provider)\n",
    "        final_data = df[\n",
    "            (df[\"provider\"] == provider_checksum) & (df[\"block\"] == max_block)\n",
    "        ]\n",
    "\n",
    "        if not final_data.empty:\n",
    "            share = final_data[\"share_pct\"].values[0]\n",
    "            print(f\"{label}: {amount:.6f} tokens ({share:.2f}% of pool)\")\n",
    "        else:\n",
    "            print(f\"{label}: {amount:.6f} tokens (exited)\")\n",
    "\n",
    "\n",
    "def print_concentration_summary(hhi_df):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CONCENTRATION METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Average HHI: {hhi_df['hhi'].mean():.2f}\")\n",
    "    print(f\"Current HHI: {hhi_df['hhi'].iloc[-1]:.2f}\")\n",
    "    print(f\"Max providers at any block: {hhi_df['active_providers'].max()}\")\n",
    "    print(f\"Current active providers: {hhi_df['active_providers'].iloc[-1]}\")\n",
    "\n",
    "    current_hhi = hhi_df[\"hhi\"].iloc[-1]\n",
    "    status = get_concentration_status(current_hhi)\n",
    "    print(f\"Pool status: {status}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def plot_bubble_ownership_snapshot(df):\n",
    "    latest_block = df[\"block\"].max()\n",
    "    latest_data = df[df[\"block\"] == latest_block].copy()\n",
    "    latest_data = latest_data.sort_values(\"share_pct\", ascending=False)\n",
    "\n",
    "    n_providers = len(latest_data)\n",
    "    cols = int(np.ceil(np.sqrt(n_providers)))\n",
    "\n",
    "    latest_data[\"x_pos\"] = latest_data.index % cols\n",
    "    latest_data[\"y_pos\"] = latest_data.index // cols\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=latest_data[\"x_pos\"],\n",
    "            y=latest_data[\"y_pos\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=latest_data[\"share_pct\"] * 15,\n",
    "                sizemode=\"diameter\",\n",
    "                sizemin=30,\n",
    "                color=latest_data[\"share_pct\"],\n",
    "                colorscale=\"RdYlGn_r\",\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Share (%)\", thickness=20, len=0.7),\n",
    "                line=dict(color=\"darkgray\", width=3),\n",
    "                opacity=0.8,\n",
    "            ),\n",
    "            hovertemplate=(\n",
    "                \"<b>%{customdata[0]}</b><br>\"\n",
    "                \"Share: %{customdata[1]:.4f}%<br>\"\n",
    "                \"LP Tokens: %{customdata[2]:.6f}\"\n",
    "                \"<extra></extra>\"\n",
    "            ),\n",
    "            customdata=latest_data[\n",
    "                [\"provider_label\", \"share_pct\", \"cum_provider\"]\n",
    "            ].values,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Pool Ownership Snapshot at Block {latest_block:,}\",\n",
    "        xaxis=dict(visible=False, range=[-0.5, cols - 0.5]),\n",
    "        yaxis=dict(visible=False, scaleanchor=\"x\", scaleratio=1),\n",
    "        height=600,\n",
    "        width=800,\n",
    "        showlegend=False,\n",
    "        hovermode=\"closest\",\n",
    "        plot_bgcolor=\"white\",\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edeb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is all you need for the liquidity graphs we built earlier\n",
    "pair_address = token_filter[0]  # Your pair\n",
    "df = load_pair_data_for_analysis(pair_address, START_BLOCK, END_BLOCK)\n",
    "\n",
    "# Then use the existing analysis function\n",
    "liquidity_df, concentration_metrics = analyze_pool_liquidity(\n",
    "    db_path=DB_PATH,\n",
    "    pair_address=pair_address,\n",
    "    block_start=START_BLOCK,\n",
    "    block_end=END_BLOCK,\n",
    "    show_plots=True,\n",
    "    show_validation=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "closedblind-ylehWVqW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
