{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad62ba3-3164-4c20-b7c6-585b1dbda86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import logging\n",
    "import tempfile\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pprint import pprint\n",
    "from web3.logs import STRICT, IGNORE, DISCARD, WARN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from decimal import Decimal\n",
    "from hexbytes import HexBytes\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "from datetime import datetime, timezone\n",
    "from web3 import Web3\n",
    "from web3.exceptions import Web3RPCError,TransactionNotFound, BlockNotFound\n",
    "from web3.providers.rpc.utils import (\n",
    "    ExceptionRetryConfiguration,\n",
    "    REQUEST_RETRY_ALLOWLIST,\n",
    ")\n",
    "from collections import defaultdict, OrderedDict\n",
    "pd.options.display.float_format = \"{:20,.4f}\".format\n",
    "# === CONFIGURATION ===\n",
    "ETHERSCAN_API_KEY_DICT = {\n",
    "    \"hearthquake\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_HEARTHQUAKE\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "    \"opensee\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_OPENSEE\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "    \"eco\": {\n",
    "        \"INFURA_URL\": os.getenv(\"INFURA_URL_ECO\"),\n",
    "        \"ETHERSCAN_API_KEY\": os.getenv(\"ETHERSCAN_API_KEY\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "INFURA_URL = ETHERSCAN_API_KEY_DICT[\"hearthquake\"][\"INFURA_URL\"]\n",
    "ETHERSCAN_API_KEY = ETHERSCAN_API_KEY_DICT[\"hearthquake\"][\"ETHERSCAN_API_KEY\"]\n",
    "UNISWAP_V1_CONTRACT = \"0xCBCdF9626bC03E24f779434178A73a0B4bad62eD\"\n",
    "UNISWAP_V1_CONTRACT = \"0xc0a47dFe034B400B47bDaD5FecDa2621de6c4d95\"\n",
    "OUTPUT_FILE = \"out/final_tx.jsonl\"\n",
    "STATE_FILE = \"out/final_scan_state.json\"\n",
    "TOKEN_NAME_FILE = \"out/token_name.json\"\n",
    "with open(TOKEN_NAME_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    GLOBAL_DICT_TOKEN_SYMBOL = json.load(f)\n",
    "\n",
    "# EVENTS = {\n",
    "#     \"0xc0a47dFe034B400B47bDaD5FecDa2621de6c4d95\":{\n",
    "\n",
    "#     },\n",
    "#     \"0x255e60c9d597dCAA66006A904eD36424F7B26286\": {\n",
    "#         \"AddLiquidity\": {},\n",
    "#         \"RemoveLiquidity\": {},\n",
    "#         \"Transfer\": {},\n",
    "#         \"EthPurchase\": {},\n",
    "#         \"TokenPurchase\": {},\n",
    "#         \"Approval\": {},\n",
    "#     },\n",
    "#     \"0xF173214C720f58E03e194085B1DB28B50aCDeeaD\": {\n",
    "#         \"AddLiquidity\": {},\n",
    "#         \"RemoveLiquidity\": {},\n",
    "#         \"Transfer\": {},\n",
    "#         \"EthPurchase\": {},\n",
    "#         \"TokenPurchase\": {},\n",
    "#         \"Approval\": {},\n",
    "#     },\n",
    "# }\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if not logger.handlers:  # avoid duplicate logs\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "w3 = Web3(\n",
    "        Web3.HTTPProvider(\n",
    "            endpoint_uri=INFURA_URL,\n",
    "            request_kwargs={\"timeout\": 30},  # adjust as needed\n",
    "            exception_retry_configuration=ExceptionRetryConfiguration(\n",
    "                errors=(ConnectionError, HTTPError, TimeoutError),\n",
    "                retries=5,\n",
    "                backoff_factor=1,\n",
    "                method_allowlist=REQUEST_RETRY_ALLOWLIST,\n",
    "            )\n",
    "        )\n",
    "    # Web3.HTTPProvider(\"http://127.0.0.1:8545\")\n",
    ")\n",
    "\n",
    "assert w3.is_connected(), \"Web3 provider connection failed\"\n",
    "w3.eth.get_block('latest').number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badcd644-61b7-4605-bf1b-27e15732d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Helper Function: Get ABI from Etherscan or Disk\n",
    "# --------------------\n",
    "def get_abi(contract_address: str, api_key: str) -> list:\n",
    "    \"\"\"\n",
    "    Retrieves the ABI for a given contract address.\n",
    "    Checks if the ABI is available in the local 'ABI' folder.\n",
    "    If not, it fetches the ABI from Etherscan using the provided API key,\n",
    "    then saves it to disk for future use.\n",
    "    \n",
    "    Parameters:\n",
    "        contract_address (str): The contract address (checksum not required here).\n",
    "        api_key (str): Your Etherscan API key.\n",
    "        \n",
    "    Returns:\n",
    "        list: The ABI loaded as a Python list.\n",
    "    \"\"\"\n",
    "    # Ensure the ABI folder exists.\n",
    "    abi_folder = \"ABI\"\n",
    "    if not os.path.exists(abi_folder):\n",
    "        os.makedirs(abi_folder)\n",
    "    \n",
    "    # Save ABI with filename based on contract address.\n",
    "    filename = os.path.join(abi_folder, f\"{contract_address}.json\")\n",
    "    \n",
    "    # If file exists, load and return the ABI.\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\") as file:\n",
    "            abi = json.load(file)\n",
    "    else:\n",
    "        try:\n",
    "            url = f\"https://api.etherscan.io/v2/api?chainid=1&module=contract&action=getabi&address={contract_address}&apikey={api_key}\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            if data[\"status\"] == \"1\":\n",
    "                # Parse the ABI and save it for later use.\n",
    "                abi = json.loads(data[\"result\"])\n",
    "                with open(filename, \"w\") as file:\n",
    "                    json.dump(abi, file)    \n",
    "        except Exception as e:\n",
    "            Exception(f\"Error fetching ABI for contract {contract_address}: {data['result']}\")\n",
    "    return abi\n",
    "\n",
    "# -----------------------\n",
    "# Helper: Convert event to dict\n",
    "# -----------------------\n",
    "def event_to_dict(event):\n",
    "    d = dict(event)\n",
    "    if \"args\" in d:\n",
    "        d[\"args\"] = dict(d[\"args\"])\n",
    "    if \"transactionHash\" in d:\n",
    "        d[\"transactionHash\"] = d[\"transactionHash\"].hex()\n",
    "    if \"blockHash\" in d:\n",
    "        d[\"blockHash\"] = d[\"blockHash\"].hex()\n",
    "    return d\n",
    "\n",
    "\n",
    "class Web3JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        # HexBytes → hex string\n",
    "        if isinstance(obj, HexBytes):\n",
    "            return obj.hex()\n",
    "        # Peel off any other web3-specific types here as needed...\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# ETHERSCAN VERSION\n",
    "# Used to find at which block 1 contract has been deployed\n",
    "# Might be useful later, put it in JSON in the end\n",
    "# -----------------------\n",
    "def get_contract_creation_block_etherscan(contract_address: str, etherscan_api_key: str) -> int:\n",
    "    \"\"\"\n",
    "    Retrieves the contract creation block from Etherscan.\n",
    "    Returns the block number as an integer.\n",
    "    \"\"\"\n",
    "    url = (f\"https://api.etherscan.io/api?module=contract&action=getcontractcreation\"\n",
    "           f\"&contractaddresses={contract_address}&apikey={etherscan_api_key}\")\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if data.get(\"status\") == \"1\":\n",
    "        results = data.get(\"result\", [])\n",
    "        if results and len(results) > 0:\n",
    "            return int(results[0][\"blockNumber\"])\n",
    "        else:\n",
    "            raise Exception(\"No contract creation data found.\")\n",
    "    else:\n",
    "        raise Exception(\"Error fetching creation block: \" + data.get(\"result\", \"Unknown error\"))\n",
    "\n",
    "# -----------------------\n",
    "# Used to find at which block 1 contract has been deployed\n",
    "# Might be useful later, put it in JSON in the end\n",
    "# -----------------------\n",
    "def get_contract_creation_block_custom(start_block=0, end_block=100000):\n",
    "\n",
    "    def get_contract_deployments(start_block, end_block, max_workers=8):\n",
    "        deployments = []\n",
    "\n",
    "        def process_block(block_number):\n",
    "            block = w3.eth.get_block(block_number, full_transactions=True)\n",
    "            block_deployments = []\n",
    "            for tx in block.transactions:\n",
    "                if tx.to is None:\n",
    "                    try:\n",
    "                        receipt = w3.eth.get_transaction_receipt(tx.hash)\n",
    "                        contract_address = receipt.contractAddress\n",
    "                        if contract_address:\n",
    "                            block_deployments.append(\n",
    "                                {\n",
    "                                    \"block_number\": block_number,\n",
    "                                    \"contract_address\": contract_address,\n",
    "                                }\n",
    "                            )\n",
    "                    except:\n",
    "                        print(tx.hash)\n",
    "            return block_deployments\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_block = {\n",
    "                executor.submit(process_block, bn): bn\n",
    "                for bn in range(start_block, end_block + 1)\n",
    "            }\n",
    "            for future in as_completed(future_to_block):\n",
    "                block_deployments = future.result()\n",
    "                deployments.extend(block_deployments)\n",
    "\n",
    "        return deployments\n",
    "\n",
    "    deployments = get_contract_deployments(start_block, end_block)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    with open(\"contract_deployments.json\", \"w\") as f:\n",
    "        json.dump(deployments, f, indent=4)\n",
    "\n",
    "# -- Step 2: Reconstruct an Event’s Signature --\n",
    "def get_event_signature(event_name: str, abi: list) -> str:\n",
    "    \"\"\"\n",
    "    Given an event name and an ABI, find the event definition and reconstruct its signature.\n",
    "    For example, for event Transfer(address,address,uint256) this returns its keccak256 hash.\n",
    "    \"\"\"\n",
    "    from eth_utils import keccak, encode_hex\n",
    "\n",
    "    for item in abi:\n",
    "        if item.get(\"type\") == \"event\" and item.get(\"name\") == event_name:\n",
    "            # Build the signature string: \"Transfer(address,address,uint256)\"\n",
    "            types = \",\".join([inp[\"type\"] for inp in item.get(\"inputs\", [])])\n",
    "            signature = f\"{event_name}({types})\"\n",
    "            return encode_hex(keccak(text=signature))\n",
    "    raise ValueError(f\"Event {event_name} not found in ABI.\")\n",
    "\n",
    "def block_to_utc(block_number):\n",
    "    \"\"\"\n",
    "    Convert a block number into its UTC timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        w3 (Web3): A Web3 instance\n",
    "        block_number (int): The block number\n",
    "\n",
    "    Returns:\n",
    "        datetime: The block timestamp in UTC\n",
    "    \"\"\"\n",
    "    block = w3.eth.get_block(block_number)\n",
    "    timestamp = block[\"timestamp\"]\n",
    "    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n",
    "\n",
    "def read_and_sort_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file, each line being a JSON object with a field `blockNumber`,\n",
    "    and returns a list of those objects sorted by blockNumber (ascending).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                # Handle bad JSON if needed, e.g., log or skip\n",
    "                print(line)\n",
    "                print(f\"Skipping bad JSON line: {e}\")\n",
    "                continue\n",
    "            # Optionally, you could check that 'blockNumber' exists, is int, etc.\n",
    "            if \"blockNumber\" not in obj:\n",
    "                print(f\"Skipping line with no blockNumber: {obj}\")\n",
    "                continue\n",
    "            data.append(obj)\n",
    "    # Now sort by blockNumber ascending\n",
    "    # If blockNumber in file is already int, fine; else convert\n",
    "    sorted_data = sorted(data, key=lambda o: int(o[\"blockNumber\"]))\n",
    "    return sorted_data\n",
    "\n",
    "def get_address_abi_contract(contract_address, etherscan_api_key=ETHERSCAN_API_KEY):\n",
    "    address = w3.to_checksum_address(contract_address)\n",
    "    contract_abi = get_abi(address, etherscan_api_key)\n",
    "    contract = w3.eth.contract(address=contract_address, abi=contract_abi)\n",
    "\n",
    "    return address, contract_abi, contract\n",
    "\n",
    "# Find the amount of token depending on the contract at the very specific block_number\n",
    "# but it use ETHERSCAN API (to go further: explorer the reconstruct from all the Transfer event but slow)\n",
    "# Not super useful for the moment\n",
    "def get_erc20_balance_at_block(user_address, token_address, block_number):\n",
    "    \"\"\"\n",
    "        Query ERC-20 balance of an address at a specific block.\n",
    "\n",
    "        user_address = \"0xe2dFC8F41DB4169A24e7B44095b9E92E20Ed57eD\"\n",
    "        token_address = \"0x514910771AF9Ca656af840dff83E8264EcF986CA\"\n",
    "        block_number = 23405236\n",
    "        balance = get_erc20_balance_at_block(user_address, token_address, block_number)\n",
    "\n",
    "        Parameters:\n",
    "            user_address: string, account to check\n",
    "            token_address: Web3 contract instance for the ERC-20 token\n",
    "            block_number: int, historical block\n",
    "\n",
    "        Returns:\n",
    "            int: token balance\n",
    "            None if contract is a proxy\n",
    "    \"\"\"\n",
    "    token_address, token_abi, token_contract = get_address_abi_contract(token_address)\n",
    "    user_address =  w3.to_checksum_address(user_address)\n",
    "    token_name =  None\n",
    "    token_symbol = None \n",
    "    try:\n",
    "        token_name = token_contract.functions.name().call()\n",
    "        token_symbol = token_contract.functions.symbol().call()\n",
    "    except Exception as e:\n",
    "        print(f\"Error {e}\")\n",
    "        print(f\"{token_address}\")\n",
    "        return None\n",
    "    balance = token_contract.functions.balanceOf(user_address).call(\n",
    "        block_identifier=block_number\n",
    "    )\n",
    "    print(\n",
    "        f\"Address {user_address} had {w3.from_wei(balance, \"ether\")} of {token_symbol} at block {block_number}\"\n",
    "    )\n",
    "    return balance\n",
    "\n",
    "\n",
    "def get_token_name_by_contract(token_address, TOKEN_NAME_FILE=TOKEN_NAME_FILE, proxy_address=None, global_cache=GLOBAL_DICT_TOKEN_SYMBOL):\n",
    "    \"\"\"\n",
    "    Returns the token name for `token_address`, using a local JSON cache.\n",
    "    If not in cache, will call get_token_name_by_contract (your ABI/Web3 function),\n",
    "    store the result (or None) in the cache file, and return it.\n",
    "    \"\"\"\n",
    "    # 1. Load cache\n",
    "    cache = global_cache\n",
    "    # if os.path.exists(TOKEN_NAME_FILE):\n",
    "    #     try:\n",
    "    #         with open(TOKEN_NAME_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    #             cache = json.load(f)\n",
    "    #     except Exception as e:\n",
    "    #         # If file is corrupted, proceed with empty cache\n",
    "    #         print(f\"Warning: cannot read token name cache: {e}\")\n",
    "\n",
    "    # 2. Check cache\n",
    "    if token_address in cache:\n",
    "        return cache[token_address]\n",
    "\n",
    "    # Not in cache → fetch from contract\n",
    "    name = None\n",
    "    symbol = None\n",
    "    address = None\n",
    "    try:\n",
    "        if proxy_address:\n",
    "            proxy_address, proxy_abi, proxy_contract = get_address_abi_contract(\n",
    "                proxy_address\n",
    "            )\n",
    "            token_address = proxy_contract.functions.getToken(token_address).call()\n",
    "        token_address, token_abi, token_contract = get_address_abi_contract(token_address)\n",
    "        # call name\n",
    "        name_raw = token_contract.functions.name().call()\n",
    "        symbol_raw = token_contract.functions.symbol().call()\n",
    "        address = token_contract.address\n",
    "        # Convert raw to str if needed\n",
    "        name = str(name_raw)\n",
    "        if isinstance(name_raw, (bytes, bytearray)):\n",
    "            name = name_raw.decode(\"utf-8\", errors=\"ignore\").rstrip(\"\\x00\")\n",
    "        symbol = str(symbol_raw)\n",
    "        if isinstance(symbol_raw, (bytes, bytearray)):\n",
    "            symbol = symbol_raw.decode(\"utf-8\", errors=\"ignore\").rstrip(\"\\x00\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching token name/symbol for {address}: {e}\")\n",
    "        if token_address:\n",
    "            cache[token_address] = {\n",
    "                \"name\": None,\n",
    "                \"symbol\": None,\n",
    "                \"address\": None,\n",
    "            }\n",
    "        try:\n",
    "            dirn = os.path.dirname(TOKEN_NAME_FILE) or \".\"\n",
    "            fd, tmp = tempfile.mkstemp(dir=dirn, text=True)\n",
    "            with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(cache, f, indent=2, ensure_ascii=False)\n",
    "            os.replace(tmp, TOKEN_NAME_FILE)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to save token cache: {e}\")\n",
    "        return {\n",
    "            \"name\": None,\n",
    "            \"symbol\": None,\n",
    "            \"address\": None,\n",
    "        }\n",
    "\n",
    "    # Update cache\n",
    "    cache[address] = {\n",
    "        \"name\": name,\n",
    "        \"symbol\": symbol,\n",
    "        \"address\": address,\n",
    "    }\n",
    "\n",
    "    # Write back atomically (overwrite)\n",
    "    try:\n",
    "        dirn = os.path.dirname(TOKEN_NAME_FILE) or \".\"\n",
    "        fd, tmp = tempfile.mkstemp(dir=dirn, text=True)\n",
    "        with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cache, f, indent=2, ensure_ascii=False)\n",
    "        os.replace(tmp, TOKEN_NAME_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: failed to save token cache: {e}\")\n",
    "\n",
    "    return cache[address]\n",
    "\n",
    "\n",
    "def decode_topics(log):\n",
    "    _, abi, contract = get_address_abi_contract(log[\"address\"])\n",
    "    # Try matching this log against the ABI events\n",
    "    for item in abi:\n",
    "        if item.get(\"type\") == \"event\":\n",
    "            event_signature = (\n",
    "                f'{item[\"name\"]}({\",\".join(i[\"type\"] for i in item[\"inputs\"])})'\n",
    "            )\n",
    "            event_hash = w3.keccak(text=event_signature).hex()\n",
    "\n",
    "            if log[\"topics\"][0].hex() == event_hash:\n",
    "                # Found matching event\n",
    "                decoded = contract.events[item[\"name\"]]().process_log(log)\n",
    "                return {\n",
    "                    \"event\": item[\"name\"],\n",
    "                    \"args\": dict(decoded[\"args\"]),\n",
    "                }\n",
    "\n",
    "    return {}  # no matching event in ABI\n",
    "\n",
    "\n",
    "def release_list(a):\n",
    "    del a[:]\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe59f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_jsonl(OUTPUT_FILE, txs):\n",
    "    with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        for tx in txs:\n",
    "            f.write(json.dumps(tx, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_state(STATE_FILE):\n",
    "    if not os.path.exists(STATE_FILE):\n",
    "        return []\n",
    "\n",
    "    with open(STATE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f, object_pairs_hook=OrderedDict)\n",
    "\n",
    "    return [tuple(pair) for pair in data]\n",
    "\n",
    "def save_state(interval, STATE_FILE):\n",
    "    data = sorted([[l, r] for (l, r) in interval])\n",
    "    tmp = STATE_FILE + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f)\n",
    "    os.replace(tmp, STATE_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2355af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_BLOCK = 0\n",
    "\n",
    "# END_BLOCK = 'latest'\n",
    "END_BLOCK = 10000000\n",
    "CHUNK_SIZE = 10000\n",
    "\n",
    "token_filter = [\n",
    "    \"0x2C4BD064B998838076FA341A83D007FC2FA50957\",\n",
    "    \"0x255E60C9D597DCAA66006A904ED36424F7B26286\",\n",
    "    \"0xE8E45431B93215566BA923a7E611B7342EA954DF\",\n",
    "    \"0xF173214C720F58E03E194085B1DB28B50ACDEEAD\",\n",
    "    \"0xC6581CE3A005E2801C1E0903281BBD318EC5B5C2\",\n",
    "    \"0x494D82667C3ED3AC859CCA94B1BE65B0540EE3BB\",\n",
    "    \"0x077D52B047735976DFDA76FEF74D4D988AC25196\",\n",
    "    \"0xC4A1C45D5546029FD57128483AE65B56124BFA6A\",\n",
    "    \"0x7DC095A5CF7D6208CC680FA9866F80A53911041A\",\n",
    "    \"0x2135D193BF81ABBEAD93906166F2BE32B2492C04\",\n",
    "    \"0x4D2F5CFBA55AE412221182D8475BC85799A5644B\",\n",
    "    \"0x87D80DBD37E551F58680B4217B23AF6A752DA83F\",\n",
    "    \"0x060A0D4539623B6AA28D9FC39B9D6622AD495F41\",\n",
    "    \"0x6B4540F5EE32DDD5616C792F713435E6EE4F24AB\",\n",
    "    \"0xB99A23B1A4585FC56D0EC3B76528C27CAD427473\",\n",
    "    \"0x04045481B044534ED3CB1E24254B471CFADDEB3D\",\n",
    "    \"0xC0E77CDD039A3F731AE0F5C6C9F4A91D4BC28880\",\n",
    "]\n",
    "token_filter = [Web3.to_checksum_address(k) for k in token_filter]\n",
    "FULL_EVENT_BY_CONTRACTS = json.load(open(r\"real/FULL_EVENT_BY_CONTRACTS.json\"))\n",
    "FULL_EVENT_BY_CONTRACTS = {\n",
    "    Web3.to_checksum_address(k): v for k, v in FULL_EVENT_BY_CONTRACTS.items()\n",
    "}\n",
    "PARTIAL_EVENT_BY_CONTRACTS = {\n",
    "    k: FULL_EVENT_BY_CONTRACTS[k] for k in token_filter if k in FULL_EVENT_BY_CONTRACTS\n",
    "}\n",
    "\n",
    "# We start to crawl\n",
    "EVENTS = FULL_EVENT_BY_CONTRACTS\n",
    "END_BLOCK = w3.eth.get_block(END_BLOCK).number\n",
    "validated_interval = load_state(STATE_FILE)\n",
    "delay = 3\n",
    "try:\n",
    "    logging.info(f\"Scanning blocks {START_BLOCK} to {END_BLOCK}\")\n",
    "    l_current_block = START_BLOCK\n",
    "    r_current_block = min(l_current_block + CHUNK_SIZE, END_BLOCK)\n",
    "    if validated_interval:\n",
    "        l_current_block, r_current_block = validated_interval.pop()\n",
    "        if r_current_block == END_BLOCK:\n",
    "            l_current_block = END_BLOCK\n",
    "    while l_current_block < END_BLOCK:\n",
    "        txs = []\n",
    "        if (l_current_block, r_current_block) not in validated_interval:\n",
    "            logging.info(f\"Processing blocks [{l_current_block}, {r_current_block}]\")\n",
    "            try:\n",
    "                params = {\n",
    "                    \"fromBlock\": l_current_block,\n",
    "                    \"toBlock\": r_current_block,\n",
    "                    \"address\": list(EVENTS.keys()),\n",
    "                }\n",
    "                logs = w3.eth.get_logs(params)\n",
    "                for log in logs:\n",
    "                    transaction = {\n",
    "                        \"transactionHash\": w3.to_hex(log[\"transactionHash\"]),\n",
    "                        \"blockNumber\": log[\"blockNumber\"],\n",
    "                        \"address\": log[\"address\"],\n",
    "                        \"data\": w3.to_hex(log[\"data\"]),\n",
    "                    }\n",
    "                    topics = decode_topics(log)\n",
    "                    transaction.update(topics)\n",
    "                    txs.append(transaction)\n",
    "                append_jsonl(OUTPUT_FILE, txs)\n",
    "                validated_interval.append((l_current_block, r_current_block))\n",
    "                save_state(validated_interval, STATE_FILE)\n",
    "                l_current_block = r_current_block + 1\n",
    "                r_current_block = min(l_current_block + CHUNK_SIZE, END_BLOCK)\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if e.response.status_code == 429:\n",
    "                    jitter = random.uniform(3, delay)\n",
    "                    total_delay = (delay + jitter) / 10\n",
    "                    logging.warning(\n",
    "                        f\"Delay: {total_delay}: {e}\"\n",
    "                    )\n",
    "                    time.sleep(total_delay)\n",
    "                    delay += 2  # exponential backoff\n",
    "                    continue\n",
    "                elif e.response.status_code == 402:\n",
    "                    logging.critical(\n",
    "                        f\"Payment Required Code {e.response.status_code}, Stopping Blocks [{l_current_block}, {r_current_block}]: {e}\"\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    logging.error(f\"HTTP error occurred: {e}\")\n",
    "                    break\n",
    "            except Web3RPCError as e:\n",
    "                logging.warning(f\"{e}\")\n",
    "                r_current_block = (l_current_block + r_current_block) // 2\n",
    "            except Exception as e:\n",
    "                logging.error(\n",
    "                    f\"Exception when processing interval [{l_current_block}, {r_current_block}]: {e}\"\n",
    "                )\n",
    "                logging.error(\n",
    "                    print(traceback.format_exc())\n",
    "                )\n",
    "                break\n",
    "except KeyboardInterrupt:\n",
    "    validated_interval.pop()\n",
    "    save_state(validated_interval, STATE_FILE)\n",
    "    logging.info(f\"Interrupted by user — exiting loop.\")\n",
    "except Exception as e:\n",
    "    logging.fatal(f\"Unexpected fatal error in main: {e}\")\n",
    "logging.info(f\"Finished parsing the last block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_transactions = read_and_sort_jsonl(OUTPUT_FILE)\n",
    "DISTINCT_PROVIDER = set()\n",
    "D_BLOCK_TOTAL_LIQUIDITY_BY_CONTRACT_BY_BLOCK = defaultdict(\n",
    "    lambda: defaultdict(lambda: Decimal(\"0\"))\n",
    ")\n",
    "L_LIQUIDITY_BOOK = []\n",
    "L_UNI_LIQUIDITY = []\n",
    "FAILED_EVENTS = []\n",
    "\n",
    "def analyze_transaction(transaction):\n",
    "    block = transaction[\"blockNumber\"]\n",
    "    event = transaction[\"event\"]\n",
    "    event_args = transaction[\"args\"]\n",
    "    address = transaction[\"address\"]\n",
    "    tx_hash = transaction[\"transactionHash\"]\n",
    "    #if address not in token_filter:\n",
    "    #    return\n",
    "    # Transaction data in case we need to investigate  further (but we could directly store them in jsonl from the sniffer)\n",
    "    # transaction_information = w3.eth.get_transaction(transaction[\"txHash\"])\n",
    "    # d_transaction_information = event_to_dict(transaction_information)\n",
    "    # In case we need ABI and retrieve token information\n",
    "    # token_address, contract_abi, contract = get_address_abi_contract(\n",
    "    #    transaction[\"contract\"]\n",
    "    # )\n",
    "\n",
    "    # Event part\n",
    "    if event == \"AddLiquidity\":\n",
    "        provider = event_args[\"provider\"]\n",
    "        token_amount = event_args[\"token_amount\"]\n",
    "        eth_amount = event_args[\"eth_amount\"]\n",
    "        # Provider are liquidity participant, we track all of them to count\n",
    "        DISTINCT_PROVIDER.add(provider)\n",
    "        L_LIQUIDITY_BOOK.append(\n",
    "            {\n",
    "                \"block\": block,\n",
    "                \"address\": address,\n",
    "                \"event\": event,\n",
    "                \"provider\": provider,\n",
    "                \"token_amount\": w3.from_wei(token_amount, \"ether\"),\n",
    "                \"eth_amount\": w3.from_wei(eth_amount, \"ether\"),\n",
    "            }\n",
    "        )\n",
    "    elif event == \"RemoveLiquidity\":\n",
    "        provider = event_args[\"provider\"]\n",
    "        token_amount = event_args[\"token_amount\"]\n",
    "        eth_amount = event_args[\"eth_amount\"]\n",
    "        L_LIQUIDITY_BOOK.append(\n",
    "            {\n",
    "                \"block\": block,\n",
    "                \"address\": address,\n",
    "                \"event\": event,\n",
    "                \"provider\": provider,\n",
    "                \"token_amount\": -w3.from_wei(token_amount, \"ether\"),\n",
    "                \"eth_amount\": -w3.from_wei(eth_amount, \"ether\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    elif event == \"Transfer\":\n",
    "        _from = event_args[\"_from\"]\n",
    "        _to = event_args[\"_to\"]\n",
    "        _value = event_args[\"_value\"]  # UniswapV1-TOKEN-Liquidity-debt\n",
    "        if _from == w3.to_checksum_address(\n",
    "            \"0x0000000000000000000000000000000000000000\"\n",
    "        ):\n",
    "            # We Mint Liquidity\n",
    "            D_BLOCK_TOTAL_LIQUIDITY_BY_CONTRACT_BY_BLOCK[block][address] += w3.from_wei(\n",
    "                _value, \"ether\"\n",
    "            )\n",
    "            L_UNI_LIQUIDITY.append(\n",
    "                {\n",
    "                    \"block\": block,\n",
    "                    \"address\": address,\n",
    "                    \"event\": event,\n",
    "                    \"provider\": _to,\n",
    "                    \"value\": w3.from_wei(_value, \"ether\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if _to == w3.to_checksum_address(\n",
    "            w3.to_checksum_address(\"0x0000000000000000000000000000000000000000\")\n",
    "        ):\n",
    "            # We Burn Liquidity\n",
    "            D_BLOCK_TOTAL_LIQUIDITY_BY_CONTRACT_BY_BLOCK[block][\n",
    "                address\n",
    "            ] -= w3.from_wei(_value, \"ether\")\n",
    "            L_UNI_LIQUIDITY.append(\n",
    "                {\n",
    "                    \"block\": block,\n",
    "                    \"address\": address,\n",
    "                    \"event\": event,\n",
    "                    \"provider\": _from,\n",
    "                    \"value\": -w3.from_wei(_value, \"ether\"),\n",
    "                }\n",
    "            )\n",
    "    # Pursechase will be used to compute SWAP (Volume, Fees) Later\n",
    "    elif event == \"TokenPurchase\":\n",
    "        buyer = event_args[\"buyer\"]\n",
    "        eth_sold = event_args[\"eth_sold\"]\n",
    "        tokens_bought = event_args[\"tokens_bought\"]\n",
    "    elif event == \"EthPurchase\":\n",
    "        buyer = event_args[\"buyer\"]\n",
    "        tokens_sold = event_args[\"tokens_sold\"]\n",
    "        eth_bought = event_args[\"eth_bought\"]\n",
    "    # Not that useful for the moment\n",
    "    elif event == \"Approval\":\n",
    "        _owner = event_args[\"_owner\"]\n",
    "        _spender = event_args[\"_spender\"]\n",
    "        _value = event_args[\"_value\"]\n",
    "    else:\n",
    "        FAILED_EVENTS.append(event)\n",
    "        # print(f\"Event not Known: {event}\")\n",
    "\n",
    "\n",
    "result = []\n",
    "print(len(sorted_transactions))\n",
    "for tx in sorted_transactions:\n",
    "    analyze_transaction(tx)\n",
    "\n",
    "print(len(DISTINCT_PROVIDER))\n",
    "print(len(FAILED_EVENTS))\n",
    "release_list(sorted_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummy_fast(\n",
    "    n_blocks=100_000,\n",
    "    n_providers=200,\n",
    "    n_addresses=100,\n",
    "    block_max=24_000_000,\n",
    "    max_events_per_block=5,\n",
    "    max_mint_amount=1000,\n",
    "    seed=None,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    providers = np.array([f\"prov_{i}\" for i in range(n_providers)], dtype=object)\n",
    "    addresses = np.array([f\"add_{j}\" for j in range(n_addresses)], dtype=object)\n",
    "\n",
    "    blocks = rng.integers(0, block_max + 1, size=n_blocks)\n",
    "    blocks = np.unique(blocks)\n",
    "    blocks.sort()\n",
    "\n",
    "    # build arrays for all events upfront\n",
    "    event_list = []\n",
    "    for blk in blocks:\n",
    "        k = min(max_events_per_block, n_providers)\n",
    "        chosen = rng.choice(n_providers, size=k, replace=False)\n",
    "        for pi in chosen:\n",
    "            addr_idx = rng.choice(n_addresses)\n",
    "            sign = rng.choice([-1, 1])\n",
    "            mag = rng.uniform(0, max_mint_amount)\n",
    "            event_list.append((blk, pi, addr_idx, sign * mag))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    arr = np.array(\n",
    "        event_list,\n",
    "        dtype=[\n",
    "            (\"block\", np.int64),\n",
    "            (\"prov_i\", np.int64),\n",
    "            (\"addr_i\", np.int64),\n",
    "            (\"value\", np.float64),\n",
    "        ],\n",
    "    )\n",
    "    # sort by block\n",
    "    arr.sort(order=\"block\")\n",
    "\n",
    "    # initialize cumulative array (provider-level)\n",
    "    cum = np.zeros(n_providers, dtype=np.float64)\n",
    "    cum_values = np.zeros(len(arr), dtype=np.float64)\n",
    "\n",
    "    # Numba function to enforce non-negative cumulative:\n",
    "    @njit\n",
    "    def clamp_cumulative(arr_block, arr_prov, arr_val, cum_out, cum):\n",
    "        n = len(arr_val)\n",
    "        for i in range(n):\n",
    "            pi = arr_prov[i]\n",
    "            v = arr_val[i]\n",
    "            prev = cum[pi]\n",
    "            new = prev + v\n",
    "            if new < 0:\n",
    "                v = -prev\n",
    "                new = 0.0\n",
    "            cum_out[i] = new\n",
    "            cum[pi] = new\n",
    "\n",
    "    clamp_cumulative(arr[\"block\"], arr[\"prov_i\"], arr[\"value\"], cum_values, cum)\n",
    "\n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"block\": arr[\"block\"],\n",
    "            \"provider\": providers[arr[\"prov_i\"]],\n",
    "            \"address\": addresses[arr[\"addr_i\"]],\n",
    "            \"value\": arr[\"value\"],\n",
    "            \"cum_value_provider\": cum_values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute share per block\n",
    "    df_wide = (\n",
    "        df.pivot(index=\"block\", columns=\"provider\", values=\"cum_value_provider\")\n",
    "        .ffill()\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "    pool = df_wide.clip(lower=0).sum(axis=1)\n",
    "    block_to_pool = pool.to_dict()\n",
    "    df[\"share\"] = df.apply(\n",
    "        lambda r: (\n",
    "            (r[\"cum_value_provider\"] / block_to_pool.get(r[\"block\"], 0.0) * 100.0)\n",
    "            if block_to_pool.get(r[\"block\"], 0.0) > 0\n",
    "            else 0.0\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# df = make_dummy_fast(\n",
    "#     n_blocks=100_000,\n",
    "#     n_providers=250,\n",
    "#     n_addresses=10,\n",
    "#     block_max=100_100,\n",
    "#     max_events_per_block=10,\n",
    "#     max_mint_amount=1000,\n",
    "#     seed=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef438cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(L_LIQUIDITY_BOOK) + len(L_UNI_LIQUIDITY))\n",
    "df_liquidity_book = pd.DataFrame(L_LIQUIDITY_BOOK)\n",
    "df_uni_liquidity = pd.DataFrame(L_UNI_LIQUIDITY)\n",
    "df = pd.concat([df_liquidity_book, df_uni_liquidity], ignore_index=False)\n",
    "numeric_cols = [\"token_amount\", \"eth_amount\", \"value\"]\n",
    "df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "df[\"event\"] = df[\"event\"].astype(\"category\")\n",
    "df[\"provider\"] = df[\"provider\"].apply(Web3.to_checksum_address)\n",
    "df[\"address\"] = df[\"address\"].apply(Web3.to_checksum_address)\n",
    "unique = df[\"address\"].unique()\n",
    "# addr_to_symbol = {\n",
    "#     addr: GLOBAL_DICT_TOKEN_SYMBOL[addr][\"symbol\"]\n",
    "#     for addr in unique\n",
    "# }\n",
    "addr_to_symbol = {\n",
    "    addr: get_token_name_by_contract(token_address=addr, proxy_address=UNISWAP_V1_CONTRACT)[\"symbol\"]\n",
    "    for addr in unique\n",
    "}\n",
    "df[\"symbol\"] = df[\"address\"].map(addr_to_symbol)\n",
    "df[\"symbol\"] = df[\"symbol\"].astype(\"category\")\n",
    "df[\"block\"] = df[\"block\"].astype(np.int32)\n",
    "# df[\"value\"] = df[\"value\"].astype(float)\n",
    "df[\"value\"] = pd.to_numeric(\n",
    "    df[\"value\"], downcast=\"float\"\n",
    ")  # Downcast float to save memory\n",
    "\n",
    "# df[\"token_amount\"] = df[\"token_amount\"].astype(float)\n",
    "# df[\"eth_amount\"] = df[\"eth_amount\"].astype(float)\n",
    "\n",
    "# df[\"token_amount\"] = df[\"token_amount\"].astype(float)\n",
    "# df[\"eth_amount\"] = df[\"eth_amount\"].astype(float)\n",
    "df = df.drop(columns=[\"token_amount\", \"eth_amount\"])\n",
    "\n",
    "df\n",
    "#df.to_pickle(\"out/V1/df_before_graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"out/V1/df_before_graph\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ca0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by block and address, and aggregate into dicts of provider → value\n",
    "# We use a custom aggregator:\n",
    "def prov_val_dict(subdf):\n",
    "    # subdf is the slice for a given (block, address)\n",
    "    # return dict mapping provider → value\n",
    "    return dict(zip(subdf[\"provider\"], subdf[\"value\"]))\n",
    "\n",
    "g = (\n",
    "    df.groupby([\"block\", \"address\"])[[\"provider\", \"value\"]]\n",
    "    .apply(prov_val_dict, include_groups=False)\n",
    "    .reset_index(name=\"prov_value_dict\")\n",
    ")\n",
    "\n",
    "df_pivot = g.pivot(index=\"block\", columns=\"address\", values=\"prov_value_dict\")\n",
    "df_pivot = df_pivot.fillna({addr: {} for addr in df[\"address\"].unique()})\n",
    "df_pivot = df_pivot.reset_index()\n",
    "\n",
    "def make_empty_if_nan(cell):\n",
    "    if cell is pd.NA or (isinstance(cell, float) and np.isnan(cell)):\n",
    "        return {}\n",
    "    return cell\n",
    "\n",
    "\n",
    "# df_pivot = df_pivot.applymap(make_empty_if_nan)\n",
    "# df_pivot = df_pivot.map(make_empty_if_nan)\n",
    "\n",
    "del df_pivot\n",
    "df_pivot = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d96a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO ME APPROPRIATED\n",
    "# ---------------------------\n",
    "# ASSUMPTION:\n",
    "# df_pivot has columns: 'block', 'add_0', 'add_1', ... each add_* cell is a dict (provider->value)\n",
    "# Example head: df_pivot[['block','add_0','add_1']].head()\n",
    "# ---------------------------\n",
    "\n",
    "df = df_pivot.copy()  # keep original safe\n",
    "addr_cols = [c for c in df.columns if c.startswith(\"add_\")]  # adjust pattern if needed\n",
    "\n",
    "\n",
    "# safe cell-sum (handles dict, empty dict, NaN)\n",
    "def sum_dict_cell(x):\n",
    "    if isinstance(x, dict):\n",
    "        # sum provider values inside the cell\n",
    "        return float(sum(x.values()))\n",
    "    # handle missing / NaN cell\n",
    "    if pd.isna(x):\n",
    "        return 0.0\n",
    "    # fallback (if cell already numeric)\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# 1) sort by block to ensure temporal order\n",
    "df = df.sort_values(\"block\").reset_index(drop=True)\n",
    "\n",
    "# 2) compute per-address net change (delta) at each block\n",
    "# use Series.map inside apply to avoid applymap deprecation\n",
    "df_delta = df[addr_cols].apply(lambda col: col.map(sum_dict_cell))\n",
    "# rename columns to make intent clear\n",
    "df_delta = df_delta.rename(columns={c: f\"{c}_delta\" for c in df_delta.columns})\n",
    "\n",
    "# attach block back (index aligned)\n",
    "df_delta.insert(0, \"block\", df[\"block\"].values)\n",
    "\n",
    "# 3) compute cumulative liquidity per address (cumsum across rows in block order)\n",
    "# set index to block for convenience\n",
    "df_delta_idx = df_delta.set_index(\"block\")\n",
    "df_cum = (\n",
    "    df_delta_idx.cumsum()\n",
    ")  # cumsum per column (per-address cumulative liquidity). See pandas cumsum. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "# 4) merge the numeric results back into the original table (optional)\n",
    "# - for each address add two columns: {add_X}_delta and {add_X}_liquidity\n",
    "for addr in addr_cols:\n",
    "    delta_col = f\"{addr}_delta\"\n",
    "    liq_col = f\"{addr}_liquidity\"\n",
    "    df[delta_col] = df_delta[delta_col].values\n",
    "    df[liq_col] = df_cum[delta_col].values\n",
    "\n",
    "# Now `df` has, per row (block), for each address:\n",
    "#   - add_X_delta      : net change at that block (sum of providers' values in that address cell)\n",
    "#   - add_X_liquidity  : cumulative liquidity for that address up to that block\n",
    "\n",
    "# ---------------------------\n",
    "# OPTIONAL: Reindex to full block range and forward-fill (WARNING: can be enormous)\n",
    "# If you truly need a row for every integer block between min->max:\n",
    "#  - this will create (max_block - min_block + 1) rows and can be memory/time-expensive.\n",
    "#  - prefer using the \"query last event\" approach below if range is large.\n",
    "# ---------------------------\n",
    "min_b, max_b = int(df[\"block\"].min()), int(df[\"block\"].max())\n",
    "\n",
    "# build full block index (only if you're sure it's acceptable memory-wise)\n",
    "# blocks_full = np.arange(min_b, max_b + 1, dtype=int)\n",
    "# df_cum_full = df_cum.reindex(blocks_full).ffill().fillna(0.0)   # reindex + forward-fill. See reindex/ffill docs. :contentReference[oaicite:2]{index=2}\n",
    "# df_cum_full.index.name = \"block\"\n",
    "\n",
    "# ---------------------------\n",
    "# FAST & memory-friendly query: \"what are per-address liquidities at arbitrary block B?\"\n",
    "# without reindexing entire range:\n",
    "# ---------------------------\n",
    "blocks_sorted = df[\"block\"].values  # sorted unique event blocks\n",
    "\n",
    "\n",
    "def liquidity_at_block(B):\n",
    "    \"\"\"\n",
    "    Return a dict {addr_col: liquidity_value} representing cumulative liquidity\n",
    "    at block B (i.e. last event <= B). If B < first event, returns zeros.\n",
    "    \"\"\"\n",
    "    i = np.searchsorted(blocks_sorted, B, side=\"right\") - 1\n",
    "    if i < 0:\n",
    "        # before first event\n",
    "        return {addr: 0.0 for addr in addr_cols}\n",
    "    row = df.iloc[i]\n",
    "    return {addr: float(row[f\"{addr}_liquidity\"]) for addr in addr_cols}\n",
    "\n",
    "\n",
    "# Example:\n",
    "# liquidity_at_block(1000)  -> gives per-address liquidity snapshot at block 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ac509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ST GRAPH, evolution of the UNISWAP v1 (UNI-V1) amount of token issued/burned (GLOBAL TOTAL over block)\n",
    "# Important to compare the size of every pool but we need to link \"value\" to either $ or something relevant for comparison\n",
    "# NEED: df\n",
    "totals = (\n",
    "    df.groupby([\"block\", \"address\"], as_index=False)[\"value\"]\n",
    "    .sum()\n",
    "    .sort_values([\"address\", \"block\"])\n",
    ")\n",
    "totals[\"cum_value\"] = totals.groupby(\"address\")[\"value\"].cumsum()\n",
    "\n",
    "# # 2) fill missing blocks only inside each address' span (min..max), then cumulate\n",
    "# totals = totals.groupby(\"address\", group_keys=False).apply(\n",
    "#     lambda g: (\n",
    "#         g.set_index(\"block\")\n",
    "#         .reindex(range(g[\"block\"].min(), g[\"block\"].max() + 1), fill_value=0)\n",
    "#         .rename_axis(\"block\")\n",
    "#         .reset_index()\n",
    "#         .assign(address=g.name)\n",
    "#     )\n",
    "# )\n",
    "# totals = (\n",
    "#     totals[[\"block\", \"address\", \"value\"]]\n",
    "#     .sort_values([\"address\", \"block\"])\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "pools_of_interest = [\n",
    "    w3.to_checksum_address(\"0x006B6E89EE1531CFE5B6D32DA0D80CC30506A339\"),\n",
    "    w3.to_checksum_address(\"0x010E2558EAB0639EDADC9F83C81CC87DF66F8029\"),\n",
    "    w3.to_checksum_address(\"0x01A700DC924D837740B2CF5EA8C9FC46A5A76A3A\"),\n",
    "]\n",
    "\n",
    "# pools_of_interest = [\"add_1\",\"add_2\",\"add_3\"]\n",
    "cum_long_sub = totals[totals[\"address\"].isin(pools_of_interest)]\n",
    "\n",
    "fig = px.area(\n",
    "    cum_long_sub,\n",
    "    x=\"block\",\n",
    "    y=\"cum_value\",\n",
    "    color=\"address\",\n",
    "    line_group=\"address\",\n",
    "    title=\"Cumulative liquidity evolution per pool\",\n",
    "    labels={\"cum_value\": \"Cumulative liquidity\", \"address\": \"Pool address\"},\n",
    ")\n",
    "\n",
    "# Optionally, you can also do px.line instead of px.area if you prefer lines without fill\n",
    "fig = px.line(cum_long_sub, x=\"block\", y=\"cum_value\", color=\"address\",\n",
    "              title=\"Cumulative liquidity per pool\")\n",
    "# You can also make it not stacked (i.e. overlayed) by doing:\n",
    "# fig = px.area(\n",
    "#     cum_long_sub,\n",
    "#     x=\"block\",\n",
    "#     y=\"cum_value\",\n",
    "#     color=\"address\",\n",
    "#     line_group=\"address\",\n",
    "#     facet_col=None,\n",
    "#     # maybe set `groupnorm=None` or other arguments\n",
    "# )\n",
    "\n",
    "fig.update_layout(legend_title=\"Pool address\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016dffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_address = w3.to_checksum_address(\"0x006B6e89eE1531cfE5b6d32da0d80CC30506A339\")\n",
    "\n",
    "df_one_addr = df[(df[\"address\"] == chosen_address) & (df[\"event\"] == 'Transfer')].copy()\n",
    "df_one_addr = df_one_addr.sort_values([\"block\"])\n",
    "\n",
    "\n",
    "def forward_backward_fill_blocks(df, step=100):\n",
    "    \"\"\"\n",
    "    For each row, create blocks from (block-step) to (block+step).\n",
    "    Forward fill after the event, backward fill before from latest data or 0.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe with columns: block, address, event, provider, value, symbol\n",
    "    step : int\n",
    "        Number of blocks before and after each transfer event\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        DataFrame with forward/backward filled values\n",
    "    \"\"\"\n",
    "\n",
    "    # Create offset array once: [-step, -step+1, ..., 0, ..., step-1, step]\n",
    "    offsets = np.arange(-step, step + 1)\n",
    "    n_repeats = len(offsets)  # 2*step + 1\n",
    "\n",
    "    # Repeat each row n_repeats times\n",
    "    expanded = df.loc[df.index.repeat(n_repeats)].reset_index(drop=True).copy()\n",
    "\n",
    "    # Create offset column using modulo indexing - guaranteed to match length\n",
    "    expanded[\"offset\"] = offsets[np.arange(len(expanded)) % n_repeats]\n",
    "\n",
    "    # Calculate actual block numbers\n",
    "    expanded[\"block\"] = expanded[\"block\"] + expanded[\"offset\"]\n",
    "\n",
    "    # For backward fill (offset < 0), set value to NaN\n",
    "    expanded.loc[expanded[\"offset\"] < 0, \"value\"] = np.nan\n",
    "\n",
    "    # Sort by block for fill operations\n",
    "    expanded = expanded.sort_values(\"block\").reset_index(drop=True)\n",
    "\n",
    "    # Forward fill then fill remaining with 0\n",
    "    expanded[\"value\"] = expanded[\"value\"].ffill().fillna(0)\n",
    "\n",
    "    # Drop helper column\n",
    "    result = expanded.drop(columns=[\"offset\"])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# smoothed = forward_backward_fill_blocks(df, step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed = forward_backward_fill_blocks(df_one_addr, step=5)\n",
    "smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e64d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = \"add_1\"\n",
    "\n",
    "# filtered_df = detailed.xs(token, level=\"address\", drop_level=True)\n",
    "\n",
    "# # Reset index to have columns\n",
    "# df_d = filtered_df.reset_index()  # columns: block, address, provider, value\n",
    "\n",
    "# # Sort by pool address, then provider, then block\n",
    "# df_d = df_d.sort_values([\"block\"])\n",
    "\n",
    "\n",
    "# # Compute cumulative sum per (address, provider)\n",
    "# df_d[\"cum_value_provider\"] = df_d.groupby([\"provider\"])[\"value\"].cumsum()\n",
    "# df_d[\"total_liquidity\"] = df_d[\"value\"].cumsum()\n",
    "# df_d[\"share\"] = (df_d[\"cum_value_provider\"] / df_d[\"total_liquidity\"]) * 100\n",
    "# df_d[\"cum_value_provider\"] = df_d[\"cum_value_provider\"].astype(float)\n",
    "# df_d[\"total_liquidity\"] = df_d[\"total_liquidity\"].astype(float)\n",
    "# df_d[\"share\"] = df_d[\"share\"].astype(float).fillna(0)\n",
    "# # with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "# #     display(\n",
    "# #         df_d[\n",
    "# #             [\n",
    "# #                 \"block\",\n",
    "# #                 \"provider\",\n",
    "# #                 \"value\",\n",
    "# #                 \"cum_value_provider\",\n",
    "# #                 \"share\",\n",
    "# #                 #\"total_liquidity\",\n",
    "# #             ]\n",
    "# #         ]\n",
    "# #     )\n",
    "def forward_backward_fill_blocks(df, step=100):\n",
    "    \"\"\"\n",
    "    For each row, create blocks from (block-step) to (block+step).\n",
    "    Forward fill after the event, backward fill before from latest data or 0.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe with columns: block, address, event, provider, value, symbol\n",
    "    step : int\n",
    "        Number of blocks before and after each transfer event\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        DataFrame with forward/backward filled values\n",
    "    \"\"\"\n",
    "\n",
    "    # Create offset array once: [-step, -step+1, ..., 0, ..., step-1, step]\n",
    "    offsets = np.arange(-step, step + 1)\n",
    "    n_repeats = len(offsets)  # 2*step + 1\n",
    "\n",
    "    # Repeat each row n_repeats times\n",
    "    expanded = df.loc[df.index.repeat(n_repeats)].reset_index(drop=True).copy()\n",
    "\n",
    "    # Create offset column using modulo indexing - guaranteed to match length\n",
    "    expanded[\"offset\"] = offsets[np.arange(len(expanded)) % n_repeats]\n",
    "\n",
    "    # Calculate actual block numbers\n",
    "    expanded[\"block\"] = expanded[\"block\"] + expanded[\"offset\"]\n",
    "\n",
    "    # For backward fill (offset < 0), set value to NaN\n",
    "    expanded.loc[expanded[\"offset\"] < 0, \"value\"] = np.nan\n",
    "\n",
    "    # Sort by block for fill operations\n",
    "    expanded = expanded.sort_values(\"block\").reset_index(drop=True)\n",
    "\n",
    "    # Forward fill then fill remaining with 0\n",
    "    expanded[\"value\"] = expanded[\"value\"].ffill().fillna(0)\n",
    "\n",
    "    # Drop helper column\n",
    "    result = expanded.drop(columns=[\"offset\"])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# 1. Filter for the token address of interest\n",
    "chosen_address = w3.to_checksum_address(\"0x006B6e89eE1531cfE5b6d32da0d80CC30506A339\")\n",
    "\n",
    "df_one_addr = df[df[\"address\"] == chosen_address ].copy()\n",
    "\n",
    "# filter + aggregate per (block, provider)\n",
    "df_one_addr_grouped = df_one_addr.groupby([\"block\", \"provider\"], as_index=False)[\"value\"].sum()\n",
    "\n",
    "# cumulative per provider\n",
    "gno = df_one_addr_grouped.sort_values([\"provider\", \"block\"])\n",
    "gno[\"cum_provider\"] = gno.groupby(\"provider\")[\"value\"].cumsum()\n",
    "\n",
    "# pool cumulative by block (sum the block deltas, then cumsum)\n",
    "pool = gno.groupby(\"block\", as_index=False)[\"value\"].sum().sort_values(\"block\")\n",
    "pool[\"cum_pool\"] = pool[\"value\"].cumsum()\n",
    "\n",
    "# merge pool cumulative into provider rows, compute share %\n",
    "liquidity_df = gno.merge(pool[[\"block\", \"cum_pool\"]], on=\"block\", how=\"left\")\n",
    "liquidity_df[\"share_pct\"] = (\n",
    "    liquidity_df[\"cum_provider\"] / liquidity_df[\"cum_pool\"] * 100\n",
    ")\n",
    "liquidity_df[\"share_pct\"] = liquidity_df[\"share_pct\"].astype(\n",
    "    pd.SparseDtype(\"float64\", fill_value=0.0)\n",
    ")\n",
    "\n",
    "smoothed = forward_backward_fill_blocks(liquidity_df, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd395cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_norm = px.area(\n",
    "    liquidity_df,\n",
    "    x=\"block\",\n",
    "    y=\"cum_provider\",\n",
    "    color=\"provider\",\n",
    "    line_group=\"provider\",\n",
    "    groupnorm=\"percent\",  # normalize to 100%\n",
    "    labels={\n",
    "        \"cum_provider\": \"Liquidity (normalized)\",\n",
    "        \"block\": \"Block\",\n",
    "        \"provider\": \"Provider\",\n",
    "    },\n",
    "    title=f\"Normalized Cumulative Liquidity Share for {chosen_address}\",\n",
    ")\n",
    "fig_norm.update_layout(\n",
    "    xaxis=dict(\n",
    "        tickmode=\"array\",\n",
    "        tickvals=liquidity_df[\"block\"].unique(),\n",
    "        ticktext=[str(block) for block in liquidity_df[\"block\"]],#.unique()],\n",
    "    )\n",
    ")\n",
    "fig_norm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPLETE LIQUIDITY POOL ANALYSIS CODE\n",
    "# ============================================================\n",
    "def forward_backward_fill_blocks(df, step=100):\n",
    "    \"\"\"\n",
    "    For each row, create blocks from (block-step) to (block+step).\n",
    "    Forward fill after the event, backward fill before from latest data or 0.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe with columns: block, address, event, provider, value, symbol\n",
    "    step : int\n",
    "        Number of blocks before and after each transfer event\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        DataFrame with forward/backward filled values\n",
    "    \"\"\"\n",
    "\n",
    "    # Create offset array once: [-step, -step+1, ..., 0, ..., step-1, step]\n",
    "    offsets = np.arange(-step, step + 1)\n",
    "    n_repeats = len(offsets)  # 2*step + 1\n",
    "\n",
    "    # Repeat each row n_repeats times\n",
    "    expanded = df.loc[df.index.repeat(n_repeats)].reset_index(drop=True).copy()\n",
    "\n",
    "    # Create offset column using modulo indexing - guaranteed to match length\n",
    "    expanded[\"offset\"] = offsets[np.arange(len(expanded)) % n_repeats]\n",
    "\n",
    "    # Calculate actual block numbers\n",
    "    expanded[\"block\"] = expanded[\"block\"] + expanded[\"offset\"]\n",
    "\n",
    "    # For backward fill (offset < 0), set value to NaN\n",
    "    expanded.loc[expanded[\"offset\"] < 0, \"value\"] = np.nan\n",
    "\n",
    "    # Sort by block for fill operations\n",
    "    expanded = expanded.sort_values(\"block\").reset_index(drop=True)\n",
    "\n",
    "    # Forward fill then fill remaining with 0\n",
    "    expanded[\"value\"] = expanded[\"value\"].ffill().fillna(0)\n",
    "\n",
    "    # Drop helper column\n",
    "    result = expanded.drop(columns=[\"offset\"])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_provider_labels(providers, w3):\n",
    "    \"\"\"\n",
    "    Create human-readable labels for provider addresses.\n",
    "    Format: Provider A, Provider B, etc. with shortened address\n",
    "    \"\"\"\n",
    "    labels = {}\n",
    "    for idx, provider in enumerate(providers):\n",
    "        # Ensure checksum address\n",
    "        checksum_provider = w3.to_checksum_address(provider)\n",
    "        # Shorten address: first 6 + last 4 characters\n",
    "        short_addr = f\"{checksum_provider[:6]}...{checksum_provider[-4:]}\"\n",
    "        labels[checksum_provider] = f\"Provider {chr(65 + idx)} ({short_addr})\"\n",
    "    return labels\n",
    "\n",
    "\n",
    "def calculate_pool_liquidity_sparse(df, token_address, w3):\n",
    "    \"\"\"\n",
    "    Calculate pool liquidity distribution WITHOUT smoothing.\n",
    "    Only keeps actual event blocks for accurate staircase visualization.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Transfer events with columns: block, address, provider, value\n",
    "    token_address : str\n",
    "        Token contract address to analyze\n",
    "    w3 : Web3\n",
    "        Web3 instance for address checksumming\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Liquidity data at actual event blocks only\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Filter for the token address (with checksum)\n",
    "    token_address = w3.to_checksum_address(token_address)\n",
    "\n",
    "    # Ensure all addresses in df are checksummed\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered[\"address\"] = df_filtered[\"address\"].apply(\n",
    "        lambda x: w3.to_checksum_address(x)\n",
    "    )\n",
    "    df_filtered[\"provider\"] = df_filtered[\"provider\"].apply(\n",
    "        lambda x: w3.to_checksum_address(x)\n",
    "    )\n",
    "\n",
    "    df_filtered = df_filtered[df_filtered[\"address\"] == token_address].copy()\n",
    "\n",
    "    # 2. Aggregate transfers per (block, provider)\n",
    "    df_grouped = (\n",
    "        df_filtered.groupby([\"block\", \"provider\"], as_index=False)[\"value\"]\n",
    "        .sum()\n",
    "        .sort_values([\"block\", \"provider\"])\n",
    "    )\n",
    "\n",
    "    # 3. Get all unique blocks and providers\n",
    "    all_blocks = sorted(df_grouped[\"block\"].unique())\n",
    "    all_providers = sorted(df_grouped[\"provider\"].unique())\n",
    "\n",
    "    # 4. Create provider labels\n",
    "    provider_labels = create_provider_labels(all_providers, w3)\n",
    "\n",
    "    # 5. Calculate cumulative per provider across ALL their events\n",
    "    df_grouped = df_grouped.sort_values([\"provider\", \"block\"])\n",
    "    df_grouped[\"cum_provider\"] = df_grouped.groupby(\"provider\")[\"value\"].cumsum()\n",
    "\n",
    "    # 6. For each provider, get their balance at each event block\n",
    "    provider_histories = []\n",
    "\n",
    "    for provider in all_providers:\n",
    "        provider_data = df_grouped[df_grouped[\"provider\"] == provider].copy()\n",
    "\n",
    "        # Get first and last blocks for this provider\n",
    "        first_block = provider_data[\"block\"].min()\n",
    "\n",
    "        # Only include this provider at blocks where they have > 0 balance\n",
    "        for block in all_blocks:\n",
    "            if block >= first_block:\n",
    "                # Get their balance at this block\n",
    "                balance_at_block = (\n",
    "                    provider_data[provider_data[\"block\"] <= block][\"cum_provider\"].iloc[\n",
    "                        -1\n",
    "                    ]\n",
    "                    if len(provider_data[provider_data[\"block\"] <= block]) > 0\n",
    "                    else 0\n",
    "                )\n",
    "\n",
    "                # Only add if balance > 0\n",
    "                if balance_at_block > 1e-8:\n",
    "                    provider_histories.append(\n",
    "                        {\n",
    "                            \"block\": block,\n",
    "                            \"provider\": provider,\n",
    "                            \"cum_provider\": balance_at_block,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    df_full = pd.DataFrame(provider_histories)\n",
    "\n",
    "    # 7. Calculate total pool per block\n",
    "    pool_per_block = (\n",
    "        df_full.groupby(\"block\", as_index=False)[\"cum_provider\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"cum_provider\": \"cum_pool\"})\n",
    "    )\n",
    "\n",
    "    # 8. Merge and calculate share percentage\n",
    "    df_full = df_full.merge(pool_per_block, on=\"block\", how=\"left\")\n",
    "\n",
    "    df_full[\"share_pct\"] = np.where(\n",
    "        df_full[\"cum_pool\"].abs() < 1e-10,\n",
    "        0.0,\n",
    "        (df_full[\"cum_provider\"] / df_full[\"cum_pool\"] * 100),\n",
    "    )\n",
    "    df_full[\"share_pct\"] = df_full[\"share_pct\"].clip(0, 100)\n",
    "\n",
    "    # 9. Add human-readable provider labels\n",
    "    df_full[\"provider_label\"] = df_full[\"provider\"].map(provider_labels)\n",
    "\n",
    "    # 10. Final filter: remove any remaining near-zero shares\n",
    "    df_full = df_full[df_full[\"share_pct\"] >= 0.1].copy()\n",
    "\n",
    "    return df_full.sort_values([\"block\", \"provider\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def plot_staircase_ownership(df):\n",
    "    \"\"\"\n",
    "    Staircase stacked area chart showing ownership % at each block.\n",
    "    Each provider's area height = their % share of the pool.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate block range in millions\n",
    "    min_block = df[\"block\"].min()\n",
    "    max_block = df[\"block\"].max()\n",
    "\n",
    "    # Create figure manually with individual traces per provider\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Get unique providers\n",
    "    providers = sorted(df[\"provider_label\"].unique())\n",
    "\n",
    "    # Add a trace for each provider (let plotly use default colors)\n",
    "    for idx, provider in enumerate(providers):\n",
    "        provider_data = df[df[\"provider_label\"] == provider].sort_values(\"block\")\n",
    "\n",
    "        # Only include blocks where this provider actually exists\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=provider_data[\"block\"],\n",
    "                y=provider_data[\"share_pct\"],\n",
    "                name=provider,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5, shape=\"hv\"),\n",
    "                stackgroup=\"one\",\n",
    "                groupnorm=\"\",\n",
    "                hovertemplate=\"<b>%{fullData.name}</b><br>Block: %{x}<br>Share: %{y:.4f}%<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add vertical lines for each million blocks\n",
    "    million_blocks = range(\n",
    "        int(min_block // 1_000_000) * 1_000_000,\n",
    "        int(max_block // 1_000_000 + 1) * 1_000_000 + 1,\n",
    "        1_000_000,\n",
    "    )\n",
    "\n",
    "    for million_block in million_blocks:\n",
    "        if min_block <= million_block <= max_block:\n",
    "            fig.add_vline(\n",
    "                x=million_block,\n",
    "                line_width=2,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"black\",\n",
    "                opacity=0.4,\n",
    "                annotation_text=f\"{million_block / 1_000_000:.0f}M\",\n",
    "                annotation_position=\"top\",\n",
    "                annotation_font_size=12,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Pool Ownership Distribution (Staircase View)\",\n",
    "        hovermode=\"x\",\n",
    "        yaxis_title=\"Ownership Share (%)\",\n",
    "        xaxis_title=\"Block Number\",\n",
    "        legend=dict(\n",
    "            title=\"Provider\",\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "        ),\n",
    "        yaxis=dict(range=[0, 100]),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_absolute_liquidity_staircase(df):\n",
    "    \"\"\"\n",
    "    Staircase chart showing absolute liquidity amounts (not percentages).\n",
    "    Provider areas are proportional to their actual liquidity contribution.\n",
    "    \"\"\"\n",
    "\n",
    "    min_block = df[\"block\"].min()\n",
    "    max_block = df[\"block\"].max()\n",
    "\n",
    "    # Create figure manually\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Get unique providers\n",
    "    providers = sorted(df[\"provider_label\"].unique())\n",
    "\n",
    "    # Add a trace for each provider (default colors)\n",
    "    for idx, provider in enumerate(providers):\n",
    "        provider_data = df[df[\"provider_label\"] == provider].sort_values(\"block\")\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=provider_data[\"block\"],\n",
    "                y=provider_data[\"cum_provider\"],\n",
    "                name=provider,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5, shape=\"hv\"),\n",
    "                stackgroup=\"one\",\n",
    "                hovertemplate=\"<b>%{fullData.name}</b><br>Block: %{x}<br>Amount: %{y:.6f}<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add vertical lines for each million blocks\n",
    "    million_blocks = range(\n",
    "        int(min_block // 1_000_000) * 1_000_000,\n",
    "        int(max_block // 1_000_000 + 1) * 1_000_000 + 1,\n",
    "        1_000_000,\n",
    "    )\n",
    "\n",
    "    for million_block in million_blocks:\n",
    "        if min_block <= million_block <= max_block:\n",
    "            fig.add_vline(\n",
    "                x=million_block,\n",
    "                line_width=2,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"black\",\n",
    "                opacity=0.4,\n",
    "                annotation_text=f\"{million_block / 1_000_000:.0f}M\",\n",
    "                annotation_position=\"top\",\n",
    "                annotation_font_size=12,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Pool Liquidity by Provider (Absolute Values)\",\n",
    "        hovermode=\"x\",\n",
    "        yaxis_title=\"Liquidity Amount (Token Units)\",\n",
    "        xaxis_title=\"Block Number\",\n",
    "        legend=dict(\n",
    "            title=\"Provider\",\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_ownership_concentration(df):\n",
    "    \"\"\"\n",
    "    Calculate and plot concentration metrics (Herfindahl-Hirschman Index).\n",
    "    HHI ranges from 0 (perfect competition) to 10,000 (monopoly).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert sparse to dense and handle inf values\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    share_pct_values = df_clean[\"share_pct\"].to_numpy()\n",
    "    share_pct_values = np.where(np.isinf(share_pct_values), 0, share_pct_values)\n",
    "    share_pct_values = np.where(np.isnan(share_pct_values), 0, share_pct_values)\n",
    "\n",
    "    df_clean = df_clean.assign(share_pct_clean=share_pct_values)\n",
    "\n",
    "    # Calculate HHI per block\n",
    "    hhi_data = []\n",
    "    for block in df_clean[\"block\"].unique():\n",
    "        block_data = df_clean[df_clean[\"block\"] == block]\n",
    "        # HHI = sum of squared market shares\n",
    "        hhi = (block_data[\"share_pct_clean\"] ** 2).sum()\n",
    "\n",
    "        # Count active providers (with >0.01% share)\n",
    "        active_providers = (block_data[\"share_pct_clean\"] > 0.01).sum()\n",
    "\n",
    "        hhi_data.append(\n",
    "            {\"block\": block, \"hhi\": hhi, \"active_providers\": active_providers}\n",
    "        )\n",
    "\n",
    "    hhi_df = pd.DataFrame(hhi_data)\n",
    "\n",
    "    # Create dual-axis plot\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Add HHI trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hhi_df[\"block\"],\n",
    "            y=hhi_df[\"hhi\"],\n",
    "            name=\"HHI (Concentration)\",\n",
    "            line=dict(color=\"#F46821\", width=2),\n",
    "        ),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    # Add provider count trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hhi_df[\"block\"],\n",
    "            y=hhi_df[\"active_providers\"],\n",
    "            name=\"Active Providers\",\n",
    "            line=dict(color=\"#29BEFD\", width=2),\n",
    "        ),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title=\"Pool Concentration Analysis\", hovermode=\"x unified\")\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Block Number\")\n",
    "    fig.update_yaxes(title_text=\"HHI Score\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Number of Providers\", secondary_y=True)\n",
    "\n",
    "    # Add interpretation zones\n",
    "    fig.add_hrect(\n",
    "        y0=0,\n",
    "        y1=1500,\n",
    "        fillcolor=\"green\",\n",
    "        opacity=0.1,\n",
    "        annotation_text=\"Competitive\",\n",
    "        secondary_y=False,\n",
    "    )\n",
    "    fig.add_hrect(\n",
    "        y0=1500,\n",
    "        y1=2500,\n",
    "        fillcolor=\"yellow\",\n",
    "        opacity=0.1,\n",
    "        annotation_text=\"Moderate\",\n",
    "        secondary_y=False,\n",
    "    )\n",
    "    fig.add_hrect(\n",
    "        y0=2500,\n",
    "        y1=10000,\n",
    "        fillcolor=\"red\",\n",
    "        opacity=0.1,\n",
    "        annotation_text=\"Concentrated\",\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    return fig, hhi_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN EXECUTION CODE\n",
    "# ============================================================\n",
    "\n",
    "# 1. Define the token address you want to analyze\n",
    "chosen_address = w3.to_checksum_address(\"0x006B6e89eE1531cfE5b6d32da0d80CC30506A339\")\n",
    "\n",
    "# 2. Calculate liquidity distribution (sparse, no smoothing)\n",
    "print(\"Calculating pool liquidity distribution...\")\n",
    "liquidity_df = calculate_pool_liquidity_sparse(df, chosen_address, w3)\n",
    "\n",
    "print(f\"Total rows in liquidity data: {len(liquidity_df)}\")\n",
    "print(f\"Block range: {liquidity_df['block'].min()} to {liquidity_df['block'].max()}\")\n",
    "print(f\"Number of unique providers: {liquidity_df['provider'].nunique()}\")\n",
    "\n",
    "# 3. Plot percentage ownership (stacked 0-100%)\n",
    "print(\"\\nGenerating percentage ownership chart...\")\n",
    "fig_pct = plot_staircase_ownership(liquidity_df)\n",
    "fig_pct.show()\n",
    "\n",
    "# 4. Plot absolute liquidity amounts\n",
    "print(\"Generating absolute liquidity chart...\")\n",
    "fig_abs = plot_absolute_liquidity_staircase(liquidity_df)\n",
    "fig_abs.show()\n",
    "\n",
    "# 5. Plot concentration metrics\n",
    "print(\"Generating concentration analysis...\")\n",
    "fig_conc, concentration_metrics = plot_ownership_concentration(liquidity_df)\n",
    "fig_conc.show()\n",
    "\n",
    "# 6. Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LIQUIDITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = (\n",
    "    liquidity_df.groupby([\"provider\", \"provider_label\"])[\"cum_provider\"]\n",
    "    .last()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "for (provider, label), amount in summary.items():\n",
    "    provider_checksum = w3.to_checksum_address(provider)\n",
    "    final_share = liquidity_df[\n",
    "        (liquidity_df[\"provider\"] == provider_checksum)\n",
    "        & (liquidity_df[\"block\"] == liquidity_df[\"block\"].max())\n",
    "    ][\"share_pct\"].values\n",
    "\n",
    "    if len(final_share) > 0:\n",
    "        print(f\"{label}: {amount:.6f} tokens ({final_share[0]:.2f}% of pool)\")\n",
    "    else:\n",
    "        print(f\"{label}: {amount:.6f} tokens (exited)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONCENTRATION METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average HHI: {concentration_metrics['hhi'].mean():.2f}\")\n",
    "print(f\"Current HHI: {concentration_metrics['hhi'].iloc[-1]:.2f}\")\n",
    "print(f\"Max providers at any block: {concentration_metrics['active_providers'].max()}\")\n",
    "print(f\"Current active providers: {concentration_metrics['active_providers'].iloc[-1]}\")\n",
    "\n",
    "# Interpretation\n",
    "current_hhi = concentration_metrics[\"hhi\"].iloc[-1]\n",
    "if current_hhi < 1500:\n",
    "    print(\"Pool status: ✅ COMPETITIVE (Decentralized)\")\n",
    "elif current_hhi < 2500:\n",
    "    print(\"Pool status: ⚠️  MODERATE CONCENTRATION\")\n",
    "else:\n",
    "    print(\"Pool status: 🔴 HIGHLY CONCENTRATED\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bubble_ownership(df):\n",
    "    \"\"\"\n",
    "    Bubble chart showing provider ownership at the latest block.\n",
    "    Bubble size is proportional to ownership share.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get data at the latest block only\n",
    "    latest_block = df[\"block\"].max()\n",
    "    latest_data = df[df[\"block\"] == latest_block].copy()\n",
    "\n",
    "    # Sort by share percentage descending\n",
    "    latest_data = latest_data.sort_values(\"share_pct\", ascending=False)\n",
    "\n",
    "    # Create bubble chart\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add bubble trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=latest_data[\"provider_label\"],\n",
    "            y=[1] * len(latest_data),  # All on same horizontal line\n",
    "            mode=\"markers+text\",\n",
    "            marker=dict(\n",
    "                size=latest_data[\"share_pct\"] * 10,  # Scale up for visibility\n",
    "                sizemode=\"diameter\",\n",
    "                sizemin=20,\n",
    "                color=latest_data[\"share_pct\"],\n",
    "                colorscale=\"Viridis\",\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Share (%)\", thickness=15, len=0.7),\n",
    "                line=dict(color=\"white\", width=2),\n",
    "            ),\n",
    "            text=latest_data[\"share_pct\"].apply(lambda x: f\"{x:.2f}%\"),\n",
    "            textposition=\"middle center\",\n",
    "            textfont=dict(size=14, color=\"white\", family=\"Arial Black\"),\n",
    "            hovertemplate=(\n",
    "                \"<b>%{x}</b><br>\"\n",
    "                + \"Share: \"\n",
    "                + latest_data[\"share_pct\"].apply(lambda x: f\"{x:.4f}%\")\n",
    "                + \"<br>\"\n",
    "                + \"Amount: \"\n",
    "                + latest_data[\"cum_provider\"].apply(lambda x: f\"{x:.6f}\")\n",
    "                + \"<extra></extra>\"\n",
    "            ),\n",
    "            customdata=latest_data[[\"share_pct\", \"cum_provider\"]],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Pool Ownership at Block {latest_block} (Bubble Size = Share %)\",\n",
    "        xaxis=dict(title=\"\", tickangle=-45, showgrid=False),\n",
    "        yaxis=dict(visible=False, range=[0.5, 1.5]),\n",
    "        height=500,\n",
    "        showlegend=False,\n",
    "        hovermode=\"closest\",\n",
    "        plot_bgcolor=\"rgba(240, 240, 240, 0.5)\",\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_bubble_ownership_2d(df):\n",
    "    \"\"\"\n",
    "    Alternative: 2D bubble chart with providers positioned in a grid.\n",
    "    Bubble size represents ownership share.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get data at the latest block only\n",
    "    latest_block = df[\"block\"].max()\n",
    "    latest_data = df[df[\"block\"] == latest_block].copy()\n",
    "\n",
    "    # Sort by share percentage descending\n",
    "    latest_data = latest_data.sort_values(\"share_pct\", ascending=False).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    # Calculate grid positions\n",
    "    n_providers = len(latest_data)\n",
    "    cols = int(np.ceil(np.sqrt(n_providers)))\n",
    "\n",
    "    latest_data[\"x_pos\"] = latest_data.index % cols\n",
    "    latest_data[\"y_pos\"] = latest_data.index // cols\n",
    "\n",
    "    # Create bubble chart\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=latest_data[\"x_pos\"],\n",
    "            y=latest_data[\"y_pos\"],\n",
    "            mode=\"markers+text\",\n",
    "            marker=dict(\n",
    "                size=latest_data[\"share_pct\"] * 15,  # Scale for visibility\n",
    "                sizemode=\"diameter\",\n",
    "                sizemin=30,\n",
    "                color=latest_data[\"share_pct\"],\n",
    "                colorscale=\"RdYlGn_r\",  # Red (high concentration) to Green (low)\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Share (%)\", thickness=20, len=0.7),\n",
    "                line=dict(color=\"darkgray\", width=3),\n",
    "                opacity=0.8,\n",
    "            ),\n",
    "            text=latest_data[\"provider_label\"]\n",
    "            .str.split(\"(\")\n",
    "            .str[0]\n",
    "            .str.strip(),  # Just \"Provider A\"\n",
    "            textposition=\"middle center\",\n",
    "            textfont=dict(size=12, color=\"black\", family=\"Arial Black\"),\n",
    "            hovertemplate=(\n",
    "                \"<b>%{customdata[0]}</b><br>\"\n",
    "                + \"Share: %{customdata[1]:.4f}%<br>\"\n",
    "                + \"Amount: %{customdata[2]:.6f}<br>\"\n",
    "                + \"<extra></extra>\"\n",
    "            ),\n",
    "            customdata=latest_data[\n",
    "                [\"provider_label\", \"share_pct\", \"cum_provider\"]\n",
    "            ].values,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add percentage labels inside bubbles\n",
    "    for idx, row in latest_data.iterrows():\n",
    "        fig.add_annotation(\n",
    "            x=row[\"x_pos\"],\n",
    "            y=row[\"y_pos\"] - 0.15,\n",
    "            text=f\"{row['share_pct']:.2f}%\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=10, color=\"white\", family=\"Arial Black\"),\n",
    "            bgcolor=\"rgba(0,0,0,0.5)\",\n",
    "            borderpad=2,\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Pool Ownership Distribution at Block {latest_block}\",\n",
    "        xaxis=dict(visible=False, range=[-0.5, cols - 0.5]),\n",
    "        yaxis=dict(visible=False, scaleanchor=\"x\", scaleratio=1),\n",
    "        height=600,\n",
    "        width=800,\n",
    "        showlegend=False,\n",
    "        hovermode=\"closest\",\n",
    "        plot_bgcolor=\"white\",\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Add to main execution code:\n",
    "print(\"\\nGenerating bubble ownership chart...\")\n",
    "fig_bubble = plot_bubble_ownership(liquidity_df)\n",
    "fig_bubble.show()\n",
    "\n",
    "print(\"\\nGenerating 2D bubble ownership chart...\")\n",
    "fig_bubble_2d = plot_bubble_ownership_2d(liquidity_df)\n",
    "fig_bubble_2d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885583f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIECE OF CODE TO GET THE UNIV1 EXCHANGE ADDRESS\n",
    "\n",
    "### We focus on the Factory contract of uniswap v1 \"0xc0a47dFe034B400B47bDaD5FecDa2621de6c4d95\"\n",
    "# We need the ProviderNode to be initialized already\n",
    "# Why debug_traceTransaction Is the Best Option\n",
    "# It replays the transaction within the exact historical state using your archive node and returns a detailed call graph, including internal calls and value flows—not just high-level transfers. You can choose tracers like callTracer (which outputs call frames and nested structure) for highest clarity and insight.\n",
    "# Alternatives like event logs or transaction receipts won't capture internal calls, since those are not emitted as events. You need a trace API to follow what's happening inside smart contract execution.\n",
    "uniswap_v1_factory_address, uniswap_v1_factory_abi, uniswap_v1_factory_contract = get_address_abi_contract(\n",
    "    \"0xc0a47dFe034B400B47bDaD5FecDa2621de6c4d95\"\n",
    ")\n",
    "def trace_internal_transactions(tx_hash: str, tracer: str = \"callTracer\") -> dict:\n",
    "    \"\"\"\n",
    "    Performs debug_traceTransaction with specified tracer (default: callTracer).\n",
    "    Returns the full trace result as a Python dict.\n",
    "    \"\"\"\n",
    "    trace = w3.provider.make_request(\n",
    "        \"debug_traceTransaction\", [tx_hash, {\"tracer\": tracer}]\n",
    "    )\n",
    "    return trace.get(\"result\", {})\n",
    "\n",
    "\n",
    "def extract_internal_transfers_from_trace(trace: dict) -> list:\n",
    "    \"\"\"\n",
    "    Recursively traverses the 'calls' in the trace to gather internal transfers.\n",
    "    Returns list of dicts with from, to, value, gasUsed, etc.\n",
    "    \"\"\"\n",
    "    transfers = []\n",
    "\n",
    "    def recurse(call):\n",
    "        # Internal transfer if value is non-zero\n",
    "        value = int(call.get(\"value\", \"0x0\"), 16)\n",
    "        if value > 0:\n",
    "            transfers.append(\n",
    "                {\n",
    "                    \"from\": call.get(\"from\"),\n",
    "                    \"to\": call.get(\"to\"),\n",
    "                    \"value\": value,\n",
    "                    \"gas\": int(call.get(\"gas\", \"0x0\"), 16),\n",
    "                    \"gasUsed\": int(call.get(\"gasUsed\", \"0x0\"), 16),\n",
    "                    \"type\": call.get(\"type\"),\n",
    "                    \"error\": call.get(\"error\"),\n",
    "                }\n",
    "            )\n",
    "        for sub in call.get(\"calls\", []) or []:\n",
    "            recurse(sub)\n",
    "\n",
    "    recurse(trace)\n",
    "    return transfers\n",
    "\n",
    "\n",
    "def get_internal_transactions_for_contract(\n",
    "    contract_address: str, from_block: int, to_block: int\n",
    "):\n",
    "    \"\"\"Scan blocks, identify txs to/from contract, and trace internal calls.\"\"\"\n",
    "    results = []\n",
    "    for block_num in range(from_block, to_block + 1):\n",
    "        block = w3.eth.get_block(block_num, full_transactions=True)\n",
    "        for tx in block.transactions:\n",
    "            if (\n",
    "                 tx[\"to\"]\n",
    "                 and tx[\"to\"] == uniswap_v1_factory_address\n",
    "                 or tx[\"from\"] == uniswap_v1_factory_address\n",
    "            ):\n",
    "\n",
    "                function, params = uniswap_v1_factory_contract.decode_function_input(\n",
    "                    tx[\"input\"]\n",
    "                )\n",
    "                if function.fn_name == 'createExchange':\n",
    "                    #print(tx)\n",
    "                    # print('Called function:', function.fn_name)\n",
    "                    #print('With arguments:', params)\n",
    "                    uni_created_token = params['token']\n",
    "                    univ1_token_address = w3.to_checksum_address(uni_created_token)\n",
    "                    univ1_factory_abi = get_abi(univ1_token_address, ETHERSCAN_API_KEY)\n",
    "                    univ1_factory_contract = w3.eth.contract(\n",
    "                        address=univ1_token_address, abi=univ1_factory_abi\n",
    "                    )\n",
    "                    token_name =  None\n",
    "                    token_symbol = None \n",
    "                    try:\n",
    "                        token_name = univ1_factory_contract.functions.name().call()\n",
    "                        token_symbol = univ1_factory_contract.functions.symbol().call()\n",
    "                        print(f\"Token Name: {token_name}\")\n",
    "                        print(f\"Token Symbol: {token_symbol}\")\n",
    "                    except:\n",
    "                        print(f\"Contract is a proxy {uni_created_token}\")\n",
    "\n",
    "                    token_created_exchange_address = uniswap_v1_factory_contract.functions.getExchange(\n",
    "                        uni_created_token\n",
    "                    ).call()\n",
    "                    print(f\"Token {token_name} UniExchange_address: {token_created_exchange_address}\")\n",
    "                    a = w3.to_checksum_address(token_created_exchange_address)\n",
    "                    b = get_abi(a, ETHERSCAN_API_KEY)\n",
    "                    c = w3.eth.contract(\n",
    "                        address=a, abi=b\n",
    "                    )\n",
    "                    try:\n",
    "                        #print(f\"{c.functions.name.call()}\")\n",
    "                        #print(f\"{c.functions.symbol.call()}\")\n",
    "                        print(f\"Token Address: {c.functions.tokenAddress.call()}\")\n",
    "                        #print(\n",
    "                        #    f\"Is this the same ? {c.functions.tokenAddress.call() == uni_created_token}\"\n",
    "                        #)\n",
    "                    except:\n",
    "                        print(f\"proxy: {c}\")\n",
    "            #     tx_hash = tx.hash.hex()\n",
    "            #     trace = trace_internal_transactions(tx_hash)\n",
    "            #     transfers = extract_internal_transfers_from_trace(trace)\n",
    "            #     results.append(\n",
    "            #         {\n",
    "            #             \"tx_hash\": tx_hash,\n",
    "            #             \"block\": block_num,\n",
    "            #             \"internal_transfers\": transfers,\n",
    "            #         }\n",
    "            #     )\n",
    "    return results\n",
    "\n",
    "internal_txs = get_internal_transactions_for_contract(\n",
    "    uniswap_v1_factory_contract, 6627900, w3.eth.get_block(\"latest\").number\n",
    "    #uniswap_v1_factory_contract, 6500000, w3.eth.get_block(\"latest\").number\n",
    ")\n",
    "for entry in internal_txs:\n",
    "    print(entry[\"tx_hash\"], entry[\"internal_transfers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a68ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super important code, for 1 transaction, we get all the Transfer event and we analyze which token has been exchanged\n",
    "# we also get the Gas + ETH send. If we analyze all the transaction from 1 exchange, we can probably deduct all the liquidity\n",
    "# token issued by the pair_exchange\n",
    "transaction = w3.eth.get_transaction(\n",
    "    \"1b53439a36b357c712a4abe860607c6e4d88a002dd26f97244a8ef3208b2f8b6\"\n",
    ")\n",
    "(\n",
    "    uniswap_v1_BNB_exchange_address,\n",
    "    uniswap_v1_BNB_exchange_abi,\n",
    "    uniswap_v1_BNB_exchange_contract,\n",
    ") = get_address_abi_contract(\"0x255e60c9d597dCAA66006A904eD36424F7B26286\")\n",
    "\n",
    "d_transaction = event_to_dict(transaction)\n",
    "tx_eth_value = w3.from_wei(d_transaction[\"value\"], \"ether\")\n",
    "decoded = uniswap_v1_BNB_exchange_contract.events.Transfer().process_receipt(\n",
    "    w3.eth.get_transaction_receipt(transaction.hash),\n",
    "    DISCARD,\n",
    ")\n",
    "for ev in decoded:\n",
    "    d_ev = event_to_dict(ev)\n",
    "    d_from = d_ev['args']['_from']\n",
    "    d_to = d_ev['args']['_to']\n",
    "    d_value = w3.from_wei(d_ev[\"args\"][\"_value\"], \"ether\")\n",
    "    d_address = d_ev['address']\n",
    "    d_block = d_ev['blockNumber']\n",
    "    d_tx_hash = d_ev[\"transactionHash\"]\n",
    "    token_address = w3.to_checksum_address(d_address)\n",
    "    token_abi = get_abi(token_address, ETHERSCAN_API_KEY)\n",
    "    token_contract = w3.eth.contract(address=token_address, abi=token_abi)\n",
    "    symbol = token_contract.functions.symbol().call()\n",
    "    decimals = token_contract.functions.decimals().call()\n",
    "    print(\n",
    "        f\"Block number {d_block}, {tx_eth_value} ETH was used, {d_to} received {d_value} of {symbol} from {d_from} (tx_hash is {d_tx_hash})\"\n",
    "    )\n",
    "\n",
    "def analyze_transaction_transfers(tx_hash, pair_exchange_contract, etherscan_api_key):\n",
    "    result = []\n",
    "    transaction = w3.eth.get_transaction(\n",
    "        tx_hash\n",
    "    )\n",
    "    d_transaction = event_to_dict(transaction)\n",
    "    tx_eth_value = w3.from_wei(d_transaction[\"value\"], \"ether\")\n",
    "    decoded = pair_exchange_contract.events.Transfer().process_receipt(\n",
    "        w3.eth.get_transaction_receipt(transaction.hash),\n",
    "        DISCARD,\n",
    "    )\n",
    "    for ev in decoded:\n",
    "        _ = {}\n",
    "        d_ev = event_to_dict(ev)\n",
    "        d_from = d_ev[\"args\"][\"_from\"]\n",
    "        d_to = d_ev[\"args\"][\"_to\"]\n",
    "        d_value = w3.from_wei(d_ev[\"args\"][\"_value\"], \"ether\")\n",
    "        d_address = d_ev[\"address\"]\n",
    "        d_block = d_ev[\"blockNumber\"]\n",
    "        d_tx_hash = d_ev[\"transactionHash\"]\n",
    "        token_address = w3.to_checksum_address(d_address)\n",
    "        token_abi = get_abi(token_address, etherscan_api_key)\n",
    "        token_contract = w3.eth.contract(address=token_address, abi=token_abi)\n",
    "        symbol = token_contract.functions.symbol().call()\n",
    "        decimals = token_contract.functions.decimals().call()\n",
    "        _[\"d_block\"] = d_block\n",
    "        _[\"d_from\"] = d_from\n",
    "        _[\"d_to\"] = d_to\n",
    "        _[\"d_value\"] = d_value\n",
    "        _[\"d_address\"] = d_address\n",
    "        _[\"d_tx_hash\"] = d_tx_hash\n",
    "        _[\"symbol\"] = symbol\n",
    "        _[\"decimals\"] = decimals\n",
    "        print(\n",
    "            f\"Block number {d_block}, {tx_eth_value} ETH was used, {d_to} received {d_value} of {symbol} from {d_from} (tx_hash is {d_tx_hash})\"\n",
    "        )\n",
    "        result.append(_)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf91254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the exploration block to trace function block by block\n",
    "# Super interesting to get all the liquidity\n",
    "# The problem is, we need the receipt of the transaction, and also its not straightforward to see\n",
    "# the amount of liquidity directly from those call\n",
    "\n",
    "uniswap_v1_factory_address, uniswap_v1_factory_abi, uniswap_v1_factory_contract = (\n",
    "    get_address_abi_contract(\"0x255e60c9d597dCAA66006A904eD36424F7B26286\")\n",
    ")\n",
    "def get_internal_transactions_for_contract(\n",
    "    contract_address: str, from_block: int, to_block: int\n",
    "):\n",
    "    \"\"\"Scan blocks, identify txs to/from contract, and trace internal calls.\"\"\"\n",
    "    results = []\n",
    "    for block_num in range(from_block, to_block + 1):\n",
    "        block = w3.eth.get_block(block_num, full_transactions=True)\n",
    "        for tx in block.transactions:\n",
    "            if (\n",
    "                tx[\"to\"]\n",
    "                and (tx[\"to\"] == uniswap_v1_factory_address\n",
    "                or tx[\"from\"] == uniswap_v1_factory_address)\n",
    "            ):\n",
    "                function, params = uniswap_v1_factory_contract.decode_function_input(\n",
    "                    tx[\"input\"]\n",
    "                )\n",
    "                if function.fn_name == \"createExchange\":\n",
    "                    print(tx)\n",
    "                    print('Called function:', function.fn_name)\n",
    "                    print('With arguments:', params)\n",
    "                    # print(f\"Deadline: {block_to_utc(params['deadline'])}\")\n",
    "                    print(f\"Deadline: {datetime.fromtimestamp(params['deadline'], tz=timezone.utc)}\")\n",
    "                    print(f\"Value: {w3.from_wei(tx.value, 'ether')}\")\n",
    "                    try:\n",
    "                        print(f\"Receipt from transaction: {w3.eth.get_transaction_receipt(tx.hash)}\")\n",
    "                    except:\n",
    "                        print(f\"Can't find 0x{tx.hash.hex()}\")\n",
    "                    # tx.hash.hex()\n",
    "    return results\n",
    "\n",
    "# Interesting but I'm pruned\n",
    "# trace = w3.provider.make_request(\"trace_transaction\", [tx_hash])\n",
    "# print(trace)\n",
    "block_1 = 6845140\n",
    "block_2 = 6850000\n",
    "internal_txs = get_internal_transactions_for_contract(\n",
    "    uniswap_v1_factory_contract,\n",
    "    block_1,\n",
    "    block_2\n",
    "    #w3.eth.get_block(\"latest\").number,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5e204-5ad3-4c7d-b8ba-e9e0ada96965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very old piece of code, just interesting for getting the signature of event\n",
    "# Let's keep it in case of\n",
    "\n",
    "# Example: get the Transfer event signature.\n",
    "transfer_sig = get_event_signature(\"Transfer\", token_abi)\n",
    "add_liq_sig = get_event_signature(\"AddLiquidity\", token_abi)\n",
    "remove_liq_sig = get_event_signature(\"RemoveLiquidity\", token_abi)\n",
    "print(\"Transfer signature hash:\", transfer_sig)\n",
    "\n",
    "# -- Step 3: Determine Token Genesis Block and Set Starting Block --\n",
    "# Assume you have a helper function get_contract_creation_block() that returns the creation block number.\n",
    "try:\n",
    "    genesis_block = get_contract_creation_block_etherscan(token_address, ETHERSCAN_API_KEY)\n",
    "    start_block = max(genesis_block - 1, 0)\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving genesis block, defaulting to block 0:\", e)\n",
    "    start_block = 0\n",
    "\n",
    "# -- Step 4: Fetch Transfer Events and Dump to a File (JSON Serializing) --\n",
    "def get_transfer_events_paginated(token_contract, from_block: int, to_block: int, chunk_size: int = 5000, max_workers: int = 1) -> list:\n",
    "    \"\"\"\n",
    "    Fetches Transfer events for a token_contract in the block range [from_block, to_block],\n",
    "    paginating by chunk_size to avoid Infura's result limit. Uses moderate parallelization.\n",
    "\n",
    "    Args:\n",
    "        token_contract: A Web3 contract instance with a loaded ABI.\n",
    "        from_block (int): The starting block number.\n",
    "        to_block (int): The ending block number.\n",
    "        chunk_size (int): How many blocks to query per chunk (default 5000).\n",
    "        max_workers (int): Maximum number of parallel workers (default 4).\n",
    "\n",
    "    Returns:\n",
    "        List of events.\n",
    "    \"\"\"\n",
    "    events_collected = []\n",
    "    block_ranges = []\n",
    "    \n",
    "    # Divide the full range into chunks.\n",
    "    for start_blk in range(from_block, to_block + 1, chunk_size):\n",
    "        end_blk = min(start_blk + chunk_size - 1, to_block)\n",
    "        block_ranges.append((start_blk, end_blk))\n",
    "    \n",
    "    def fetch_range(brange):\n",
    "        print(f\"Fetching for {brange}\")\n",
    "        start_blk, end_blk = brange\n",
    "        attempts = 0\n",
    "        max_retries = 5\n",
    "        while attempts < max_retries:\n",
    "            try:\n",
    "                # Add delay to mitigate rate limits.\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "                #events = token_contract.events.Transfer.get_logs(from_block=start_blk, to_block=end_blk)\n",
    "                events = token_contract.events.AddLiquidity.get_logs(from_block=start_blk, to_block=end_blk)\n",
    "                print(len(events))\n",
    "                return events\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):\n",
    "                    sleep_time = random.uniform(1, 5)\n",
    "                    print(f\"429 error for blocks {start_blk}-{end_blk}: retrying after {sleep_time:.2f} seconds...\")\n",
    "                    time.sleep(sleep_time)\n",
    "                    attempts += 1\n",
    "                else:\n",
    "                    print(f\"Error fetching logs for blocks {start_blk}-{end_blk}: {e}\")\n",
    "                    return []\n",
    "        return []  # Return empty list if all retries fail.\n",
    "    \n",
    "    # Use moderate parallelization.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_range = {executor.submit(fetch_range, brange): brange for brange in block_ranges}\n",
    "        for future in concurrent.futures.as_completed(future_to_range):\n",
    "            events = future.result()\n",
    "            events_collected.extend(events)\n",
    "    \n",
    "    return events_collected\n",
    "\n",
    "# Custom function to convert a web3 event (and its custom types) to a plain dict.\n",
    "def serialize_event(event):\n",
    "    # Convert the AttributeDict to a normal dict.\n",
    "    event_dict = dict(event)\n",
    "    # Ensure all values are JSON serializable (convert any bytes, HexBytes etc. to a string)\n",
    "    for key, value in event_dict.items():\n",
    "        if hasattr(value, \"hex\"):\n",
    "            event_dict[key] = value.hex()\n",
    "    # Also convert inner \"args\" if present.\n",
    "    if \"args\" in event_dict:\n",
    "        args = dict(event_dict[\"args\"])\n",
    "        for k, v in args.items():\n",
    "            if hasattr(v, \"hex\"):\n",
    "                args[k] = v.hex()\n",
    "        event_dict[\"args\"] = args\n",
    "    return event_dict\n",
    "\n",
    "# Fetch logs from start_block to the current block (latest)\n",
    "latest_block = w3.eth.block_number\n",
    "\n",
    "\n",
    "event_list = get_transfer_events_paginated(token_contract, start_block, latest_block)\n",
    "# Dump the result to a file (pretty-printing the JSON)\n",
    "output_filename = f\"transfer_events_{contract_address}.json\"\n",
    "with open(output_filename, \"w\") as f:\n",
    "    json.dump(serialized_events, f, indent=4)\n",
    "\n",
    "print(f\"Dumped {len(serialized_events)} events to {output_filename}\")\n",
    "\n",
    "\n",
    "# Mint function signature and selector\n",
    "target_types = \"address,int24,int24,uint128,bytes\"\n",
    "function_name = \"mint\"\n",
    "\n",
    "target_types = \"uint16\"\n",
    "function_name = \"increaseObservationCardinalityNext\"\n",
    "\n",
    "target_types = (\"bytes[]\",)\n",
    "function_name = \"multicall\"\n",
    "\n",
    "target_selector = Web3.keccak(text=\"multicall(bytes[])\")[:4].hex()\n",
    "print(target_selector)\n",
    "# MINT_SELECTOR = \"ac9650d8\"\n",
    "\n",
    "\n",
    "# ----- Helper Functions -----\n",
    "def load_processed_tx_hashes(file_path):\n",
    "    \"\"\"Load processed transaction hashes from a text file (one per line).\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return set(line.strip() for line in f if line.strip())\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "\n",
    "def append_processed_tx_hashes(new_hashes, file_path):\n",
    "    \"\"\"Append a set of transaction hashes to a file, one per line.\"\"\"\n",
    "    with open(file_path, \"a\") as f:\n",
    "        for tx in new_hashes:\n",
    "            # f.write(tx + \"\\n\")\n",
    "            pass\n",
    "\n",
    "\n",
    "def append_mint_calls(mint_calls, file_path):\n",
    "    \"\"\"Append mint call results to the output file, one JSON object per line.\"\"\"\n",
    "    with open(file_path, \"a\") as f:\n",
    "        for call in mint_calls:\n",
    "            f.write(json.dumps(call) + \"\\n\")\n",
    "\n",
    "\n",
    "def load_transactions_data(file_path):\n",
    "    \"\"\"Load the transactions data from a JSON file (assumed to be a dict of tx_hash -> tx data).\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        return {}\n",
    "\n",
    "\n",
    "def sanitize_value(value):\n",
    "    \"\"\"\n",
    "    Recursively convert bytes to hex strings.\n",
    "    \"\"\"\n",
    "    if isinstance(value, bytes):\n",
    "        return value.hex()\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        return type(value)(sanitize_value(v) for v in value)\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "def decode_tx_by_function(tx, contract, target_function_name):\n",
    "    \"\"\"\n",
    "    Decode a transaction's input using the contract ABI, returning a human-readable\n",
    "    mapping of parameter names to values if the transaction calls the target function.\n",
    "\n",
    "    Parameters:\n",
    "      tx (dict): The transaction dictionary.\n",
    "      contract: The contract instance (with ABI loaded).\n",
    "      target_function_name (str): The name of the function to decode (e.g. \"mint\").\n",
    "\n",
    "    Returns:\n",
    "      dict or None: If the transaction calls the target function, returns a dict:\n",
    "         {\n",
    "           \"transaction_hash\": <tx hash>,\n",
    "           \"blockNumber\": <block number>,\n",
    "           \"<target_function_name>_args\": { <param1>: <value1>, <param2>: <value2>, ... }\n",
    "         }\n",
    "         Otherwise, returns None.\n",
    "    \"\"\"\n",
    "    input_data = tx.get(\"input\", \"\")\n",
    "    if not input_data:\n",
    "        return None\n",
    "    try:\n",
    "        # This will automatically try to decode the function input based on the contract ABI.\n",
    "        func_obj, params = contract.decode_function_input(input_data)\n",
    "        if func_obj.fn_name != target_function_name:\n",
    "            return None\n",
    "        # Build a mapping from parameter names to sanitized values.\n",
    "        param_mapping = {}\n",
    "        for inp in func_obj.abi.get(\"inputs\", []):\n",
    "            name = inp.get(\"name\")\n",
    "            value = params.get(name)\n",
    "            param_mapping[name] = sanitize_value(value)\n",
    "        return {\n",
    "            \"transaction_hash\": tx.get(\"hash\"),\n",
    "            \"blockNumber\": tx.get(\"blockNumber\"),\n",
    "            f\"{target_function_name}_args\": param_mapping,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding {target_function_name} call in tx {tx.get('hash')}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_single_transaction(tx, target_selector, target_types, function_name):\n",
    "    \"\"\"\n",
    "    Process a single transaction.\n",
    "    If the transaction's input field starts with the target selector, decode the parameters\n",
    "    according to target_types and return a dict with the function call details;\n",
    "    otherwise return None.\n",
    "\n",
    "    Parameters:\n",
    "      tx (dict): A transaction dictionary.\n",
    "      target_selector (str): The 4-byte function selector (as hex string, e.g. \"0xabcdef12\").\n",
    "      target_types (list): A list of ABI types for the function's parameters\n",
    "                           (e.g. [\"address\", \"int24\", \"int24\", \"uint128\", \"bytes\"]).\n",
    "      function_name (str): A label for the function being decoded (e.g. \"mint\").\n",
    "\n",
    "    Returns:\n",
    "      dict or None: A dictionary containing the transaction hash, block number, and a key\n",
    "                    named \"<function_name>_args\" mapped to the decoded parameters, or None if not matching.\n",
    "    \"\"\"\n",
    "    input_data = tx.get(\"input\", \"\")\n",
    "    if input_data and input_data.startswith(target_selector):\n",
    "        # Remove the \"0x\" and the selector (first 10 characters: \"0x\" + 8 hex digits)\n",
    "        data_without_selector = input_data[8:]\n",
    "        try:\n",
    "            # Decode the parameters using the provided types.\n",
    "            raw_params = decode(target_types, bytes.fromhex(data_without_selector))\n",
    "            # Sanitize: convert any bytes into hex strings.\n",
    "            sanitized_params = tuple(sanitize_value(p) for p in raw_params)\n",
    "            # Create a mapping from parameter names to values.\n",
    "            param_mapping = {\n",
    "                name: value for name, value in zip(parameter_names, sanitized_params)\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"transaction_hash\": tx.get(\"hash\"),\n",
    "                \"blockNumber\": tx.get(\"blockNumber\"),\n",
    "                f\"{function_name}_args\": sanitized_params,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding {function_name} call in tx {tx.get('hash')}: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_transactions_in_batches(\n",
    "    transactions_file,\n",
    "    processed_tx_file,\n",
    "    mint_calls_file,\n",
    "    batch_size,\n",
    "    target_selector,\n",
    "    target_types,\n",
    "    function_name,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process transactions from transactions_file to find Mint calls.\n",
    "\n",
    "    - Loads the transactions (as a dict mapping tx_hash -> tx data).\n",
    "    - Loads already processed transaction hashes from processed_tx_file.\n",
    "    - Iterates over transactions that haven't been processed.\n",
    "    - In parallel, processes each transaction to see if it is a Mint call.\n",
    "    - Every batch_size transactions processed, flush the results to mint_calls_file\n",
    "      (one JSON object per line) and append the processed transaction hashes to processed_tx_file.\n",
    "    \"\"\"\n",
    "    all_tx = load_transactions_data(transactions_file)\n",
    "    processed = load_processed_tx_hashes(processed_tx_file)\n",
    "\n",
    "    # Only process transactions that have not been processed yet.\n",
    "    tx_list = [tx for tx in all_tx.values() if tx.get(\"hash\") not in processed]\n",
    "    print(f\"Total transactions to process: {len(tx_list)}\")\n",
    "\n",
    "    processed_in_batch = set()\n",
    "    mint_calls_batch = []\n",
    "    total_processed = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                process_single_transaction,\n",
    "                tx,\n",
    "                target_selector,\n",
    "                target_types,\n",
    "                function_name,\n",
    "            ): tx\n",
    "            for tx in tx_list\n",
    "        }\n",
    "        for future in as_completed(futures):\n",
    "            tx = futures[future]\n",
    "            tx_hash = tx.get(\"hash\")\n",
    "            processed_in_batch.add(tx_hash)\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                mint_calls_batch.append(result)\n",
    "            total_processed += 1\n",
    "\n",
    "            # If we've processed a batch, flush the results.\n",
    "            if total_processed % batch_size == 0:\n",
    "                if mint_calls_batch:\n",
    "                    print(type(mint_calls_batch))\n",
    "                    print(mint_calls_batch)\n",
    "                    append_mint_calls(mint_calls_batch, mint_calls_file)\n",
    "                    print(\n",
    "                        f\"Flushed {len(mint_calls_batch)} mint call entries to {mint_calls_file}.\"\n",
    "                    )\n",
    "                    mint_calls_batch = []\n",
    "                if processed_in_batch:\n",
    "                    append_processed_tx_hashes(processed_in_batch, processed_tx_file)\n",
    "                    processed_in_batch = set()\n",
    "\n",
    "    # Flush any remaining entries after processing all transactions.\n",
    "    if mint_calls_batch:\n",
    "        append_mint_calls(mint_calls_batch, mint_calls_file)\n",
    "        print(\n",
    "            f\"Flushed remaining {len(mint_calls_batch)} mint call entries to {mint_calls_file}.\"\n",
    "        )\n",
    "    if processed_in_batch:\n",
    "        append_processed_tx_hashes(processed_in_batch, processed_tx_file)\n",
    "        print(\n",
    "            f\"Updated processed transaction file with remaining {len(processed_in_batch)} entries.\"\n",
    "        )\n",
    "\n",
    "    print(\"Transaction processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45079a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code explore block 1 by 1 to find every transaction emitted or received from 1 address\n",
    "# It's super slow but very deep investigation\n",
    "\n",
    "def get_internal_transactions_with_trace(\n",
    "    contract_address: str,\n",
    "    from_block: int,\n",
    "    to_block: int,\n",
    "    max_workers: int = 16,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch all transactions involving a contract and optionally trace internal calls.\n",
    "\n",
    "    Parameters:\n",
    "        w3: Web3 instance connected to a local archive node.\n",
    "        contract_address: Ethereum contract address (string).\n",
    "        from_block: Starting block number (int).\n",
    "        to_block: Ending block number (int).\n",
    "        max_workers: Number of threads for parallel fetching.\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries, each containing:\n",
    "            - 'transaction': The transaction object.\n",
    "            - 'internal_calls': List of internal calls (empty if trace not available).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    def process_block(block_number: int):\n",
    "        \"\"\"Fetch transactions in a block and trace internal calls if available.\"\"\"\n",
    "        block = w3.eth.get_block(block_number, full_transactions=True)\n",
    "        block_results = []\n",
    "\n",
    "        for tx in block.transactions:\n",
    "            # Filter top-level transactions to/from the contract\n",
    "            if tx[\"to\"] == contract_address or tx[\"from\"] == contract_address:\n",
    "                tx_entry = {\"transaction\": tx, \"internal_calls\": []}\n",
    "\n",
    "                # Try to fetch internal calls via trace_transaction\n",
    "                try:\n",
    "                    trace = w3.manager.request_blocking(\n",
    "                        \"trace_transaction\", [tx[\"hash\"].hex()]\n",
    "                    )\n",
    "                    tx_entry[\"internal_calls\"] = trace\n",
    "                except Exception as e:\n",
    "                    # If trace API not enabled, just skip internal calls\n",
    "                    tx_entry[\"internal_calls\"] = []\n",
    "\n",
    "                block_results.append(tx_entry)\n",
    "\n",
    "        return block_results\n",
    "\n",
    "    # Use ThreadPoolExecutor to fetch blocks in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_block, b): b\n",
    "            for b in range(from_block, to_block + 1)\n",
    "        }\n",
    "        for future in as_completed(futures):\n",
    "            results.extend(future.result())\n",
    "\n",
    "    return results\n",
    "\n",
    "contract_address = Web3.toChecksumAddress(\"0x255e60c9d597dCAA66006A904eD36424F7B26286\")\n",
    "from_block = 6845140\n",
    "to_block = 6850000\n",
    "txs_with_internal = get_internal_transactions_with_trace(\n",
    "    contract_address, from_block, to_block\n",
    ")\n",
    "for tx_entry in txs_with_internal:\n",
    "    print(tx_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3db616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important code\n",
    "# We look for the Genesis Uniswap factory, and we get all its events (Only 1 for the factory: 'NewExchange')\n",
    "# Then we scan from 0 to latest block every NexEchange created from this Factory\n",
    "# (We have the filter of events in case we are filtering events from contract that have multiple events to remove when we don't care)\n",
    "address, abi, contract = get_address_abi_contract(\"0xC0A47DFE034B400B47BDAD5FECDA2621DE6C4D95\") # Uniswap Genesis Factory\n",
    "start_block = 0\n",
    "end_block = 'latest'\n",
    "# list all event names\n",
    "event_names = [ev.event_name for ev in contract.events]\n",
    "print(event_names)\n",
    "\n",
    "# define which events you want and filters directly\n",
    "events_to_scan = [\n",
    "    contract.events.NewExchange().get_logs,\n",
    "    #contract.events.Transfer().get_logs,\n",
    "    #contract.events.Approval().get_logs,\n",
    "]\n",
    "L_LOGS = [] # IMPORTANT\n",
    "for get_logs_fn in events_to_scan:\n",
    "    logs = get_logs_fn(\n",
    "        from_block=start_block,\n",
    "        to_block=end_block,\n",
    "        argument_filters={},  # or {\"from\": some_address}, {\"to\": [addr1, addr2]}\n",
    "    )\n",
    "    for log in logs:\n",
    "        # print(log[\"transactionHash\"].hex(), log[\"blockNumber\"], log[\"event\"])\n",
    "        L_LOGS.append(log)\n",
    "\n",
    "# Important code we use in combination with the events filter\n",
    "# We created a list of Exchange created by the Uniswap V1 Factory Contract and we list all their Events\n",
    "# We create the Dictionnary\n",
    "# \"exchange_address_1\": {\"event_1\": {}, event_2: {}, event_3:{}}\n",
    "# This dict fed with the code allow us to retrieve every transactions with the events(logs) of this exchange\n",
    "# we can then sniff Liquidity out of it\n",
    "\n",
    "FULL_EVENT_BY_CONTRACTS = {}  # IMPORTANT\n",
    "for i in L_LOGS:\n",
    "    add, abi, contract = get_address_abi_contract(i.args.exchange)\n",
    "    event_names = [ev.event_name for ev in contract.events]\n",
    "    FULL_EVENT_BY_CONTRACTS[add] = {event: {} for event in event_names}\n",
    "    time.sleep(1)\n",
    "\n",
    "print(len(FULL_EVENT_BY_CONTRACTS)) # ~ 4019\n",
    "filename = \"real/FULL_EVENT_BY_CONTRACTS.json\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\"{filename} already exists!\")\n",
    "else:\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(filename, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I Think it helps print the name of the token we find in Uniswap pair exchange V1\n",
    "FULL_EVENT_BY_CONTRACTS = json.load(open(r\"real/FULL_EVENT_BY_CONTRACTS.json\"))\n",
    "# We often need the Initial Factory informations\n",
    "uniswap_factory_address, uniswap_factory_abi, uniswap_factory_contract = (\n",
    "    get_address_abi_contract(Web3.to_checksum_address(\"0xC0A47DFE034B400B47BDAD5FECDA2621DE6C4D95\"))\n",
    ")  # Uniswap Genesis Factory\n",
    "result = []\n",
    "for exchange_address in FULL_EVENT_BY_CONTRACTS.keys():\n",
    "    try:\n",
    "        token_addr = uniswap_factory_contract.functions.getToken(exchange_address).call()\n",
    "        token_name = get_token_name_by_contract(token_addr)\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Something went wrong: {e} for: Exchange {exchange_address}, underlying token {token_addr}\"\n",
    "        )\n",
    "        continue\n",
    "    if isinstance(token_name, bytes):\n",
    "        name = token_name.decode(\"utf-8\", errors=\"ignore\").replace(\"\\x00\", \"\").strip()\n",
    "    else:\n",
    "        name = token_name.strip()\n",
    "    result.append((name, exchange_address))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dafe4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = liquidity_df[\"block\"].max()\n",
    "blocks = sorted(liquidity_df[\"block\"].unique())\n",
    "prev = blocks[-2] if len(blocks) >= 2 else None\n",
    "\n",
    "df_now = liquidity_df[liquidity_df[\"block\"] == latest][[\"provider\", \"share_pct\"]].copy()\n",
    "if prev is not None:\n",
    "    df_prev = (\n",
    "        liquidity_df[liquidity_df[\"block\"] == prev][[\"provider\", \"share_pct\"]]\n",
    "        .copy()\n",
    "        .rename(columns={\"share_pct\": \"share_prev\"})\n",
    "    )\n",
    "    df = df_now.merge(df_prev, on=\"provider\", how=\"left\")\n",
    "    df[\"share_prev\"] = df[\"share_prev\"].fillna(0.0)\n",
    "else:\n",
    "    df = df_now.copy()\n",
    "    df[\"share_prev\"] = 0.0\n",
    "\n",
    "max_share = max(df[\"share_pct\"].max(), df[\"share_prev\"].max(), 1e-9)\n",
    "desired_px = 80\n",
    "sizeref = 2 * max_share / (desired_px**2)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "if prev is not None:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"provider\"],\n",
    "            y=df[\"share_prev\"],\n",
    "            mode=\"markers\",\n",
    "            name=f\"Block {prev}\",\n",
    "            marker=dict(\n",
    "                size=df[\"share_prev\"],\n",
    "                sizemode=\"area\",\n",
    "                sizeref=sizeref,\n",
    "                color=\"lightgrey\",\n",
    "                opacity=0.4,\n",
    "                line=dict(color=\"grey\", width=1),\n",
    "            ),\n",
    "            hovertemplate=\"Provider: %{x}<br>Prev share: %{marker.size:.2%}<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df[\"provider\"],\n",
    "        y=df[\"share_pct\"],\n",
    "        mode=\"markers+text\",\n",
    "        name=f\"Block {latest}\",\n",
    "        marker=dict(\n",
    "            size=df[\"share_pct\"],\n",
    "            sizemode=\"area\",\n",
    "            sizeref=sizeref,\n",
    "            color=\"steelblue\",\n",
    "            line=dict(color=\"DarkSlateGrey\", width=1),\n",
    "        ),\n",
    "        text=df[\"share_pct\"].apply(lambda s: f\"{s:.1%}\"),\n",
    "        textposition=\"top center\",\n",
    "        hovertemplate=\"Provider: %{x}<br>Current share: %{marker.size:.2%}<extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Liquidity share per provider at block {latest}\",\n",
    "    xaxis=dict(title=\"Provider\"),\n",
    "    yaxis=dict(title=\"Share of pool\"),\n",
    "    showlegend=True,\n",
    "    margin=dict(t=50, b=100),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "closedblind-ylehWVqW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
